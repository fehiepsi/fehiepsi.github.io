[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "fehiepsi's blog",
    "section": "",
    "text": "My name is Du Phan. I got my PhD in Mathematics and Data Science from POSTECH, South Korea. I have a deep interest in machine learning, deep learning, and recently Bayesian statistics. Having learned a lot from free resources on the internet, I hope that with this blog, I can contribute my knowledge back to the community.\nMy hobbies include playing flute, chess, and badminton.\nview my resume\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/projecteuler-first50.html",
    "href": "blog/projecteuler-first50.html",
    "title": "Solutions to Project Euler’s first 50 problems",
    "section": "",
    "text": "These are codes to solve the first 50 problems in Project Euler. The thing is that I try to make these python codes give solutions in less than 1 second. This is achieved by using the excellent numba package. This post is written in an IPython notebook.\n\n# !mkdir pe\n\n\ndef problem1():\n    n = 1000\n    s = 0\n    for i in range(1, n):\n        if i % 3 == 0 or i % 5 == 0:\n            s += i\n    return s\n\nprint(problem1())\n%timeit problem1()\n\n233168\n1000 loops, best of 3: 245 µs per loop\n\n\n\ndef problem2():\n    n = 4000000\n    s = 0\n    a, b = 1, 2\n    while a &lt;= n:\n        if a % 2 == 0:\n            s += a\n        a, b = b, a+b\n    return s\n\nprint(problem2())\n%timeit problem2()\n\n4613732\n100000 loops, best of 3: 8.44 µs per loop\n\n\n\n\ndef problem3():\n    n = 600851475143\n    i = 1\n    while n != 1:\n        i += 1\n        while n % i == 0:\n            n = n // i\n    return i\n\nprint(problem3())\n%timeit problem3()\n\n6857\n1000 loops, best of 3: 1.68 ms per loop\n\n\n\ndef problem4():\n    m = 10000\n    for i in range(100, 1000):\n        for j in range(i, 1000):\n            p = i * j\n            if str(p) == str(p)[::-1] and p &gt; m:\n                m = p\n    return m\n\nprint(problem4())\n%timeit problem4()\n\n906609\n1 loop, best of 3: 352 ms per loop\n\n\n\ndef common_divisor(a, b):\n    while a != 0:\n        a, b = b%a, a\n    return b\n\ndef problem5():\n    n = 20\n    p = 1\n    for i in range(2, n+1):\n        p = (p * i) // common_divisor(p, i)\n    return p\n\nprint(problem5())\n%timeit problem5()\n\n232792560\n100000 loops, best of 3: 15.9 µs per loop\n\n\n\ndef problem6():\n    n = 100\n    s = 0\n    for i in range(1, n):\n        for j in range(i+1, n+1):\n            s += i * j\n    return 2 * s\n\nprint(problem6())\n%timeit problem6()\n\n25164150\n1000 loops, best of 3: 530 µs per loop\n\n\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef sieveEratosthenes(n):\n    s = np.ones(n+1)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n\n@jit\ndef problem7():\n    n = 10001\n    limit = 10**6\n    sieve = sieveEratosthenes(limit)\n    c = 0\n    for i in range(len(sieve)):\n        if sieve[i] == 1:\n            c += 1\n        if c == n:\n            return i\n\nprint(problem7())\n%timeit problem7()\n\n104743\n10 loops, best of 3: 27.3 ms per loop\n\n\n\n%%file pe/problem8.txt\n73167176531330624919225119674426574742355349194934\n96983520312774506326239578318016984801869478851843\n85861560789112949495459501737958331952853208805511\n12540698747158523863050715693290963295227443043557\n66896648950445244523161731856403098711121722383113\n62229893423380308135336276614282806444486645238749\n30358907296290491560440772390713810515859307960866\n70172427121883998797908792274921901699720888093776\n65727333001053367881220235421809751254540594752243\n52584907711670556013604839586446706324415722155397\n53697817977846174064955149290862569321978468622482\n83972241375657056057490261407972968652414535100474\n82166370484403199890008895243450658541227588666881\n16427171479924442928230863465674813919123162824586\n17866458359124566529476545682848912883142607690042\n24219022671055626321111109370544217506941658960408\n07198403850962455444362981230987879927244284909188\n84580156166097919133875499200524063689912560717606\n05886116467109405077541002256983155200055935729725\n71636269561882670428252483600823257530420752963450\n\nOverwriting pe/problem8.txt\n\n\n\ndef problem8():\n    a = []\n    with open('pe/problem8.txt') as f:\n        for line in f:\n            a.append(line.rstrip('\\n'))\n    m = 0\n    for n in range(996):\n        p = 1\n        for i in range(5):\n            p *= int(a[(n+i)//50][(n+i)%50])\n        if p &gt; m:\n            m = p\n    return m\n\nprint(problem8())\n%timeit problem8()\n\n40824\n100 loops, best of 3: 4.2 ms per loop\n\n\n\ndef problem9():\n    n = 1000\n    for b in range(1, n//2):\n        for a in range(1, n-2*b):\n            c = 1000 - a - b\n            if a**2 + b**2 == c**2:\n                return a*b*c\n\nprint(problem9())\n%timeit problem9()\n\n31875000\n1 loop, best of 3: 229 ms per loop\n\n\n\nimport numpy as np\nfrom numba import jit, int64\n\n@jit\ndef sieveEratosthenes(n):\n    s = np.ones(n+1, dtype=np.int8)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n\n@jit(locals={'s': int64})\ndef problem10():\n    n = 2000000\n    sieve = sieveEratosthenes(n-1)\n    s = 0\n    for i in range(len(sieve)):\n        if sieve[i] == 1:\n            s += i\n    return s\n\nprint(problem10())\n%timeit problem10()\n\n142913828922\n100 loops, best of 3: 9.03 ms per loop\n\n\n\n%%file pe/problem11.txt\n08 02 22 97 38 15 00 40 00 75 04 05 07 78 52 12 50 77 91 08\n49 49 99 40 17 81 18 57 60 87 17 40 98 43 69 48 04 56 62 00\n81 49 31 73 55 79 14 29 93 71 40 67 53 88 30 03 49 13 36 65\n52 70 95 23 04 60 11 42 69 24 68 56 01 32 56 71 37 02 36 91\n22 31 16 71 51 67 63 89 41 92 36 54 22 40 40 28 66 33 13 80\n24 47 32 60 99 03 45 02 44 75 33 53 78 36 84 20 35 17 12 50\n32 98 81 28 64 23 67 10 26 38 40 67 59 54 70 66 18 38 64 70\n67 26 20 68 02 62 12 20 95 63 94 39 63 08 40 91 66 49 94 21\n24 55 58 05 66 73 99 26 97 17 78 78 96 83 14 88 34 89 63 72\n21 36 23 09 75 00 76 44 20 45 35 14 00 61 33 97 34 31 33 95\n78 17 53 28 22 75 31 67 15 94 03 80 04 62 16 14 09 53 56 92\n16 39 05 42 96 35 31 47 55 58 88 24 00 17 54 24 36 29 85 57\n86 56 00 48 35 71 89 07 05 44 44 37 44 60 21 58 51 54 17 58\n19 80 81 68 05 94 47 69 28 73 92 13 86 52 17 77 04 89 55 40\n04 52 08 83 97 35 99 16 07 97 57 32 16 26 26 79 33 27 98 66\n88 36 68 87 57 62 20 72 03 46 33 67 46 55 12 32 63 93 53 69\n04 42 16 73 38 25 39 11 24 94 72 18 08 46 29 32 40 62 76 36\n20 69 36 41 72 30 23 88 34 62 99 69 82 67 59 85 74 04 36 16\n20 73 35 29 78 31 90 01 74 31 49 71 48 86 81 16 23 57 05 54\n01 70 54 71 83 51 54 69 16 92 33 48 61 43 52 01 89 19 67 48\n\nOverwriting pe/problem11.txt\n\n\n\ndef problem11():\n    a = []\n    with open('pe/problem11.txt') as f:\n        for line in f:\n            a.append([int(x) for x in line.split()])\n    for i in range(20):\n        a[i] = [0, 0, 0] + a[i] + [0, 0, 0]\n    for j in range(3):\n        a.append([0] * 26)\n    m = 0\n    for i in range(20):\n        for j in range(3, 20):\n            r = c = dr = dl = a[i][j]\n            for k in range(1, 4):\n                r *= a[i][j+k]\n                c *= a[i+k][j]\n                dr *= a[i+k][j+k]\n                dl *= a[i+k][j-k]\n            m1 = max(r, c, dr, dl)\n            if m1 &gt; m:\n                m = m1\n    return m\n\nprint(problem11())\n%timeit problem11\n\n70600674\nThe slowest run took 45.10 times longer than the fastest. This could mean that an intermediate result is being cached.\n10000000 loops, best of 3: 27.3 ns per loop\n\n\n\nfrom numba import jit\n\n@jit\ndef find_num_divisor(n):\n    s = 1\n    while n % 2 == 0:\n        n //= 2\n        s += 1\n    i = 3\n    while i &lt;= n:\n        c = 0\n        while n % i == 0:\n            n //= i\n            c += 1\n        s *= (c + 1)\n        i += 2\n    return s\n\n@jit\ndef problem12():\n    n = 500\n    s = 1\n    i = 1\n    while find_num_divisor(s) &lt;= n:\n        i += 1\n        s += i\n    return s\n\nprint(problem12())\n%timeit problem12()\n\n76576500\n10 loops, best of 3: 161 ms per loop\n\n\n\n%%file pe/problem13.txt\n37107287533902102798797998220837590246510135740250\n46376937677490009712648124896970078050417018260538\n74324986199524741059474233309513058123726617309629\n91942213363574161572522430563301811072406154908250\n23067588207539346171171980310421047513778063246676\n89261670696623633820136378418383684178734361726757\n28112879812849979408065481931592621691275889832738\n44274228917432520321923589422876796487670272189318\n47451445736001306439091167216856844588711603153276\n70386486105843025439939619828917593665686757934951\n62176457141856560629502157223196586755079324193331\n64906352462741904929101432445813822663347944758178\n92575867718337217661963751590579239728245598838407\n58203565325359399008402633568948830189458628227828\n80181199384826282014278194139940567587151170094390\n35398664372827112653829987240784473053190104293586\n86515506006295864861532075273371959191420517255829\n71693888707715466499115593487603532921714970056938\n54370070576826684624621495650076471787294438377604\n53282654108756828443191190634694037855217779295145\n36123272525000296071075082563815656710885258350721\n45876576172410976447339110607218265236877223636045\n17423706905851860660448207621209813287860733969412\n81142660418086830619328460811191061556940512689692\n51934325451728388641918047049293215058642563049483\n62467221648435076201727918039944693004732956340691\n15732444386908125794514089057706229429197107928209\n55037687525678773091862540744969844508330393682126\n18336384825330154686196124348767681297534375946515\n80386287592878490201521685554828717201219257766954\n78182833757993103614740356856449095527097864797581\n16726320100436897842553539920931837441497806860984\n48403098129077791799088218795327364475675590848030\n87086987551392711854517078544161852424320693150332\n59959406895756536782107074926966537676326235447210\n69793950679652694742597709739166693763042633987085\n41052684708299085211399427365734116182760315001271\n65378607361501080857009149939512557028198746004375\n35829035317434717326932123578154982629742552737307\n94953759765105305946966067683156574377167401875275\n88902802571733229619176668713819931811048770190271\n25267680276078003013678680992525463401061632866526\n36270218540497705585629946580636237993140746255962\n24074486908231174977792365466257246923322810917141\n91430288197103288597806669760892938638285025333403\n34413065578016127815921815005561868836468420090470\n23053081172816430487623791969842487255036638784583\n11487696932154902810424020138335124462181441773470\n63783299490636259666498587618221225225512486764533\n67720186971698544312419572409913959008952310058822\n95548255300263520781532296796249481641953868218774\n76085327132285723110424803456124867697064507995236\n37774242535411291684276865538926205024910326572967\n23701913275725675285653248258265463092207058596522\n29798860272258331913126375147341994889534765745501\n18495701454879288984856827726077713721403798879715\n38298203783031473527721580348144513491373226651381\n34829543829199918180278916522431027392251122869539\n40957953066405232632538044100059654939159879593635\n29746152185502371307642255121183693803580388584903\n41698116222072977186158236678424689157993532961922\n62467957194401269043877107275048102390895523597457\n23189706772547915061505504953922979530901129967519\n86188088225875314529584099251203829009407770775672\n11306739708304724483816533873502340845647058077308\n82959174767140363198008187129011875491310547126581\n97623331044818386269515456334926366572897563400500\n42846280183517070527831839425882145521227251250327\n55121603546981200581762165212827652751691296897789\n32238195734329339946437501907836945765883352399886\n75506164965184775180738168837861091527357929701337\n62177842752192623401942399639168044983993173312731\n32924185707147349566916674687634660915035914677504\n99518671430235219628894890102423325116913619626622\n73267460800591547471830798392868535206946944540724\n76841822524674417161514036427982273348055556214818\n97142617910342598647204516893989422179826088076852\n87783646182799346313767754307809363333018982642090\n10848802521674670883215120185883543223812876952786\n71329612474782464538636993009049310363619763878039\n62184073572399794223406235393808339651327408011116\n66627891981488087797941876876144230030984490851411\n60661826293682836764744779239180335110989069790714\n85786944089552990653640447425576083659976645795096\n66024396409905389607120198219976047599490197230297\n64913982680032973156037120041377903785566085089252\n16730939319872750275468906903707539413042652315011\n94809377245048795150954100921645863754710598436791\n78639167021187492431995700641917969777599028300699\n15368713711936614952811305876380278410754449733078\n40789923115535562561142322423255033685442488917353\n44889911501440648020369068063960672322193204149535\n41503128880339536053299340368006977710650566631954\n81234880673210146739058568557934581403627822703280\n82616570773948327592232845941706525094512325230608\n22918802058777319719839450180888072429661980811197\n77158542502016545090413245809786882778948721859617\n72107838435069186155435662884062257473692284509516\n20849603980134001723930671666823555245252804609722\n53503534226472524250874054075591789781264330331690\n\nOverwriting pe/problem13.txt\n\n\n\ndef problem13():\n    a = []\n    with open('pe/problem13.txt') as f:\n        for line in f:\n            a.append(int(line.rstrip('\\n')))\n    return int(str(sum(a))[:10])\n\nprint(problem13())\n%timeit problem13()\n\n5537376230\n1000 loops, best of 3: 262 µs per loop\n\n\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef problem14():\n    n = 1000000\n    table = np.zeros(n)\n    table[1] = 1\n    for i in range(2, n):\n        t = i\n        c = 0\n        while t &gt;= i:\n            c += 1\n            if t % 2 == 0:\n                t //= 2\n            else:\n                t = 3 * t + 1\n        table[i] = c + table[t]\n    m = 0\n    r = 0\n    for i in range(n):\n        if table[i] &gt; m:\n            m = table[i]\n            r = i\n    return r\n\nprint(problem14())\n%timeit problem14()\n\n837799\n10 loops, best of 3: 31.6 ms per loop\n\n\n\nfrom math import factorial\n\ndef problem15():\n    m = n = 20\n    return factorial(m+n) // (factorial(m) * factorial(n))\n\nprint(problem15())\n%timeit problem15()\n\n137846528820\nThe slowest run took 4.87 times longer than the fastest. This could mean that an intermediate result is being cached.\n100000 loops, best of 3: 2.11 µs per loop\n\n\n\ndef problem16():\n    n = 1000\n    return sum([int(x) for x in str(2**n)])\n\nprint(problem16())\n%timeit problem16()\n\n1366\n10000 loops, best of 3: 123 µs per loop\n\n\n\ntable = {0: '', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five',\n         6: 'six', 7: 'seven', 8: 'eight', 9: 'nine', 10: 'ten', 11: 'eleven',\n         12: 'twelve', 13: 'thirteen', 14: 'fourteen', 15: 'fifteen',\n         16: 'sixteen', 17: 'seventeen', 18: 'eighteen', 19: 'nineteen',\n         20: 'twenty', 30: 'thirty', 40: 'forty', 50: 'fifty', 60: 'sixty',\n         70: 'seventy', 80: 'eighty', 90: 'ninety', 100: 'hundred',\n         1000: 'thousand'}\n\ndef word(n):\n    if n == 1000:\n        return table[1] + table[1000]\n    r = table[0]\n    if n &gt;= 100:\n        r += table[n//100] + table[100] \n        if n % 100 == 0:\n            return r\n        else:\n            r += 'and'\n            n = n % 100\n    if n &gt;= 20:\n        return r + table[(n//10)*10] + table[n%10]\n    return r + table[n]\n\ndef problem17():\n    return sum([len(word(n)) for n in range(1, 1001)])\n\nprint(problem17())\n%timeit problem17()\n\n21124\n1000 loops, best of 3: 1.84 ms per loop\n\n\n\n%%file pe/problem18.txt\n75\n95 64\n17 47 82\n18 35 87 10\n20 04 82 47 65\n19 01 23 75 03 34\n88 02 77 73 07 63 67\n99 65 04 28 06 16 70 92\n41 41 26 56 83 40 80 70 33\n41 48 72 33 47 32 37 16 94 29\n53 71 44 65 25 43 91 52 97 51 14\n70 11 33 28 77 73 17 78 39 68 17 57\n91 71 52 38 17 14 91 43 58 50 27 29 48\n63 66 04 68 89 53 67 30 73 16 69 87 40 31\n04 62 98 27 23 09 70 98 73 93 38 53 60 04 23\n\nOverwriting pe/problem18.txt\n\n\n\ndef problem18():\n    a = []\n    with open('pe/problem18.txt') as f:\n        for line in f:\n            a.append([int(x) for x in line.split()])\n    n = len(a)\n    for i in range(n-2, -1, -1):\n        for j in range(i+1):\n            a[i][j] = a[i][j] + max(a[i+1][j], a[i+1][j+1])\n    return a[0][0]\n\nprint(problem18())\n%timeit problem18()\n\n1074\nThe slowest run took 12.25 times longer than the fastest. This could mean that an intermediate result is being cached.\n1000 loops, best of 3: 271 µs per loop\n\n\n\nfrom datetime import date\n\ndef problem19():\n    c = 0\n    for y in range(1901, 2001):\n        for m in range(1, 13):\n            if date(y, m, 1).weekday() == 6:\n                c += 1\n    return c\n\nprint(problem19())\n%timeit problem19()\n\n171\n1000 loops, best of 3: 571 µs per loop\n\n\n\nfrom math import factorial\n\ndef problem20():\n    n = 100\n    return sum([int(x) for x in str(factorial(n))])\n\nprint(problem20())\n%timeit problem20()\n\n648\n10000 loops, best of 3: 55.2 µs per loop\n\n\n\ndef d(n):\n    if n == 1:\n        return 0\n    s = 1\n    squaren = int(n**0.5)\n    for i in range(2, squaren+1):\n        if n % i == 0:\n            s += (i + n // i)\n    if n == squaren ** 2:\n        s -= squaren\n    return s\n\ndef problem21():\n    n = 10000\n    s = 0\n    for i in range(2, n):\n        t = d(i)\n        if t != i and d(t) == i:\n            s += i\n    return s\n\nprint(problem21())\n%timeit problem21()\n\n31626\n1 loop, best of 3: 189 ms per loop\n\n\n\nfrom urllib.request import urlopen\nfrom contextlib import closing\n\nwith closing(urlopen('https://projecteuler.net/project/resources/p022_names.txt')) as u:\n    data = u.read()\nwith open('pe/problem22.txt', 'wb') as f:\n    f.write(data)\nprint('Writing pe/problem22.txt')\n\nWriting pe/problem22.txt\n\n\n\ndef value(name):\n    return sum([ord(x) - ord('A') + 1 for x in name])\n\ndef problem22():\n    with open('pe/problem22.txt') as f:\n        a = f.readline().strip('\"').split('\",\"')\n    a.sort()\n    s = 0\n    for i in range(len(a)):\n        s += ((i + 1) * value(a[i]))\n    return s\n\nprint(problem22())\n%timeit problem22()\n\n871198282\n10 loops, best of 3: 19.5 ms per loop\n\n\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef is_abundant(n):\n    if n == 1:\n        return 0\n    s = 1\n    squaren = int(n**0.5)\n    for i in range(2, squaren+1):\n        if n % i == 0:\n            s += (i + n // i)\n    if n == squaren ** 2:\n        s -= squaren\n    if s &gt; n:\n        return 1\n    return 0\n\n@jit\ndef problem23():\n    n = 28123\n    a = []\n    for i in range(1, n):\n        if is_abundant(i):\n            a.append(i)\n    newa = np.empty(len(a), dtype=np.int32)\n    for i in range(len(a)):\n        newa[i] = a[i]\n    b = np.zeros(n+1, dtype=np.int8)\n    for i in range(newa.size):\n        for j in range(i, newa.size):\n            if newa[i] + newa[j] &gt; n:\n                break\n            b[newa[i] + newa[j]] = 1\n    s = 0\n    for i in range(1, n+1):\n        if b[i] == 0:\n            s += i\n    return s\n\nprint(problem23())\n%timeit problem23()\n\n4179871\n10 loops, best of 3: 63.9 ms per loop\n\n\n\nfrom math import factorial\n\ndef problem24():\n    n = 1000000\n    a = []\n    b = set(range(10))\n    for i in range(9, -1, -1):\n        c = sorted(b)\n        t = (n-1) // factorial(i)\n        a.append(c[t])\n        b.remove(c[t])\n        n -= (t * factorial(i))\n    return ''.join([str(x) for x in a])\n                \nprint(problem24())\n%timeit problem24()\n\n2783915460\n10000 loops, best of 3: 25.8 µs per loop\n\n\n\nfrom math import log10\n\ndef problem25():\n    n = 1000\n    a, b = 1, 1\n    c = 2\n    while int(log10(b)) + 1 &lt; n:\n        a, b = b, a+b\n        c += 1\n    return c\n\nprint(problem25())\n%timeit problem25()\n\n4782\n100 loops, best of 3: 3.9 ms per loop\n\n\n\nfrom numba import jit\n\n@jit\ndef length_cycle(n):\n    a = [1]\n    i = 1\n    while True:\n        i = (i * 10) % n\n        if i in a:\n            return len(a) - a.index(i)\n        a.append(i)\n\n@jit\ndef problem26():\n    n = 1000\n    l = 0\n    m = 1\n    for i in range(2, n):\n        t = length_cycle(i)\n        if t &gt; l:\n            l = t\n            m = i\n    return m\n\nprint(problem26())\n%timeit problem26()\n\n983\n10 loops, best of 3: 15.7 ms per loop\n\n\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef sieveEratosthenes(n):\n    s = np.ones(n+1)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n\n@jit\ndef problem27():\n    n = 1000\n    sieve = sieveEratosthenes(2*(n-1)**2 + n-1)\n    result = 0\n    max_length = 0\n    min_a = -((n // 2) * 2 - 1)\n    max_a = (n // 2) * 2 + 1\n    for b in range(n):\n        if sieve[b] == 1:\n            for a in range(min_a, max_a, 2):\n                l = 0\n                p = b\n                while p &gt;= 0 and sieve[p] == 1:\n                    l += 1\n                    p = l**2 + a*l + b\n                if l &gt; max_length:\n                    max_length = l\n                    result = a * b\n    return result\n    \nprint(problem27())\n%timeit problem27()\n\n-59231\n10 loops, best of 3: 63.6 ms per loop\n\n\n\ndef problem28():\n    n = 1001\n    s = 1\n    for i in range(3, n+1, 2):\n        a1 = i**2\n        a2 = a1 - i + 1\n        a3 = a2 - i + 1\n        a4 = a3 - i + 1\n        s += (a1 + a2 + a3 + a4)\n    return s\n\nprint(problem28())\n%timeit problem28()\n\n669171001\n1000 loops, best of 3: 463 µs per loop\n\n\n\ndef problem29():\n    n = 100\n    s = set()\n    for a in range(2, n+1):\n        for b in range(2, n+1):\n            s.add(a**b)\n    return len(s)\n\nprint(problem29())\n%timeit problem29()\n\n9183\n100 loops, best of 3: 11.3 ms per loop\n\n\n\nfrom itertools import combinations_with_replacement\n\ndef problem30():\n    n = 5\n    s = 0\n    for i in range(2, n+2):\n        for com in combinations_with_replacement(range(10), i):\n            t = sum([x**n for x in com])\n            if sorted(com) == sorted([int(x) for x in str(t)]):\n                s += t\n    return s\n\nprint(problem30())\n%timeit problem30()\n\n443839\n10 loops, best of 3: 82.5 ms per loop\n\n\n\ndef check(n, coins):\n    if n == 0 or coins[0] == 1: \n        return 1\n    c = 0\n    for i in range(len(coins)):\n        m = n - coins[i]\n        if m &gt;= 0:\n            c += check(m, coins[i:])\n    return c\n\ndef problem31():\n    n = 200\n    coins = [200, 100, 50, 20, 10, 5, 2, 1]\n    return check(n, coins)\n\nprint(problem31())\n%timeit problem31()\n\n73682\n1 loop, best of 3: 138 ms per loop\n\n\n\ndef problem32():\n    s = set()\n    for i in range(10, 100):\n        for j in range(100, 1000 // i + 1):\n            p = i * j\n            if ''.join(sorted(str(i) + str(j) + str(p))) == '123456789':\n                s.add(p)\n    for i in range(1, 10):\n        for j in range(1000, 10000 // i + 1):\n            p = i * j\n            if ''.join(sorted(str(i) + str(j) + str(p))) == '123456789':\n                s.add(p)\n    return sum(s)\n\nprint(problem32())\n%timeit problem32()\n\n14804\n10 loops, best of 3: 60.4 ms per loop\n\n\n\ndef common_divisor(a, b):\n    while a != 0:\n        a, b = b%a, a\n    return b\n\ndef problem33():\n    p, q = 1, 1\n    for i in range(10, 100):\n        i1 = int(str(i)[0])\n        i2 = int(str(i)[1])\n        if i % 10 == 0 or i1 == i2:\n            continue\n        for j in range(i+1, 100):\n            j1 = int(str(j)[0])\n            j2 = int(str(j)[1])\n            if j % 10 == 0 or j1 == j2:\n                continue\n            if ((i1 == j2 and i * j1 == j * i2) or\n                (i2 == j1 and i * j2 == j * i1)):\n                p *= i\n                q *= j\n    return q // common_divisor(p, q)\n\nprint(problem33())\n%timeit problem33()\n\n100\n100 loops, best of 3: 5.37 ms per loop\n\n\n\nfrom math import factorial\nfrom itertools import combinations_with_replacement\n\ndef problem34():\n    s = 0\n    max_digits = 7\n    for i in range(2, max_digits+1):\n        for com in combinations_with_replacement(range(10), i):\n            t = sum([factorial(x) for x in com])\n            if sorted(com) == sorted([int(x) for x in str(t)]):\n                s += t\n    return s\n\nprint(problem34())\n%timeit problem34()\n\n40730\n10 loops, best of 3: 150 ms per loop\n\n\n\nfrom numba import jit\n\n@jit\ndef sieveEratosthenes(n):\n    s = [1] * (n + 1)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n\ndef problem35():\n    n = 1000000\n    test_list = ['0', '2', '4', '5', '6', '8']\n    sieve = sieveEratosthenes(n-1)\n    c = 4\n    for p in range(10, n):\n        if sieve[p] == 1:\n            str_p = str(p)\n            flag = 1\n            for x in test_list:\n                if x in str_p:\n                    flag = 0\n                    break\n            if flag == 0:\n                continue\n            for i in range(1, len(str_p)):\n                p1 = int(''.join(str_p[i:] + str_p[:i]))\n                if sieve[p1] == 0:\n                    flag = 0\n                    break\n            if flag == 1:\n                c += 1\n    return c\n\nprint(problem35())\n%timeit problem35()\n\n55\n1 loop, best of 3: 222 ms per loop\n\n\n\ndef problem36():\n    s = 0\n    for i in range(1, 100000):\n        str_i = str(i)\n        if str_i == str_i[::-1]:\n            str_bin_i = bin(i)[2:]\n            if str_bin_i == str_bin_i[::-1]:\n                s += i\n    for j in range(100, 1000):\n        i = int(str(j) + str(j)[::-1])\n        str_bin_i = bin(i)[2:]\n        if str_bin_i == str_bin_i[::-1]:\n                s += i\n    return s\n\nprint(problem36())\n%timeit problem36()\n\n872187\n10 loops, best of 3: 71.8 ms per loop\n\n\n\ndef isPrime(n):\n    if n == 2:\n        return True\n    elif n &lt; 2 or n % 2 == 0:\n        return False\n    k = int(n**0.5)\n    i = 3\n    while i &lt;= k:\n        if n % i == 0:\n            return False\n        i += 2\n    return True\n\ndef isLeftRightTruncatePrime(n):\n    if n &lt; 10:\n        return False\n    k = n\n    while k &gt; 0:\n        if not isPrime(k):\n            return False\n        k = k // 10\n    k = n\n    while k &gt;= 10:\n        k = int(str(k)[1:])\n        if not isPrime(k):\n            return False\n    return True\n    \ndef problem37():\n    s = 0\n    count = 0\n    for i in [23, 37, 53, 73, 373, 313, 3173]:\n        if isLeftRightTruncatePrime(i) == 1:\n            s += i\n            count += 1\n    first = [\"31\", \"7\", \"37\"]\n    second = [\"\"]\n    third = [\"7\", \"373\", \"13\"]\n    while count &lt; 11:\n        for i in first:\n            for j in second:\n                for k in third:\n                    n = int(i+j+k)\n                    if isLeftRightTruncatePrime(n):\n                        s += n\n                        count += 1\n        term = [j+\"3\" for j in second] + [j+\"9\" for j in second]\n        second = term[:]\n    return s\n    \nprint(problem37())\n%timeit problem37()\n\n748317\n10 loops, best of 3: 36.5 ms per loop\n\n\n\ndef isPandigital(n):\n    return len(str(n)) == 9 and set(str(n)) == set(\"123456789\")\n\ndef problem38():\n    l = []\n    for i in range(1, 10):\n        s = \"\"\n        for j in range(1, 10):\n            s += str(i*j)\n        if isPandigital(s):\n            l.append(int(s))\n    for i in range(10, 100):\n        s = \"\"\n        for j in range(1, 5):\n            s += str(i*j)\n        if isPandigital(s):\n            l.append(int(s))\n    for i in range(100, 1000):\n        s = \"\"\n        for j in range(1, 4):\n            s += str(i*j)\n        if isPandigital(s):\n            l.append(int(s))\n    for i in range(1000, 10000):\n        s = \"\"\n        for j in range(1, 3):\n            s += str(i*j)\n        if isPandigital(s):\n            l.append(int(s))\n    return max(l)\n\nprint(problem38())\n%timeit problem38()\n\n932718654\n10 loops, best of 3: 31 ms per loop\n\n\n\ndef problem39():\n    d = dict()\n    for i in range(1, 500):\n        for j in range(1, 500):\n            k2 = i**2 + j**2\n            k = int(k2**0.5)\n            if k2 == k**2:\n                p = i + j + k\n                if p in d:\n                    d[i+j+k] += 1\n                else:\n                    d[i+j+k] = 0\n    return max(d, key=d.get)\n\nprint(problem39())\n%timeit problem39()\n\n840\n1 loop, best of 3: 489 ms per loop\n\n\n\ndef findDigit(n):\n    i = 1\n    s = 0\n    while i &lt;= n:\n        s += len(str(i))\n        if s &gt;= n:\n            break\n        i += 1\n    return int(str(i)[-(s - n + 1)])\n\ndef problem40():\n    s = 1\n    for i in range(0, 7):\n        s *= findDigit(10**i)\n    return s\n\nprint(problem40())\n%timeit problem40()\n\n210\n10 loops, best of 3: 173 ms per loop\n\n\n\nfrom itertools import permutations\n\ndef isPrime(n):\n    if n == 2:\n        return True\n    elif n &lt; 2 or n % 2 == 0:\n        return False\n    k = int(n**0.5)\n    i = 3\n    while i &lt;= k:\n        if n % i == 0:\n            return False\n        i += 2\n    return True\n\ndef problem41():\n    pandigital_prime = []\n    for i in [4, 7]:\n        s = \"\"\n        for j in range(1, i+1):\n            s += str(j)\n        for item in permutations(s, i):\n            n = int(''.join(item))\n            if isPrime(n):\n                pandigital_prime.append(n)\n    return max(pandigital_prime)\n\nprint(problem41())\n%timeit problem41()\n\n7652413\n1 loop, best of 3: 232 ms per loop\n\n\n\nfrom urllib.request import urlopen\nfrom contextlib import closing\n\nwith closing(urlopen('https://projecteuler.net/project/resources/p042_words.txt')) as u:\n    data = u.read()\nwith open('pe/problem42.txt', 'wb') as f:\n    f.write(data)\nprint('Writing pe/problem42.txt')\n\nWriting pe/problem42.txt\n\n\n\ndef problem42():\n    with open('pe/problem42.txt') as f:\n        listWords = f.readline().strip('\"').split('\",\"')\n    maxN = int((max([len(x) for x in listWords]) * 26)**0.5)\n    listTriangle = [i*(i+1)//2 for i in range(1, maxN)]\n    c = 0\n    for word in listWords:\n        value = sum([ord(x) - ord('A') + 1 for x in word])\n        if value in listTriangle:\n            c += 1\n    return c\n\nprint(problem42())\n%timeit problem42()\n\n162\n100 loops, best of 3: 7.27 ms per loop\n\n\n\nfrom itertools import permutations\n\ndef problem43():\n    s = 0\n    for i in range(6, 999//17 + 1):\n        d8910 = list(str(i * 17))\n        if '5' not in d8910 and len(set(d8910)) == 3:\n            remain = list('012346789')\n            for x in d8910:\n                remain.remove(x)\n            for n in permutations(remain, 6):\n                if n[0] != '0':\n                    n = list(n)\n                    n.insert(5, '5')\n                    n.extend(d8910)\n                    primes = [2, 3, 5, 7, 11, 13, 17]\n                    flag = True\n                    for i in range(7):\n                        if int(''.join(n[i+1:i+4])) % primes[i] != 0:\n                            flag = False\n                            break\n                    if flag == True:\n                        s += int(''.join(n))\n    return s\n\nprint(problem43())\n%timeit problem43()\n\n16695334890\n10 loops, best of 3: 91 ms per loop\n\n\n\n# not so right answer, solve the problem of minimizing the max of P_k and P_j\nfrom numba import jit\n\n@jit\ndef isPentagonal(n):\n    return ((24*n+1)**0.5+1)/6 % 1 == 0\n\n@jit\ndef problem44():\n    n = 2\n    listPentagonal = [1]\n    diff = []\n    while True:\n        listPentagonal.append(n*(3*n-1)/2)\n        diff.append(0)\n        inc = 3*n-2\n        for i in range(n-1):\n            diff[i] += inc\n            if isPentagonal(diff[i]) and isPentagonal(listPentagonal[i] + listPentagonal[n-1]):\n                return diff[i]\n        n += 1\n        \nprint(problem44())\n%timeit problem44()\n\n5482660\n10 loops, best of 3: 81.2 ms per loop\n\n\n\ndef isPentagonal(n):\n    return ((24*n+1)**0.5+1)/6 % 1 == 0\n\ndef isHexagonal(n):\n    return ((8*n+1)**0.5+1)/4 % 1 == 0\n\ndef problem45():\n    n = 286\n    while True:\n        t = n*(n+1)//2\n        if isHexagonal(t) and isPentagonal(t):\n            return t\n        n += 1\n        \nprint(problem45())\n%timeit problem44()\n\n1533776805\n10 loops, best of 3: 86.8 ms per loop\n\n\n\ndef isPrime(n):\n    if n == 2:\n        return True\n    elif n &lt; 2 or n % 2 == 0:\n        return False\n    k = int(n**0.5)\n    i = 3\n    while i &lt;= k:\n        if n % i == 0:\n            return False\n        i += 2\n    return True\n\ndef problem46():\n    n = 35\n    listOddPrime = [3, 5, 7, 11, 13, 17, 19, 23, 29, 31]\n    while True:\n        if isPrime(n):\n            listOddPrime.append(n)\n        else:\n            flag = False\n            for x in listOddPrime:\n                if ((n-x)/2)**0.5 % 1 == 0:\n                    flag = True\n            if flag == False:\n                return n\n        n += 2\n        \nprint(problem46())\n%timeit problem46()\n\n5777\n1 loop, best of 3: 410 ms per loop\n\n\n\nfrom numba import jit\n\n@jit\ndef check(n):\n    s = 0\n    if n % 2 == 0:\n        s = 1\n        n = n // 2\n        while n % 2 == 0:\n            n = n // 2\n    i = 3\n    while i*i &lt;= n and s&lt;4:\n        if n % i == 0:\n            s += 1\n            n = n // i\n            while n % i == 0:\n                n = n // i\n        i += 2\n    if (n&gt;1 and s==3) or (n==1 and s==4):\n        return True\n    return False\n\n@jit\ndef problem47():\n    a,b,c,d = (False, False, False, False)\n    n = 1\n    while True:\n        a,b,c,d = b,c,d,check(n+3) \n        if a and b and c and d:\n            return n\n        n += 1\n\nprint(problem47())\n%timeit problem47()\n\n134043\n10 loops, best of 3: 77.3 ms per loop\n\n\n\ndef problem48():\n    s = 0\n    for i in range(1, 1001):\n        s += (i**i)%(10**10)\n    return s % (10**10)\n\nprint(problem48())\n%timeit problem48()\n\n9110846700\n100 loops, best of 3: 13.8 ms per loop\n\n\n\nimport numpy as np\nfrom itertools import permutations\n\ndef sieveEratosthenes(n):\n    s = np.ones(n+1, dtype=np.int8)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n    \ndef checkCondition(n, sieve):\n    listPermutation = [int(''.join(x)) for x in permutations(str(n))]\n    listPrime = sorted([x for x in listPermutation if sieve[x] == 1 and x != n])\n    for i in range(len(listPrime) - 1):\n        for j in range(i+1, len(listPrime)):\n            if 2*listPrime[i] == n + listPrime[j]:\n                return True\n    return False\n\ndef problem49():\n    sieve = sieveEratosthenes(10000)\n    for n in range(1001,10000):\n        if sieve[n] == 1 and checkCondition(n, sieve):\n            if n != 1487:\n                return n\n\nprint(problem49())\n%timeit problem49()\n\n2969\n10 loops, best of 3: 31.7 ms per loop\n\n\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef sieveEratosthenes(n):\n    s = np.ones(n+1, dtype=np.int8)\n    s[0] = 0\n    s[1] = 0\n    for i in range(2, int(n**0.5)+1):\n        if s[i] == 1:\n            for j in range(2, (n//i)+1):\n                s[i*j] = 0\n    return s\n\n@jit\ndef problem50():\n    sieve = sieveEratosthenes(10**6)\n    lenPrime = 0\n    for i in range(len(sieve)):\n        if sieve[i] == 1:\n            lenPrime += 1\n    listPrime = np.zeros(lenPrime, dtype=np.int32)\n    j = 0\n    for x in range(len(sieve)):\n        if sieve[x] == 1:\n            listPrime[j] = x\n            j += 1\n    maxL = 21\n    r = 953\n    for i in range(len(listPrime) - 21):\n        s = listPrime[i]\n        for j in range(i+1, i+22):\n            s += listPrime[j]\n        c = 22\n        while s &lt; 10**6 and (i+c &lt; len(listPrime)):\n            if sieve[s] == 1 and c &gt; maxL:\n                maxL = c\n                r = s\n            s += listPrime[i+c]\n            c += 1\n    return r            \n    \nprint(problem50())\n%timeit problem50()\n\n997651\n100 loops, best of 3: 9.99 ms per loop\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/rudin-complex-analysis.html",
    "href": "blog/rudin-complex-analysis.html",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "",
    "text": "The following notebook contains some solutions to the complex analysis part of the Big Rudin book that I studied at POSTECH. This post is also a chance for me to test the different between MathJax and KaTeX in Nikola, to see which one has better render. It turns out that KaTeX is much faster than MathJax. As a note, to make KaTeX work with inline mode from Jupyter notebook, we have to change the default auto-render code in the theme (as suggested in this file, the renderMathInElement part)."
  },
  {
    "objectID": "blog/rudin-complex-analysis.html#chapter-10---elementary-properties-of-holomorphic-functions",
    "href": "blog/rudin-complex-analysis.html#chapter-10---elementary-properties-of-holomorphic-functions",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "Chapter 10 - Elementary Properties of Holomorphic Functions",
    "text": "Chapter 10 - Elementary Properties of Holomorphic Functions\n1. The following fact was tacitly used in this chapter: If \\(A\\) and \\(B\\) are disjoint subsets of the plane, if \\(A\\) is compact, and if \\(B\\) is closed, then there exists a \\(\\delta &gt; 0\\) such that \\(|\\alpha - \\beta| \\geq \\delta\\) for all \\(\\alpha \\in A\\) and \\(\\beta \\in B\\). Prove this, with an arbitrary metric space in place of the plane.\nProof. Let \\(A\\) be a compact set and \\(B\\) be a closed set in a metric space such that \\(A\\cap B = \\varnothing\\). Let \\(\\delta = \\inf_{\\alpha,\\beta}d(\\alpha,\\beta)\\) where the infimum is taken with all \\(\\alpha \\in A\\) and \\(\\beta\\in B\\), and let \\(\\{a_n\\}\\), \\(\\{b_n\\}\\) be two sequences in \\(A\\) and \\(B\\) correspondingly such that \\(\\lim_{n\\to\\infty}d(a_n,b_n) = \\delta\\). Suppose that \\(\\delta = 0\\). Because \\(A\\) is compact, there exists \\(c\\in A\\) and a subsequence \\(\\{a_{n_k}\\}\\) of \\(\\{a_n\\}\\) such that \\(\\lim_{k\\to\\infty}d(a_{n_k},c) = 0\\). Hence \\(\\lim_{k\\to\\infty}d(b_{n_k},c) = 0\\) because \\(\\lim_{k\\to\\infty}d(a_{n_k},b_{n_k}) = 0\\). Hence \\(c \\in \\bar{B}=B\\), which is a contradiction with the hypothesis that \\(A\\cap B = \\varnothing\\). So \\(\\delta &gt; 0\\). We get the conclusion. \\(\\Box\\) \n2. Suppose that \\(f\\) is an entire function, and that in every power series \\[\nf(z) = \\sum_{n=0}^{\\infty} c_n(z-a)^n\n\\] at least one coefficient is \\(0\\). Prove that \\(f\\) is a polynomial.\nHint. \\(n!c_n = f^{(n)}(a)\\).\nProof. Let \\(a\\in \\mathbb{C}\\). Consider the power series of \\(f\\) at \\(a\\), we have \\[\nf(z) = \\sum\\limits_{n=0}^{\\infty}c_n(z-a)^n.\n\\]\nBy assumption, \\(c_n = 0\\) for some \\(n\\leq 0\\). By \\(n!c_n = f^{(n)}(a)\\), we get \\(f^{(n)}(a) = 0\\) for some \\(n\\leq 0\\). Put \\(K = \\{z:f^{(n)}(z) = 0\\text{ for some }n\\leq 0\\}\\), we have \\(a \\in K\\). This is true for every \\(a\\in \\mathbb{C}\\). So \\(K = \\mathbb{C}\\). Moreover, we have \\[\nK = \\bigcup\\limits_{n=0}^{\\infty}\\{z:f^{(n)}(z) = 0\\}.\n\\]\nIf, for each \\(n\\), we do not have \\(f^{(n)}(z) = 0\\) for every \\(z\\in \\mathbb{C}\\), then \\(\\{z:f^{(n)}(z) = 0\\}\\) is a countable set for all \\(n\\). Hence \\(K\\) is a countable set, which is a contradiction to \\(K = \\mathbb{C}\\). So \\(f^{(N)} = 0\\) for some \\(N\\geq 0\\). Consider the power series of \\(f\\) at \\(0\\), we have \\[\nf(z) = \\sum\\limits_{n=0}^{\\infty}b_n z^n.\n\\]\nBy \\(f^{(n)}(0)=0\\) for all \\(n\\geq N\\), we have \\(b_n = f^{(n)}(0)/n! = 0\\) for all \\(n\\geq N\\). We get the conclusion. \\(\\Box\\)\n3. Suppose \\(f\\) and \\(g\\) are entire functions, and \\(|f(z)| \\leq |g(z)|\\) for every \\(z\\). What conclusion can you draw?\nProof. \\(f = Cg\\), where \\(|C| \\leq 1\\).\nIf \\(g\\) is not vanishing in \\(\\mathbb{C}\\), then \\(Z(g)\\) has no limit point. Put \\(h = f/g\\), then \\(h \\in \\mathbb{C}\\backslash Z(g)\\) and \\(h\\) is a meromorphic function in \\(\\mathbb{C}\\). Let \\(a\\in Z(g)\\). By assumption, \\(|h|\\leq 1\\) in a deleted disc \\(D'(a,r)\\) for some \\(r &gt;0\\) such that \\(g\\neq 0\\) in \\(D'(a,r)\\). So \\(h\\) has a removable singularity at \\(a\\). This is true for all \\(a\\in Z(g)\\), hence \\(h\\in H(\\mathbb{C})\\) and \\(|h|\\leq 1\\) in \\(\\mathbb{C}\\). By Liouville’s theorem, \\(h\\) is a constant \\(C\\) in \\(\\mathbb{C}\\). So we get \\(f = Cg\\) and it is clear that \\(|C| \\leq 1\\). For the case \\(g =0\\) in \\(\\mathbb{C}\\), we get \\(f = 0\\) in \\(\\mathbb{C}\\), hence \\(f= Cg\\) with \\(C = 0\\). Reversely, if \\(f=Cg\\) and \\(|C|\\leq 1\\), then \\(|f|\\leq |C||g| \\leq |g|\\). \\(\\Box\\)\n7. If \\(f\\in H(\\Omega)\\), the Cauchy formula for the derivatives of \\(f\\), \\[\nf^{(n)}(z) = \\frac{n!}{2\\pi i}\\int_{\\Gamma}\\frac{f(\\zeta)}{(\\zeta - z)^{n+1}}\\,d\\zeta\\qquad (n=1,2,3,\\ldots)\n\\] is valid under certain conditions on \\(z\\) and \\(\\Gamma\\). State these, and prove the formula.\nProof. \\(\\Gamma\\) is a cycle in \\(\\Omega\\), \\(\\mathrm{Ind}_{\\Gamma}(a)=0\\) for all \\(a\\notin \\Omega\\), \\(z\\in \\Omega\\backslash \\Gamma\\), and \\(\\mathrm{Ind}_{\\Gamma}(z) = 1\\).\nBy Cauchy’s formula, with the above conditions, we have \\[\nf(w) = \\frac{1}{2\\pi i}\\int_{\\Gamma}\\frac{f(\\zeta)}{\\zeta-w}\\,d\\zeta\\qquad (w\\in\\Omega\\backslash \\Gamma).\n\\]\nLet \\(r &gt; 0\\) such that \\(D(z;r)\\subset \\Omega\\backslash\\Gamma\\). For \\(w \\in D'(z;r/2)\\), we have \\[\n\\frac{f(w)-f(z)}{w-z} = \\frac{1}{2\\pi i(w-z)}\\int_{\\Gamma}f(\\zeta)\\left(\\frac{1}{\\zeta-w} - \\frac{1}{\\zeta - z}\\right)\\,d\\zeta.\n\\]\nLet \\(w\\to z\\), we get \\[\n\\lim_{w\\to z}\\frac{1}{w-z}\\left(\\frac{1}{\\zeta-w} - \\frac{1}{\\zeta - z}\\right) = \\left(\\frac{1}{\\zeta - z}\\right)' = \\frac{1}{(\\zeta - z)^2}.\n\\]\nMoreover, for all \\(w \\in D'(z;r/2)\\), we have \\[\n\\left|\\frac{f(\\zeta)}{w-z}\\left(\\frac{1}{\\zeta-w} - \\frac{1}{\\zeta - z}\\right)\\right| = \\left|\\frac{f(\\zeta)}{(\\zeta-w)(\\zeta-z)}\\right|\\leq \\frac{4}{r^2} \\|f \\|_{\\Gamma}\\qquad(\\zeta \\in \\Gamma).\n\\]\nAn easy argument by applying the dominated convergence theorem shows that \\[\nf'(z) = \\lim_{w\\to z}\\frac{f(w)-f(z)}{w-z} =\\frac{1}{2\\pi i}\\int_{\\Gamma}\\frac{f(\\zeta)}{(\\zeta-z)^2}\\,d\\zeta.\n\\]\nWe have proved the required formula for the case \\(n=1\\). Note that \\[\n\\lim_{w\\to z}\\frac{1}{w-z}\\left(\\frac{1}{(\\zeta-w)^n} - \\frac{1}{(\\zeta - z)^n}\\right) = \\left(\\frac{1}{(\\zeta - z)^n}\\right)' = \\frac{1}{(\\zeta - z)^{n+1}}\n\\] and for all \\(w \\in D'(z;r/2)\\), we have \\[\n\\begin{aligned}\n\\left|\\frac{f(\\zeta)}{w-z}\\left(\\frac{1}{(\\zeta-w)^n} - \\frac{1}{(\\zeta - z)^n}\\right)\\right| &= \\left|\\frac{f(\\zeta)\\sum_{i=0}^{n-1}(\\zeta-w)^i(\\zeta-z)^{n-i-1}}{(\\zeta-w)^n(\\zeta -z)^n}\\right| \\\\\n&=\\left|f(\\zeta)\\sum\\limits_{i=0}^{n-1}\\frac{1}{(\\zeta-w)^{n-i}(\\zeta-z)^{i}}\\right|\\\\\n&\\leq \\frac{2^n n}{r^n}\\|f\\|_{\\Gamma}\\qquad (\\zeta\\in \\Gamma).\n\\end{aligned}\n\\]\nBy an induction procedure, the above notes, and a similar argument as the case \\(n=1\\), we can prove the required formula for arbitrary \\(n \\geq 1\\). \\(\\Box\\)\n19. Suppose \\(f\\in H(U)\\), \\(g\\in H(U)\\), and neither \\(f\\) nor \\(g\\) has a zero in \\(U\\). If \\[\n\\frac{f'}{f}\\left(\\frac{1}{n}\\right) = \\frac{g'}{g}\\left(\\frac{1}{n}\\right)\\qquad (n=2,3,\\ldots),\n\\] find another simple relation between \\(f\\) and \\(g\\).\nProof. \\(f=Cg\\), where \\(C\\neq 0\\).\nPut \\(h = f/g\\). It is clear that \\(h\\in H(U)\\). Moreover, by \\[\nh'(z) = \\frac{f'(z)g(z) - f(z)g'(z)}{g^2(z)},\n\\] we get \\(h'(\\frac{1}{n})=0\\) for \\(n=2,3,\\ldots\\) Because \\(\\frac{1}{n}\\to 0\\) as \\(n\\to \\infty\\), we get \\(h'(z) = 0\\) for every \\(z\\in U\\). So \\(h(z) = h(0) + \\int_{[0,z]}h'(w)\\,dw = h(0)\\) for all \\(z \\in \\mathbb{C}\\). Put \\(C = h(0)=f(0)/g(0)\\neq 0\\), we get \\(f = Cg\\). Reversely, if \\(f = Cg\\) and \\(C\\neq 0\\), we get \\(f'(z)/f(z) = g'(z)/g(z)\\) for all \\(z\\in U\\), in particularly, for \\(\\frac{1}{n}\\), \\(n=2,3,\\ldots\\) \\(\\Box\\)\n21. Suppose \\(f\\in H(\\Omega)\\), \\(\\Omega\\) contains the closed unit disc, and \\(|f(z)| &lt; 1\\) if \\(|z|=1\\). How many fixed points must \\(f\\) have in the disc? That is, how many solutions does the equation \\(f(z)=z\\) have there?\nProof. 1 solution.\nLet \\(g\\) and \\(h\\) be two functions in \\(\\Omega\\) defined by \\(g(z) = f(z) - z\\) and \\(h(z) = -z\\) for every \\(z\\in \\Omega\\). By assumption, $ |g(z) - h(z)| &lt; |h(z)| $ for all \\(z\\) in the circle \\(\\{z:|z|=1\\}\\). By Rouché’s theorem, we get the number of solutions of \\(g\\) in \\(D(0;1)\\) equals to the number of solutions of \\(h\\) in \\(D(0;1)\\), which contains just one solution \\(z=0\\). We get the conclusion. \\(\\Box\\)"
  },
  {
    "objectID": "blog/rudin-complex-analysis.html#chapter-11---harmonic-functions",
    "href": "blog/rudin-complex-analysis.html#chapter-11---harmonic-functions",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "Chapter 11 - Harmonic Functions",
    "text": "Chapter 11 - Harmonic Functions\n1. Suppose \\(u\\) and \\(v\\) are real harmonic functions in a plane region \\(\\Omega\\). Under what conditions is \\(uv\\) harmonic? (Note that the answer depends strongly on the fact that the question is one about real functions.) Show that \\(u^2\\) cannot be harmonic in \\(\\Omega\\), unless \\(u\\) is constant. For which \\(f\\in H(\\Omega)\\) is \\(|f|^2\\) harmonic?\nProof. \\(u\\) is constant or \\(v\\) is constant or there is some real constant \\(C\\neq 0\\) such that \\(u_x = Cv_y\\), \\(u_y = -Cv_x\\) (in other words, \\(u+iCv\\) is holomorphic). \\(f\\) is constant.\nNote that every harmonic function has continuous partial derivatives of all orders. Suppose \\(uv\\) is harmonic, which means \\((uv)_{xx}+(uv)_{yy}=0\\) in \\(\\Omega\\), hence \\(u_xv_x + u_yv_y = 0\\) in \\(\\Omega\\) (because \\(u_{xx}+u_{yy}=0\\) and \\(v_{xx}+v_{yy}=0\\)). Suppose more that both \\(u\\) and \\(v\\) are not constant. Put \\(g = u_x - iu_y\\) and \\(h = v_y + iv_x\\). It is plain that \\(g\\) and \\(h\\) are holomorphic in \\(\\Omega\\) by checking their Cauchy-Riemann equations. If \\(Z(h) = \\Omega\\), then \\(v_x=v_y=0\\) in \\(\\Omega\\). Let \\(D\\) be any disc in \\(\\Omega\\) such that \\(\\bar{D}\\subset \\Omega\\). Then \\(v\\) is a real part of a holomorphic function \\(V\\) in \\(D\\). By \\(V' = v_x - iv_y\\), we have \\(V'=0\\) in \\(D\\), hence \\(V\\) is constant in \\(D\\) (for any \\(z,w\\) in \\(D\\), we have \\(V(z)-V(w) = \\int_{[z,w]}V'(\\zeta)\\,d\\zeta=0\\)), hence \\(v = \\mathrm{const}\\) in \\(D\\). This argument shows that for any \\(z\\in \\Omega\\), \\(v\\) is constant in some neighborhood of \\(z\\). Pick \\(z_0 \\in \\Omega\\). Now, it is easy to see that the set \\(\\{z\\in \\Omega: v(z) = v(z_0)\\}\\) is both closed and open in \\(\\Omega\\), hence is exactly \\(\\Omega\\) by the connectedness of \\(\\Omega\\). This means that \\(v\\) is constant in \\(\\Omega\\), a contradiction with our assumption. So \\(Z(h)\\) must have no limit point in \\(\\Omega\\). Put \\(k = g/h\\), we have \\(k\\) is a meromorphic function in \\(\\Omega\\). But \\[\nk = \\frac{g}{h}=\\frac{u_x - iu_y}{v_y+iv_x} = \\frac{u_xv_y - u_yv_x}{v_x^2 + v_y^2}\\in \\mathbb{R}. \\qquad (*)\n\\]\nThere does not exist any nonempty open subset of \\(\\mathbb{R}\\). So \\(k\\) is constant in each disc in which \\(g\\) has no zero (by the open mapping theorem). Now, fix \\(a\\in Z(h)\\). Let \\(r&gt;0\\) such that \\(D(0;r)\\subset \\Omega\\) and \\(h(z)\\neq 0\\) for every \\(z\\) in \\(D'(0;r)\\). Then we have \\(k\\) is constant in each of four discs \\(D(a+r/2;r/2)\\), \\(D(a+ir/2;r/2)\\), \\(D(a-r/2;r/2)\\), \\(D(a-ir/2;r/2)\\). Because the first disc intersects with the second, the second intersects with the third, and the third intersects with the fourth, we conclude that \\(k\\) is constant in the union of these four discs, hence is constant in \\(D(a;r/2)\\) because \\(D(a;r/2)\\) is a subset of this union. So \\(a\\) is a removable singularity and \\(k\\) is constant in the neighborhood \\(D(a;r/2)\\) of \\(a\\). By the connectedness of \\(\\Omega\\), we get \\(k\\) is constant in \\(\\Omega\\). Denote this constant by \\(C\\). By \\((*)\\), \\(C\\in \\mathbb{R}\\), \\(C\\neq 0\\) (if not, \\(u_x=u_y=0\\) in \\(\\Omega\\), hence \\(u\\) is constant), and \\(u_x = Cv_y\\), \\(u_y = -Cv_x\\). We get the answer for the first question. Now, if \\(u^2\\) is harmornic in \\(\\Omega\\), then, as above, we have \\(u\\) is constant or \\(u_x = Cu_y\\), \\(u_y = -Cu_x\\) for some real constant \\(C\\neq 0\\). The second case leads to \\(u_x = u_y = 0\\) in \\(\\Omega\\), which also implies \\(u\\) is constant. We affirm the second statement. (A simpler way to get this affirmation is that we take the Laplacian of \\(u^2\\) and get \\(u_x = u_y =0\\).) For the last question, put \\(f = u + iv\\). Because \\(f\\in H(\\Omega)\\), we have \\(u\\) and \\(v\\) are harmonic in \\(\\Omega\\). Now, \\(|f|^2\\) is harmonic iff \\(u^2 + v^2\\) is harmonic. Take the Laplacian of \\(u^2+v^2\\), we get \\(u_x = v_x = u_y = v_y =0\\). So \\(u\\) and \\(v\\) are constant, hence \\(f\\) is constant. \\(\\Box\\)\n2. Suppose \\(f\\) is a complex function in a region \\(\\Omega\\), and both \\(f\\) and \\(f^2\\) are harmonic in \\(\\Omega\\). Prove that either \\(f\\) or \\(\\bar{f}\\) is holomorphic in \\(\\Omega\\).\nProof. Put \\(f = u + iv\\). Because \\(f\\) is harmonic, \\(u\\) and \\(v\\) are harmonic. Suppose \\(f^2\\) is harmonic in \\(\\Omega\\), which implies \\(u^2 - v^2\\) and \\(uv\\) are harmonic. By Exercise 1, \\(uv\\) harmonic leads to \\(u\\) is constant or \\(v\\) is constant or there exists a nonzero real constant \\(C\\) such that \\(u_x = Cv_y\\) and \\(u_y = -Cv_x\\). If \\(u\\) is constant, then \\(v^2\\) is harmonic (because \\(u^2-v^2\\) is harmonic), hence \\(v\\) is constant by Excercise 1. Similarly for the case \\(v\\) is constant, we conclude than \\(f\\) is constant if \\(u\\) or \\(v\\) is constant, hence both \\(f\\) and \\(\\bar{f}\\) is holomorphic. Suppose \\(f\\) is not constant. Take the Laplacian of \\(u^2-v^2\\), we get \\((u_x)^2+(u_y)^2 = (v_x)^2 + (v_y)^2\\). Replace \\(u_x = Cv_y\\) and \\(u_y = -Cv_x\\) to this equation, we get \\(C^2 = 1\\) unless \\(v_x = v_y = 0\\) in \\(\\Omega\\). The case \\(C = 1\\) leads to the Cauchy-Riemann equations of \\(f\\), hence \\(f\\) is holomorphic. The case \\(C=-1\\) leads to the Cauchy-Riemann equations of \\(\\bar{f}\\), hence \\(\\bar{f}\\) is holomorphic. We get the conclusion. (There is another way which does not take the advantage of the first question in Excercise 1. Take the Laplacian of \\(u^2-v^2\\) and \\(uv\\), we get \\((u_x)^2+(u_y)^2 = (v_x)^2 + (v_y)^2\\) and \\(u_xv_x + u_yv_y=0\\), which implies \\((u_x+iv_x)^2 + (u_y+iv_y)^2 = 0\\). Hence \\[\n(u_x+v_y+iv_x - i u_y)(u_x -v_y+iv_x+iu_y) = 0.\n\\]\nUse \\(ab=0 \\Leftrightarrow a\\bar{b}=0\\), we get \\[\n(u_x+v_y+iv_x - i u_y)(u_x -v_y-iv_x-iu_y) = 0.\n\\]\nPut \\(g = u_x+v_y+iv_x - i u_y\\) and \\(h = u_x -v_y-ivx-iu_y\\), we get \\(gh=0\\). It is easy to check that \\(g\\) and \\(h\\) are holomorphic (by checking the Cauchy-Riemann equations, we get \\(u_x-iu_y\\) and \\(v_y+iv_x\\) are holomorphic). We claim that \\(g=0\\) in \\(\\Omega\\) or \\(h=0\\) in \\(\\Omega\\). Suppose the contrary, because \\(Z(g)\\) and \\(Z(h)\\) are countable sets, \\(Z(gh)\\) must be a countable set, which is a contradiction with \\(Z(gh)=\\Omega\\). The case \\(g=0\\) leads to \\(f\\) is holomorphic. The case \\(h=0\\) leads to \\(\\bar{f}\\) is holomorphic. \\(\\Box\\)\n3. If \\(u\\) is a harmonic function in a region \\(\\Omega\\), what can you say about the set of points at which the gradient of \\(u\\) is \\(0\\)? (This is the set on which \\(u_x = u_y = 0\\).)\nProof. \\(K = \\{z:u_x(z) = u_y(z) = 0\\}\\) has no limit point in \\(\\Omega\\) (hence has at most countable elements) or \\(K = \\Omega\\).\nPut \\(f= u_x - i u_y\\). If \\(u\\) is harmonic then \\(f\\) is a holomorphic function by checking its Cauchy-Riemann equations. Moreover, because \\(K = Z(f)\\), we get the conclusion. (The case \\(K = \\Omega\\) leads to \\(u_x = u_y = 0\\) in \\(\\Omega\\). Hence \\(u\\) is constant.) \\(\\Box\\)"
  },
  {
    "objectID": "blog/rudin-complex-analysis.html#chapter-12---the-maximum-modulus-principle",
    "href": "blog/rudin-complex-analysis.html#chapter-12---the-maximum-modulus-principle",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "Chapter 12 - The Maximum Modulus Principle",
    "text": "Chapter 12 - The Maximum Modulus Principle\n1. Suppose \\(\\Delta\\) is a closed equilaterial triangle in the plane, with vertices \\(a\\), \\(b\\), \\(c\\). Find \\(\\max(|z-a||z-b||z-c|)\\) as \\(z\\) range over \\(\\Delta\\).\nProof. \\(\\max = \\sqrt{3}|b-a|^3/8\\).\nBy the maximum modulus theorem, we have \\[\n\\max\\limits_{z\\in\\Delta}(|(z-a)(z-b)(z-c)|) = \\max\\limits_{z\\in\\partial\\Delta}(|(z-a)(z-b)(z-c)|).\n\\]\nNow, it is elementary to find the maximum of the right hand side, which is attained when \\(z\\) is one of the middle points of the edges \\(ab\\), \\(bc\\), and \\(ca\\). Indeed, without loss of generality, suppose the maximum is attained at \\(z\\) on \\(ab\\). Put \\(x = |z-a|\\) and \\(l = |b-a|\\). So the maximum is \\[\nx(l-x)\\sqrt{(x-l/2)^2 + (\\sqrt{3}l/2)^2} = x(l-x)\\sqrt{l^2-x(l-x)}.\n\\]\nWhen \\(x\\) ranges over \\([0,l]\\), \\(x(l-x)\\) ranges over \\([0,l^2/4]\\). Put \\(t = x(l-x)\\), we get the maximum is \\(t\\sqrt{l^2-t}\\), or \\(\\sqrt{l^2t^2-t^3}\\). The derivative of \\(l^2t^2-t^3\\) according to \\(t\\) is \\((2l^2-3t)t\\), which is greater than or equal to \\(0\\) when \\(t\\) ranges over \\([0,l^2/4]\\). So \\(t\\sqrt{l^2-t}\\) is increasing when \\(t\\) runs from \\(0\\) to \\(l^2/4\\), and the maximum is attained when \\(t = l^2/4\\), which is the case \\(x = l/2\\). \\(\\Box\\)\n3. Suppose \\(f\\in H(\\Omega)\\). Under what conditions can \\(|f|\\) have a local minimum in \\(\\Omega\\).\nProof. \\(f\\) is constant or \\(f\\) has at least one zero in \\(\\Omega\\).\nOn the contrary, suppose \\(f\\) is not constant and \\(f\\) has no zero in \\(\\Omega\\). So \\(f^{-1}\\in H(\\Omega)\\) and \\(|f|\\) has a local minimum at \\(a\\) iff \\(|f^{-1}|\\) has a local maximum at \\(a\\). By applying the maximum modulus theorem, if \\(|f^{-1}|\\) has a local maximum then \\(f^{-1}\\) must be constant, hence \\(f\\) is constant. So \\(f\\) has no local minimum on \\(\\Omega\\). (For more details, we consider the case \\(f\\) is not constant. Clearly \\(|f|\\) has local minima at zeroes of \\(f\\). Reversely, let \\(a\\) be a point at which \\(f\\) has a local minimum. If \\(a \\neq 0\\), then \\(f\\neq 0\\) in an open neighborhood of \\(a\\). As above, \\(f\\) must be constant in this neighborhood, hence be constant in \\(\\Omega\\). So, in this case, we can see that the set of all local minima of \\(f\\) is \\(Z(f)\\), the set of all zeroes of \\(f\\).) \\(\\Box\\)\n4. \\((a)\\) Suppose \\(\\Omega\\) is a region, \\(D\\) is a disc, \\(\\bar{D}\\subset \\Omega\\), \\(f\\in H(\\Omega)\\), \\(f\\) is not constant, and \\(|f|\\) is constant on the boundary of \\(D\\). Prove that \\(f\\) has at least one zero in \\(D\\).\n\\((b)\\) Find all entire functions \\(f\\) such that \\(|f(z)| = 1\\) whenever \\(|z|=1\\).\nProof. \\((a)\\) Suppose that \\(f\\) has no zero in \\(D\\). If \\(|f|=0\\) on \\(\\partial D\\), then by the maximum modulus theorem, \\(|f|=0\\) in \\(D\\), hence \\(f\\) is constant. So \\(f\\neq 0\\) on \\(\\partial D\\), hence \\(f \\neq 0\\) on \\(\\bar{D}\\). Put \\(c&gt;0\\) be the constant value of \\(|f|\\) on \\(\\partial D\\). By the maximum modulus theorem, we have \\(|f|\\leq c\\) in \\(D\\). Consider the function \\(f^{-1}\\) on \\(\\bar{D}\\). It is clear that \\(f^{-1}\\in C(\\bar{D})\\) and \\(f^{-1}\\in H(D)\\). By the maximum modulus theorem, we have \\(|f^{-1}| \\leq 1/c\\) in \\(D\\), hence \\(|f| \\geq c\\) in \\(D\\). So \\(|f| = c\\) in \\(D\\), hence \\(f(D) \\subset \\partial D(0;c)\\). By the open mapping theorem, \\(f\\) must be constant in \\(D\\), hence be constant in \\(\\Omega\\). This contradiction shows that \\(f\\) must have at least one zero in \\(D\\). (In this proof, we just need \\(f\\) is holomorphic in \\(D\\) and continuous on \\(\\bar{D}\\).)\n\\((b)\\) \\(f\\) is of the form \\(cz^m\\), where \\(|c| = 1\\) and \\(m \\geq 0\\).\nSuppose that \\(f\\) is not constant. By \\((a)\\), \\(f\\) has at least one zero in \\(D(0;1)\\). The set of all zeroes of \\(f\\) has no limit point in \\(\\mathbb{C}\\). Because \\(\\bar{D}(0;1)\\) is compact, the number of zeroes of \\(f\\) in \\(D(0;1)\\) is finite. Let \\(a_1,a_2,\\ldots,a_n\\in D(0;1)\\) be such zeroes and \\(m_1,m_2,\\ldots,m_n\\) be the orders of zeroes of \\(f\\) at these points. Put \\[\\begin{aligned}\ng(z) &= f(z)\\prod\\limits_{i=1}^n \\left(\\frac{1-\\bar{a}_iz}{z-a_i}\\right)^{m_i}\\\\\n&=\\frac{f(z)}{(z-\\bar{a}_1)^{m_1}(z-\\bar{a}_2)^{m_2}\\ldots(z-\\bar{a}_n)^{m_n}}\\prod\\limits_{i=1}^n (1-\\bar{a}_iz)\n\\end{aligned}\n\\]\nfor all \\(z\\in \\mathbb{C}\\). It is clear that \\(g\\in H(\\mathbb{C})\\) and \\(g\\) has no zero in \\(D(0;1)\\). Moreover, for \\(|z| = 1\\), we have \\[\n\\left|\\frac{1-\\bar{a}_iz}{z-a_i}\\right| = \\left|z.\\frac{1/z-\\bar{a}_i}{z-a_i} \\right| = \\left|z.\\frac{\\bar{z}-\\bar{a}_i}{z-a_i} \\right| = \\left|z.\\frac{\\overline{z - a_i}}{z-a_i} \\right| = 1\\qquad (i=\\overline{1,n}).\n\\]\nHence \\(|g(z)|=|f(z)|=1\\) for every \\(z\\in \\partial D(0;1)\\). By \\((a)\\), \\(g\\) must be a constant, which will be denoted by \\(c\\). Clearly \\(|c|=1\\). If there is exists \\(i\\) such that \\(a_i\\neq 0\\), then \\(g(1/\\bar{a}_i) = 0\\), which is a contradiction to \\(|c|=1\\). So \\(n=1\\) and \\(a_1 = 0\\). This means that \\(f(z) = cz^{m_1}\\), where \\(|c|=1\\) and \\(m_1 &gt; 0\\). Together with the case \\(f\\) is a constant, we see that \\(f\\) must be in the form \\(cz^{m}\\), where \\(|c|=1\\) and \\(m\\geq 0\\). In the end, it is clear that these functions satisfy our requirement. \\(\\Box\\)\n5. Suppose \\(\\Omega\\) is a bounded region, \\(\\{f_n\\}\\) is a sequence of continuous functions on \\(\\bar{\\Omega}\\) which are holomorphic in \\(\\Omega\\), and \\(\\{f_n\\}\\) converges uniformly on the boundary of \\(\\Omega\\). Prove that \\(\\{f_n\\}\\) converges uniformly on \\(\\bar{\\Omega}\\).\nProof. Fix \\(\\epsilon &gt; 0\\). There is \\(N\\) large enough such that for every \\(m,n &gt; N\\), we have \\(|f_n(z) - f_m(z)| &lt; \\epsilon\\) for every \\(z\\in \\partial \\Omega\\). By the maximum modulus theorem, for every \\(m,n&gt;N\\), we have \\[\n|f_n(z) - f_m(z)| &lt; \\epsilon \\qquad (*)\n\\]\nfor every \\(z\\in \\bar{\\Omega}\\). So for each \\(z\\in \\partial\\Omega\\), \\(\\{f_n(z)\\}\\) is a Cauchy sequence, hence converges. For each \\(z\\), put \\(f(z)\\) be this limit. Let \\(m\\) converges to \\(\\infty\\) in \\((*)\\), we get the conclusion that \\(\\{f_n\\}\\) converges to \\(f\\) uniformly on \\(\\bar{\\Omega}\\). \\(\\Box\\)\n6. Suppose \\(f\\in H(\\Omega)\\), \\(\\Gamma\\) is a cycle in \\(\\Omega\\) such that \\(\\mathrm{Ind}_{\\Gamma}(\\alpha) = 0\\) for all \\(\\alpha \\notin \\Omega\\), \\(|f(\\zeta)|\\leq 1\\) for every \\(\\zeta \\in\\Gamma^*\\), and \\(\\mathrm{Ind}_{\\Gamma}(z) \\neq 0\\). Prove that \\(|f(z)|\\leq 1\\).\nProof. Let \\(U\\) be an open connected component of \\(\\mathbb{C}\\backslash \\Gamma\\) such that the index of any point in \\(U\\) with respect to \\(\\Gamma\\) is not zero. So \\(U\\) is bounded and, by assumption, \\(U \\subset \\Omega\\). Let \\(z \\in \\partial U\\). Because \\(z \\notin U\\) and also \\(z\\) can not be in any other component of \\(\\mathbb{C}\\backslash \\Gamma\\), we have \\(z\\in \\Gamma\\). So \\(\\partial U\\subset \\Gamma\\) and \\(\\bar{U} \\subset \\Omega\\). By assumption, we get \\(\\|f\\|_{\\partial U} \\leq 1\\). Moreover, because \\(U\\) is bounded, we have \\(\\bar{U}\\) is compact. By the maximum modulus theorem, we get \\(|f(z)| \\leq 1\\) for every \\(z\\in U\\). So \\(|f(z)|\\leq 1\\) for every \\(z\\) such that \\(\\mathrm{Ind}(z) \\neq 0\\). \\(\\Box\\)\n7. In the proof of Theorem 12.8 it was tacitly assumed that \\(M(a)&gt;0\\) and \\(M(b) &gt; 0\\). Show that the theorem is true if \\(M(a) = 0\\), and that then \\(f(z) = 0\\) for all \\(z\\in \\Omega\\).\nProof. When \\(M(a) = 0\\), consider the segment \\(L = \\{a+ yi: y\\in (0,1)\\}\\), it is plain that \\(f\\) is vanishing on \\(L\\). Put \\(\\Theta = \\Omega \\cup L \\cup \\{x+yi: 2a - b &lt; x &lt; b\\}\\). By the Schwarz reflection principle, we get a function \\(F \\in H(\\Theta)\\) such that \\(F=f\\) on \\(\\Omega\\cup L\\). Because \\(\\Theta\\) is a region and \\(F\\) is vanishing on \\(L\\), \\(F\\) must be vanishing on \\(\\Theta\\), hence \\(f\\) is vanishing on \\(\\Omega\\). In other words, we have \\(M(x) = 0\\) for all \\(x \\in (a,b)\\). (To verify that we can apply the Schwarz reflection principle here, consider the map \\(z \\to i(z-a)\\).) \\(\\Box\\)\n15. Suppose \\(f\\in H(U)\\). Prove that there is a sequence \\(\\{z_n\\}\\) in \\(U\\) such that \\(|z_n|\\to 1\\) and \\(\\{f(z_n)\\}\\) is bounded.\nProof. The problem is plain for the case \\(f\\) is constant. For the case \\(f\\) has no zero in \\(U\\). For each \\(r\\in (0,1)\\), apply the maximum modulus theorem for the function \\(f^{-1}\\) with the domain \\(\\bar{D}(0;r)\\), we get \\(|f(0)|\\geq \\min_{z\\in\\partial D(0;r)}|f(z)|\\), hence we get a point \\(a_r\\) such that \\(|a_r| = r\\) and \\(|f(a_r)|\\leq |f(0)|\\). So the sequence \\(\\{a_{1-1/n}\\}\\) satisfies our requirement. Now suppose that \\(f\\) is not constant and \\(f\\) has at least one zero in \\(U\\). Put \\(Z(f)\\) be the set of zeroes of \\(f\\) in \\(U\\). It is clear that \\(Z(f)\\) has no limit point in \\(U\\). If \\(Z(f)\\) is an infinite set then it must have a limit point in \\(\\bar{U}\\), which is actually on \\(\\partial U\\). So we obtain a sequence \\(\\{z_n\\}\\subset U\\) converging to this limit point (hence \\(|z_n|\\to 1\\)) and \\(|f(z_n)|=0\\) for every \\(n\\). We get the conclusion for this case. The remaining case is that \\(Z(f)\\) is a finite set. Put \\(Z(f) = \\{a_1,a_2,\\ldots,a_n\\}\\) and let \\(m_1,m_2,\\ldots,m_n\\) be the orders of zeroes of \\(f\\) at \\(a_1,a_2,\\ldots,a_n\\) correspondingly. Put \\[\ng(z) = \\frac{f(z)}{(z-a_1)^{m_1}(z-a_2)^{m_2}\\ldots(z-a_n)^{m_n}}\\qquad (z\\in U).\n\\]\nIt is clear that \\(g\\in H(U)\\) and \\(g\\) has no zero in \\(U\\). As above, we get a sequence \\(\\{z_n\\}\\) in \\(U\\) such that \\(|z_n|\\to 1\\) and \\(|g(z_n)| &lt; M\\) for some \\(M &gt; 0\\). For \\(|z| &gt; \\max\\{|a_1|,|a_2|,\\ldots,|a_n|\\}\\), we have \\[\\begin{aligned}\n|f(z)| &= |g(z)||z_n-a_1|^{m_1}|z_n-a_2|^{m_2}\\ldots|z-a_n|^{m_n}\\\\\n&\\leq |g(z)|(1-|a_1|)^{m_1}(1-|a_2|)^{m_2}\\ldots(1-|a_n|)^{m_n}.\n\\end{aligned}\n\\]\nSo for \\(n\\) large enough, we have \\(|z_n| &gt; \\max\\{|a_1|,|a_2|,\\ldots,|a_n|\\}\\) and \\[\n|f(z_n)| &lt; M(1-|a_1|)^{m_1}(1-|a_2|)^{m_2}\\ldots(1-|a_n|)^{m_n}.\n\\]\nThe sequence \\(\\{z_n\\}\\) is the one we want to find. \\(\\Box\\)\n16. Suppose \\(\\Omega\\) is a bounded region, \\(f\\in H(\\Omega)\\), and \\[\n\\limsup\\limits_{n\\to\\infty} |f(z_n)| \\leq M\n\\]\nfor every sequence \\(\\{z_n\\}\\) in \\(\\Omega\\) which converges to a boundary point of \\(\\Omega\\). Prove that \\(|f(z)|\\leq M\\) for all \\(z\\in \\Omega\\).\nProof. It is enough to prove that \\(|f(z)| \\leq M + \\epsilon\\) for every \\(z\\in \\Omega\\) and every \\(\\epsilon &gt; 0\\). Instead, fix \\(\\epsilon &gt;0\\), we will prove that the set \\(A = \\{z\\in \\Omega: |f(z)| &gt; M+\\epsilon\\}\\) is empty. On the contrary, suppose that \\(A\\) is not empty. It is plain that \\(A\\) is open. Let \\(U\\) be a nonempty connected open component of \\(A\\) and let \\(z\\in \\partial U\\subset \\bar{\\Omega}\\). There is a sequence \\(\\{z_n\\}\\subset U\\) such that \\(z_n \\to z\\) and \\(|f(z_n)|&gt; M+\\epsilon\\) for all \\(n\\). Then we have \\(\\limsup_{n\\to\\infty} |f(z_n)| \\geq M + \\epsilon\\), which is a contradiction to our hypothesis if \\(z\\in \\partial \\Omega\\). So \\(z\\in \\Omega\\) for all \\(z\\in\\partial U\\). In other words, \\(\\bar{U}\\subset \\Omega\\). Because \\(\\Omega\\) is bounded, we have \\(\\bar{U}\\) is a compact set. Moreover, for \\(z\\in \\partial U\\), we have \\(|f(z)| \\geq M + \\epsilon\\), hence \\(|f(z)| = M +\\epsilon\\) (if not then \\(z\\in A\\), hence \\(z\\in A\\backslash U\\), which is an open set, hence \\(z\\notin \\bar{U}\\)). Apply the maximum modulus theorem for \\(f\\) with the domain \\(\\bar{U}\\), we get \\(|f(z)|\\leq M + \\epsilon\\) for every \\(z\\in U\\), which is a contradiction with our definition of \\(U\\). So \\(A\\) must be empty. \\(\\Box\\)"
  },
  {
    "objectID": "blog/rudin-complex-analysis.html#chapter-13---approximation-by-rational-functions",
    "href": "blog/rudin-complex-analysis.html#chapter-13---approximation-by-rational-functions",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "Chapter 13 - Approximation by Rational Functions",
    "text": "Chapter 13 - Approximation by Rational Functions\n1. Prove that every meromorphic function on \\(S^2\\) is rational.\nProof. Note that \\(f\\) must have an isolated singularity at \\(\\infty\\), which means that \\(f\\) is holomorphic in \\(D'(\\infty;r)=\\{z\\in \\mathbb{C}:|z|&gt; r\\}\\). This implies that the set of all poles of \\(f\\) in \\(\\mathbb{C}\\) is finite (because this set has no limit point and it is a subset of the compact set \\(\\overline{D}(0;r)\\)). If we subtract from \\(f\\) the sum of all principle parts at these poles, we get an entire function \\(g\\) with an isolated singularity at \\(\\infty\\). Of course, this singularity of \\(g\\) at \\(\\infty\\) can not be an essential singularity because of our hypothesis that \\(f\\) is meromorphic on \\(S^2\\). If this singularity is a removable singularity then \\(g\\) is bounded in a neighborhood of \\(\\infty\\), hence is bounded in \\(\\mathbb{C}\\), hence is constant by Liouville’s theorem. If this singularity is a pole, the principal part of \\(g\\) at \\(\\infty\\) is a polynomial. Subtract this polynomial from \\(g\\), we again get an entire function with a removable singularity at \\(\\infty\\). Both cases lead to the same conclusion that \\(f\\) is a rational function. \\(\\Box\\)\n2. Let \\(\\Omega = \\{z:|z|&lt;1 \\text{ and }|2z-1|&gt;1\\}\\), and suppose \\(f\\in H(\\Omega)\\).\n\\((a)\\) Must there exist such a sequence of polynomials \\(P_n\\) such that \\(P_n \\to f\\) uniformly on compact subsets of \\(\\Omega\\)?\n\\((b)\\) Must there exist such a sequence which converges to \\(f\\) uniformly in \\(\\Omega\\)?\n\\((c)\\) Is the answer to \\((b)\\) changed if we require more of \\(f\\), namely, that \\(f\\) be holomorphic in some open set which contains the closure of \\(\\Omega\\)?\nProof. \\((a)\\) Yes. Because \\(S^2\\backslash \\Omega\\) is connected (then apply Runge’s theorem).\n\\((b)\\) No. See \\((c)\\) or for simple, consider the function \\(f(z) = z^{-1}\\) with a similar argument.\n\\((c)\\) No. Consider the function \\(f(z) = (z-\\frac{1}{10})^{-1}\\). Suppose that there exists a sequence of polynomials \\(P_n\\) such that \\(P_n\\) converges to \\(f\\) uniformly in \\(\\Omega\\). This implies there there exists a polynomial \\(P\\) such that \\[\n|P(z) - (z-1/10)^{-1}| &lt; 1\n\\] for every \\(z\\in \\Omega\\). Fix \\(z \\in \\partial D(0;1)\\). Let \\(\\{z_n\\}\\) be a sequence in \\(\\Omega\\) converges to \\(z\\). We have \\[\\begin{aligned}\n|P(z)| = \\lim\\limits_{n\\to\\infty}|P(z_n)| &\\leq 1 + \\lim\\limits_{n\\to\\infty}\\frac{1}{\\left| z_n-1/10\\right| } = 1 + \\frac{1}{\\left| z-1/10\\right| }\\\\\n&\\leq 1 + \\frac{1}{1 - 1/10} &lt; 3.\n\\end{aligned}\n\\]\nBy the maximum modulus theorem, \\(|P(z)| &lt; 3\\) for every \\(z \\in D(0;1)\\). However, for \\(z = \\frac{-1}{10}\\), we have \\[\n|P(-1/10)| &gt; \\frac{1}{\\left|-1/10 - 1/10\\right|} - 1 = 4.\n\\]\nThis contradiction shows that there is not any sequence of polynomials which converges to \\((z-\\frac{1}{10})^{-1}\\) uniformly in \\(\\Omega\\). \\(\\Box\\)\n3. Is there a sequence of polynomials \\(P_n\\) such that \\(P_n(0)=1\\) for \\(n=1,2,3,\\ldots\\), but \\(P_n(z)\\to 0\\) for every \\(z\\neq 0\\), as \\(n\\to \\infty\\)?\nProof. Yes. For each \\(n\\), let \\(K_n\\) be the compact set \\(D_{n} \\cup L_{n} \\cup \\{0\\}\\), where \\(D_{n} = \\{z\\in \\bar{D}(0;n):|\\mathrm{Im} z|\\geq \\frac{1}{n}\\}\\), \\(L_n = \\{x+yi: y = 0, \\frac{1}{n}\\leq |x|\\leq n\\}\\). Let \\(U_n\\) be an neighborhood of \\(D_n \\cup L_n\\) and \\(V_n\\) be an neighborhood of \\(0\\) such that \\(U_n\\) and \\(V_n\\) are disjoint. Let \\(f_n \\in H(U_n \\cup V_n)\\) be a function whose values are \\(0\\) in \\(U_n\\) and \\(1\\) in \\(V_n\\). Because \\(S^2\\backslash K_n\\) is connected, apply Runge’s theorem, we get a polynomial \\(P_n\\) such that \\(|P_n(z) - f_n(z)| &lt; \\frac{1}{n}\\) for every \\(z\\in K_n\\), hence \\(|P_n(z)| &lt; \\frac{1}{n}\\) for every \\(z\\in D_n \\cup L_n\\) and \\(|P_n(0)-1| &lt; \\frac{1}{n}\\). Now, because \\(\\{D_n\\cup L_n\\}\\) is an increasing sequence of sets whose union is \\(\\mathbb{C}\\backslash \\{0\\}\\), we can see that the sequence of polynomials \\(P_n\\) satisfies \\(P_n(0) \\to 1\\) as \\(n \\to \\infty\\) and for \\(z \\neq 0\\), \\(P_n(z) \\to 0\\) as \\(n \\to \\infty\\). Replace \\(P_n\\) by \\(P_n - P_n(0) + 1\\), we get a sequence of polynomials satisfying our requirement. \\(\\Box\\)\n4. Is there a sequence of polynomials \\(P_n\\) such that \\[\n\\lim\\limits_{n\\to\\infty}P_n(z) =\n\\begin{cases}\n1 & \\text{if }\\mathrm{Im}z &gt; 0,\\\\\n0 &\\text{if } z \\text{ is real},\\\\\n-1 & \\text{if }\\mathrm{Im}z &lt; 0?\n\\end{cases}\n\\]\nProof. Yes. For each \\(n\\), let \\(K_n\\) be the compact set $D_n E_n L_n $, where \\(D_n = \\{z \\in\\bar{D}(0;n):\\mathrm{Im}z\\geq \\frac{1}{n}\\}\\), \\(E_n = \\{z \\in\\bar{D}(0;n):\\mathrm{Im}z\\leq -\\frac{1}{n}\\}\\), and \\(L_n = \\{ x + yi:y = 0, |x| \\leq n\\}\\). Let \\(U_n\\) be an neighborhood of \\(D_n\\), \\(V_n\\) be an neighborhood of \\(E_n\\), and \\(W_n\\) be an neighborhood of \\(L_n\\) such that \\(U_n\\), \\(V_n\\), and \\(W_n\\) are pairwise disjoint. Now let \\(f_n\\in H(U_n\\cup V_n \\cup W_n)\\) be a function whose values are \\(1\\) in \\(U_n\\), \\(-1\\) in \\(V_n\\), and \\(0\\) in \\(L_n\\). Because \\(S^2\\backslash K_n\\) is connected, apply Runge’s theorem, we get a polynomial \\(P_n\\) such that \\(|P_n(z)-f_n(z)| &lt; \\frac{1}{n}\\) for every \\(z\\in K_n\\), hence \\(|P_n(z)-1| &lt; \\frac{1}{n}\\) for every \\(z\\in D_n\\), \\(|P_n(z)+1| &lt; \\frac{1}{n}\\) for every \\(z\\in E_n\\), and \\(|P_n(z)| &lt; \\frac{1}{n}\\) for every \\(z\\in L_n\\). Because \\(\\{D_n\\}\\), \\(\\{E_n\\}\\), and \\(\\{L_n\\}\\) are three increasing sequences of sets whose unions are open upper half plane, open lower half plane, and real line correspondingly, we can see that the sequence of polynomials \\(P_n\\) satisfies our requiment. \\(\\Box\\)\n7. Show that in Theorem 13.9 we need not assume that \\(A\\) intersects each component of \\(S^2\\backslash \\Omega\\). It is enough to assume that the closure of \\(A\\) intersects each component of \\(S^2\\backslash \\Omega\\).\nProof. We have \\(\\Omega\\) is the union of a sequence \\(\\{K_n\\}\\) of compact sets such that \\(K_n\\) lies in the interior of \\(K_{n+1}\\) for every \\(n\\), every compact subsets of \\(\\Omega\\) lies in some \\(K_n\\), and for every \\(n\\), every component of \\(S^2\\backslash K_n\\) contains a component of \\(S^2 \\backslash \\Omega\\) (Theorem 13.3). Fix \\(n\\). Follow the proof of Theorem 13.9, it is enough to show that each component of \\(S^2 \\backslash K_n\\) contains a point of \\(A\\). Indeed, let \\(U\\) be a component of \\(S^2\\backslash K_n\\). It is clear that \\(U\\) is open. Moreover, \\(\\bar{A}\\) intersects each component of \\(S^2\\backslash \\Omega\\), hence intersects \\(U\\). So \\(U \\cap A \\neq \\varnothing\\). \\(\\Box\\)\n8. Prove the Mittag-Lefler theorem for the case in which \\(\\Omega\\) is the whole plane, by a direct argument which makes no appeal to Runge’s theorem.\nProof. For each \\(n\\), denote \\(\\bar{D}(0;n)\\) by \\(K_n\\) for simple. Put \\(A_1 = A \\cap K_1\\), and for each \\(n \\geq 2\\), put \\(A_n = A\\cap(K_n\\backslash K_{n-1})\\). It is easy to see that each \\(A_n\\) is finite. Now, for each \\(n\\), put \\[\nQ_n(z) = \\sum\\limits_{\\alpha \\in A_n}P_{\\alpha}(z).\n\\]\nFor each \\(n\\), we have \\(Q_n\\) is a rational function and the poles of \\(Q_n\\) lie in \\(A_n\\). Fix \\(n \\geq 2\\). We have \\(Q_n\\) is holomorphic in an open set containing \\(K_{n-1}\\). Consider the power series representation of \\(Q_n\\) at \\(0\\) and recall that \\(K_{n-1}\\) is \\(\\bar{D}(0;n-1)\\), we get the power series converges to \\(Q_n\\) uniformly in \\(K_{n-1}\\). This implies that we have a polynomial \\(R_n\\) such that \\[\n|R_n(z) - Q_n(z)| &lt; 2^{-n}\\qquad (z\\in K_{n-1}).\n\\]\nFollow the proof of the Mittag-Leffler theorem, we can see that the function \\[\nf(z) = Q_1(z) + \\sum_{n=2}^{\\infty}(Q_n(z) - R_n(z))\\qquad (z\\in \\mathbb{C})\n\\] has the desired properties. \\(\\Box\\)\n9. Suppose \\(\\Omega\\) is a simply connected region, \\(f\\in H(\\Omega)\\), \\(f\\) has no zero in \\(\\Omega\\), and \\(n\\) is a positive integer. Prove that there exists a \\(g\\in H(\\Omega)\\) such that \\(g^n = f\\).\nProof. Let \\(h\\) be a holomorphic logarithm of \\(f\\) in \\(\\Omega\\) (the existence comes from the hypotheses that \\(\\Omega\\) is a simply connected region, \\(f\\in H(\\Omega)\\), and \\(f\\) has no zero in \\(\\Omega\\)). Now, the function \\(g = \\exp(h/n)\\) is in \\(H(\\Omega)\\) and \\(g^n = \\exp(h) = f\\). \\(\\Box\\)\n10. Suppose \\(\\Omega\\) is a region, \\(f\\in H(\\Omega)\\), and \\(f\\neq 0\\). Prove that \\(f\\) has a holomorphic logarithm in \\(\\Omega\\) if and only if \\(f\\) has holomorphic \\(n\\)-th roots in \\(\\Omega\\) for every positive integer \\(n\\).\nProof. The \\((\\Rightarrow)\\) side is clear, an \\(n\\)-th root of \\(f\\) is \\(\\exp(g/n)\\) where \\(g\\) be a holomorphic logarithm in \\(\\Omega\\) of \\(f\\). For the \\((\\Leftarrow)\\) side, at first, we will show that \\(f\\) has no zero in \\(\\Omega\\). Indeed, suppose \\(f(a) = 0\\) for some \\(a\\in \\Omega\\). Because \\(f\\neq 0\\), we have \\(f(z) = (z-a)^m g(z)\\), where \\(m&gt;0\\) is the order of zero of \\(f\\) at \\(a\\), \\(g\\in H(\\Omega)\\), and \\(g(a) \\neq 0\\). Consider an holomorphic \\((m+1)\\)-th root \\(\\varphi\\) of \\(f\\). Because \\(\\varphi(a) = 0\\) and \\(\\varphi \\neq 0\\), we have \\(\\varphi(z) = (z-a)h(z)\\), where \\(h\\in H(\\Omega)\\). Therefore \\(g(z) = (z-a)h(z)^{m+1}\\) and this gives us \\(g(a) = 0\\) (a contradiction). To show that \\(f\\) has a holomorphic logarithm in \\(\\Omega\\), it is enough to show that \\[\n\\int_{\\gamma}\\frac{f'(z)}{f(z)}\\,dz = 0\\qquad (*)\n\\] for every closed path \\(\\gamma\\) in \\(\\Omega\\). Indeed, suppose \\((*)\\) is true for every closed path in \\(\\Omega\\). Fix \\(z_0\\in \\Omega\\) and let \\(l_0\\) be a logarithm of \\(f(z_0)\\) (this logarithm exists because \\(f(z_0)\\neq 0\\)). Let \\(L\\) be a function in \\(\\Omega\\) defined by \\[\nL(z) = l_0 + \\int_{\\Gamma(z)}\\frac{f'(\\zeta)}{f(\\zeta)}\\,d\\zeta\\qquad (z\\in \\Omega),\n\\] where \\(\\Gamma(z)\\) is any path in \\(\\Omega\\) from \\(z_0\\) to \\(z\\). Because of \\((*)\\), this function is well-defined. Fix \\(a\\in \\Omega\\). For any \\(D(a;r)\\subset \\Omega\\) and any \\(z\\in D'(a;r)\\), we have \\[\n\\frac{L(z)-L(a)}{z-a}-\\frac{f'(a)}{f(a)} = \\frac{1}{z-a}\\int_{[a,z]}\\left(\\frac{f'(\\zeta)}{f(\\zeta)}-\\frac{f'(a)}{f(a)}\\right)\\,dz.\n\\]\nBecause \\(f'/f\\) is continuous at \\(a\\), it is easy to see that \\(L'(a) = f'(a)/f(a)\\). So \\(L\\in H(\\Omega)\\) and the formula \\(L'=f'/f\\) gives us \\((\\exp(L)/f)'=0\\). Hence \\(\\exp(L)/f = \\mathrm{const} = \\exp(L(z_0))/f(z_0)= 1\\). So \\(L\\) is a holomorphic logarithm of \\(f\\) in \\(\\Omega\\). In the end, to show that \\((*)\\) is true, we notice that \\(\\frac{1}{2\\pi i}\\) times the left hand side of \\((*)\\) is the index of \\(f\\circ \\gamma\\) at \\(0\\), which is an integer number. Moreover, if \\(g\\) is an holomorphic \\(n\\)-th root of \\(f\\) in \\(\\Omega\\), then \\[\n\\frac{1}{2\\pi i}\\int_{\\gamma}\\frac{f'(z)}{f(z)}\\,dz = \\frac{n}{2\\pi i}\\int_{\\gamma}\\frac{g'(z)}{g(z)}\\,dz.\n\\]\nThe right hand side is an integer which is divisible by \\(n\\). We get the conclusion by noticing that \\(0\\) is the unique integer number which is divisible by all positive integer. \\(\\Box\\)\n11. Suppose that \\(f_n \\in H(\\Omega)\\) (\\(n=1,2,3,\\ldots\\)), \\(f\\) is a complex function in \\(\\Omega\\), and \\(f(z) = \\lim_{n\\to\\infty}f_n(z)\\) for every \\(z\\in \\Omega\\). Prove that \\(\\Omega\\) has a dense open subset \\(V\\) on which \\(f\\) is holomorphic.\nHint. Put \\(\\varphi = \\sup|f_n|\\). Use Baire’s theorem to prove that every disc in \\(\\Omega\\) contains a disc on which \\(\\varphi\\) is bounded. Apply Excercise 10.5.\n(In general, \\(V\\neq \\Omega\\). Compare Excercises 3 and 4.)\nProof. Put \\(\\varphi(z) = \\sup_n |f_n(z)|\\) for every \\(z \\in \\Omega\\). It is well-defined because \\(\\lim_{n\\to\\infty} f_n(z)\\) exists (and equals to \\(f(z)\\)) for every \\(z \\in \\Omega\\). Fix \\(B\\) be a closed disc in \\(\\Omega\\) (so \\(B\\) is complete). For each \\(n\\), put \\(A_n = \\{z\\in B:\\varphi(z) \\leq n\\}\\). By the definition of \\(\\varphi\\), it is clear that \\[\nA_n = \\bigcap\\limits_{m=1}^{\\infty} \\{z\\in B: |f_m(z)|\\leq n\\}.\n\\]\nHence \\(A_n\\) is a closed set in \\(B\\) for each \\(n\\). By Baire’s theorem, there exists an \\(n\\) such that \\(A_n\\) is not a nowhere-dense subset of \\(B\\). So \\(A_n\\) contains an open disc \\(U_B\\subset B\\) in which \\(\\varphi\\) is bounded from above by \\(n\\). Now, apply Exercise 10.5, we get \\(f \\in H(U_B)\\). Let \\[\nV = \\bigcup\\limits_B U_B,\n\\] where \\(B\\) ranges over all closed disc in \\(\\Omega\\). It is clear that \\(V\\) is a dense open subset of \\(\\Omega\\) and \\(f\\in H(V)\\). \\(\\Box\\)"
  },
  {
    "objectID": "blog/rudin-complex-analysis.html#chapter-14---conformal-mapping",
    "href": "blog/rudin-complex-analysis.html#chapter-14---conformal-mapping",
    "title": "Some solutions to Rudin’s complex analysis book",
    "section": "Chapter 14 - Conformal Mapping",
    "text": "Chapter 14 - Conformal Mapping\n10. Suppose \\(f\\) and \\(g\\) are holomorphic mappings of \\(U\\) into \\(\\Omega\\), \\(f\\) is one-to-one, \\(f(U) = \\Omega\\), and \\(f(0) = g(0)\\). Prove that \\[\ng(D(0;r))\\subset f(D(0;r))\\qquad (0&lt; r &lt; 1).\n\\]\nProof. Put \\(h=f^{-1}\\circ g\\). Then \\(h(U)\\subset U\\) and \\(h(0) = 0\\). By Schwartz’s lemma, we have \\(|h(z)|\\leq |z|\\) for all \\(z\\in U\\). Fix \\(r\\in (0,1)\\). If \\(z\\in D(0;r)\\) then also \\(h(z)\\in D(0;r)\\), hence \\(f^{-1}\\circ g (z) \\in D(0;r)\\), and hence \\(g(z) \\in f(D(0;r))\\). So \\(g(D(0;r))\\subset f(D(0;r))\\). \\(\\Box\\)\n12. Suppose \\(\\Omega\\) is a convex region, \\(f\\in H(\\Omega)\\), and \\(\\mathrm{Re}f'(z)&gt;0\\) for all \\(z\\in \\Omega\\). Prove that \\(f\\) is one-to-one in \\(\\Omega\\). Is the result changed if the hypothesis is weakened to \\(\\mathrm{Re}f'(z) \\geq 0\\)? (Exclude the trivial case \\(f=\\) constant.) Show by an example that “convex” cannot be replaced by “simply connected.”\nProof. No. \\(\\Omega = \\mathbb{C}\\backslash [0,\\infty)\\), \\(f(z) = -iz^{3/2}+z/\\sqrt{2}\\).\nLet \\(a,b\\in \\Omega\\) such that \\(a \\neq b\\). We have \\[\nf(b) - f(a) = \\int_{[a,b]}f'(z)\\,dz = (b-a)\\int_0^1 f'(a+(b-a)t)\\,dt.\n\\]\nHence \\[\\begin{aligned}\n\\mathrm{Re}\\left[\\frac{f(b)-f(a)}{b-a}\\right] &= \\mathrm{Re}\\left[\\int_0^1 f'(a+(b-a)t)\\,dt\\right]\\\\\n&=  \\int_0^1 \\mathrm{Re}\\left[f'(a+(b-a)t)\\right]\\,dt &gt; 0.\n\\end{aligned}\n\\]\nSo we must have \\(f(b) \\neq f(a)\\), and this implies that \\(f\\) is one-to-one. Now suppose that \\(\\mathrm{Re}f'(z)\\geq  0\\) for all \\(z\\in \\Omega\\). Let \\(Z = \\{z\\in \\Omega: \\mathrm{Re}f'(z) = 0\\}\\). Of course, \\(Z\\) is a closed set. By the mean value property of \\(\\mathrm{Re}f'\\) and \\(\\mathrm{Re}f'(z)\\geq  0\\) for all \\(z\\in \\Omega\\), it is easy to show that \\(Z\\) is an open set. So \\(Z = \\varnothing\\) or \\(Z = \\Omega\\). The first case implies, as we have shown, \\(f\\) is one-to-one in \\(\\Omega\\). The second case implies \\(f'(\\Omega) \\subset i\\mathbb{R}\\). So, by the open mapping theorem, \\(f'\\) is constant in \\(\\Omega\\), hence \\(f\\) is constant or linear. The linear case also leads to \\(f\\) is one-to-one. So, unless \\(f\\) is constant, \\(f\\) is always one-to-one when \\(\\mathrm{Re}f'(z)\\geq  0\\) for all \\(z\\in \\Omega\\). For an example that “convex” cannot be replaced by “simply connected”, consider \\(\\Omega = \\mathbb{C}\\backslash [0,\\infty)\\) and \\(f(z) = -iz^{3/2}+z/\\sqrt{2}\\) with the branch cut for \\(z^{1/2}\\) is \\([0,\\infty)\\). It is easy to check that \\(f'(z) = -\\frac{3i}{2}z^{1/2} + 1/\\sqrt{2}\\) has a positive real part and \\(f(i) = f(-i)\\). \\(\\Box\\)\n13. Suppose \\(\\Omega\\) is a region, \\(f_n\\in H(\\Omega)\\) for \\(n=1,2,3,\\ldots\\), each \\(f_n\\) is one-to-one in \\(\\Omega\\), and \\(f_n\\to f\\) uniformly on compact subsets of \\(\\Omega\\). Prove that \\(f\\) is either constant or one-to-one in \\(\\Omega\\). Show that both cases can occur.\nProof. Suppose there exists \\(a,b\\in \\Omega\\) such that \\(a\\neq b\\) and \\(f(a) = f(b)\\). Choose \\(r &gt; 0\\) such that \\(\\bar{D}(a;r)\\subset \\Omega\\) and \\(b \\notin D(a;r)\\). By the assumption, we have \\(f_n(z) - f_n(b) \\to f(z) -f(b)\\) uniformly on \\(\\bar{D}(a;r)\\). Now by Exercise 10.20 with a note that \\(f_n(z) - f_n(b) \\neq 0\\) for all \\(z\\in D(a;r)\\) and all \\(n\\), we have \\(f(z) - f(b)\\neq 0\\) for all \\(z\\in D(a;r)\\) or \\(f(z) -f(b)=0\\) for all \\(z\\in D(a;r)\\). The first case cannot happen because \\(f(a) = f(b)\\). And the second case implies that \\(f\\) is constant. \\(\\Box\\)\n16. Let \\(\\mathcal{F}\\) be the class of all \\(f\\in H(U)\\) for which \\[\n\\iint\\limits_U |f(z)|^2 \\, d x\\,d y \\leq 1.\n\\]\nIs this a normal family?\nProof. Let \\(a \\in U\\) and \\(R &gt; 0\\) such that \\(D(a;2R) \\subset U\\). For \\(z\\in D(a;R)\\), we have \\(\\bar{D}(z;r) \\subset U\\) for all \\(r\\in (0,R]\\). By the mean value property of \\(f\\), we have \\[\nf(z) = \\frac{1}{2\\pi}\\int_{0}^{2\\pi} f(z + re^{it})\\, dt.\n\\]\nMultiply two sides with \\(r\\) and integrate them according to \\(r\\) from \\(0\\) to \\(R\\), we get \\[\n\\frac{R^2}{2}f(z) = \\frac{1}{2\\pi}\\int_0^R \\int_0^{2\\pi}rf(z + re^{it})\\,dt.\n\\]\nHence, by Hölder’s inequality, we have \\[\n\\begin{aligned}\n|f(z)| &\\leq \\frac{1}{\\pi R^2}\\left[\\int_0^R \\int_0^{2\\pi} r\\,dr\\,dt\\right]^{1/2}\\left[\\int_0^R \\int_0^{2\\pi} r|f(z + re^{it})|^2\\,dr\\,dt\\right]^{1/2}\\\\\n&= \\frac{1}{\\sqrt{\\pi} R}\\left[\\iint\\limits_{D(z;R)}|f(w)|\\,dw\\right]^{1/2} \\leq \\frac{1}{\\sqrt{\\pi} R}.\n\\end{aligned}\n\\]\nFor each compact set \\(K \\subset \\Omega\\), we can cover it by finite disc \\(D(a;R)\\) with the property that \\(D(a;2R) \\subset U\\). By the above argument, \\(\\mathcal{F}\\) is uniformly bounded in each disc, hence is uniformly bounded on \\(K\\). By Theorem 14.6, we conclude that \\(\\mathcal{F}\\) is a normal family. \\(\\Box\\)"
  },
  {
    "objectID": "blog/deep-gaussian-process.html",
    "href": "blog/deep-gaussian-process.html",
    "title": "Inferences for Deep Gaussian Process models in Pyro",
    "section": "",
    "text": "In this tutorial, I want to illustrate how to use Pyro’s Gaussian Processes module to create and train some deep Gaussian Process models. For the background on how to use this module, readers can check out some tutorials at http://pyro.ai/examples/.\nThe first part is a fun example to run HMC with a 2-layer regression GP models while the second part uses SVI to classify digit numbers.\nimport warnings\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns; sns.set()\nfrom scipy.cluster.vq import kmeans2\n\nimport torch\nimport torch.nn as nn\nfrom torch.distributions import constraints\nfrom torch.distributions.transforms import AffineTransform\nfrom torchvision import transforms\n\nimport pyro\nimport pyro.contrib.gp as gp\nimport pyro.distributions as dist\nfrom pyro.contrib.examples.util import get_data_loader\nfrom pyro.infer import MCMC, NUTS, Predictive, SVI, TraceMeanField_ELBO\n\npyro.set_rng_seed(0)\nwarnings.formatwarning = (lambda message, category, *args, **kwargs:\n                          \"{}: {}\\n\".format(category.__name__, message))"
  },
  {
    "objectID": "blog/deep-gaussian-process.html#hmc-with-heaviside-data",
    "href": "blog/deep-gaussian-process.html#hmc-with-heaviside-data",
    "title": "Inferences for Deep Gaussian Process models in Pyro",
    "section": "HMC with Heaviside data",
    "text": "HMC with Heaviside data\nLet’s create a dataset from Heaviside step function.\n\nN = 20\nX = torch.rand(N)\ny = (X &gt;= 0.5).float() + torch.randn(N) * 0.05\nplt.plot(X.numpy(), y.numpy(), \"kx\");\n\n\n\n\n\n\n\n\nWe will make a 2-layer regression model. \n\nclass DeepGPR(pyro.nn.PyroModule):\n    def __init__(self, gpr1, gpr2):\n        super().__init__()\n        self.gpr1 = gpr1\n        self.gpr2 = gpr2\n\n    def forward(self):\n        h_loc, h_var = self.gpr1.model()\n        self.gpr2.X = pyro.sample(\"h\", dist.Normal(h_loc, h_var.sqrt()))\n        self.gpr2.model()\n\n# mean function is used as in [3]\ngpr1 = gp.models.GPRegression(X, None, gp.kernels.RBF(1), noise=torch.tensor(1e-3),\n                              mean_function=lambda x: x)\ngpr1.kernel.variance = pyro.nn.PyroSample(dist.Exponential(1))\ngpr1.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\n\ngpr2 = gp.models.GPRegression(torch.zeros(N), y, gp.kernels.RBF(1), noise=torch.tensor(1e-3))\ngpr2.kernel.variance = pyro.nn.PyroSample(dist.Exponential(1))\ngpr2.kernel.lengthscale = pyro.nn.PyroSample(dist.LogNormal(0.0, 1.0))\ngpmodel = DeepGPR(gpr1, gpr2)\n\nNow, we run HMC to get 100 samples.\n\nhmc_kernel = NUTS(gpmodel, max_tree_depth=5)\nmcmc = MCMC(hmc_kernel, num_samples=200)\nmcmc.run()\n\nSample: 100%|██████████| 400/400 [01:14,  5.34it/s, step size=2.72e-02, acc. prob=0.933]\n\n\nAnd plot the marginal distribution of each latent site.\n\nfor name, support in mcmc.get_samples().items():\n    if name == \"h\":\n        continue\n    sns.distplot(support)\n    plt.title(name)\n    plt.show()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s test if the posterior can predict the Heaviside data. The first step is to make a predictive model.\n\ndef predictive(X_new):\n    # this sample statement will be replaced by a posterior sample `h`\n    h = pyro.sample(\"h\", dist.Normal(torch.zeros(N), 1))\n    gpr1.y = h\n    gpr2.X = h\n    h_new_loc, _ = gpr1(X_new)\n    y_loc, _ = gpr2(h_new_loc)\n    pyro.sample(\"y\", dist.Delta(y_loc))\n\nWe will get predictions from this predictive model by using samples from posterior.\n\nX_test = torch.linspace(-0.5, 1.5, 300)\nposterior_predictive = Predictive(predictive, mcmc.get_samples())\ny_pred = posterior_predictive.get_samples(X_test)[\"y\"].detach()\n\n# plot 10 predictions\nidx = np.random.randint(0, 100, 10)\nfor i in range(10):\n    plt.plot(X_test.numpy(), y_pred[idx[i]].numpy(), 'r-')\nplt.plot(X.numpy(), y.numpy(), \"kx\");\n\n\n\n\n\n\n\n\nUnfortunately, HMC seems to not play well with this deep GP setting. I don’t know how to make it works. Probably I should use sparse GP models…"
  },
  {
    "objectID": "blog/deep-gaussian-process.html#svi-with-mnist-data",
    "href": "blog/deep-gaussian-process.html#svi-with-mnist-data",
    "title": "Inferences for Deep Gaussian Process models in Pyro",
    "section": "SVI with MNIST data",
    "text": "SVI with MNIST data\nFirst, we download the MNIST data.\n\ntrain_loader = get_data_loader(dataset_name='MNIST',\n                               data_dir='~/.data',\n                               batch_size=1000,\n                               is_training_set=True,\n                               shuffle=True)\ntest_loader = get_data_loader(dataset_name='MNIST',\n                              data_dir='~/.data',\n                              batch_size=1000,\n                              is_training_set=False,\n                              shuffle=False)\n\ndownloading data\ndownload complete.\ndownloading data\ndownload complete.\n\n\n\nX = train_loader.dataset.data.reshape(-1, 784).float() / 255\ny = train_loader.dataset.targets\n\nNow, we initialize inducing points for the first layer by using k-mean of X. It is not necessary though, and taking a random subset of X also works.\n\nXu = torch.from_numpy(kmeans2(X.numpy(), 100, minit='points')[0])\n# let's plot one of the inducing points\nplt.imshow(Xu[0].reshape(28, 28));\n\n\n\n\n\n\n\n\nIn addition, as mentioned in the section “Further Model Details” of [2], a linear mean function is required. We follow the same approach here.\n\nclass LinearT(nn.Module):\n    \"\"\"Linear transform and transpose\"\"\"\n    def __init__(self, dim_in, dim_out):\n        super().__init__()\n        self.linear = nn.Linear(dim_in, dim_out, bias=False)\n\n    def forward(self, x):\n        return self.linear(x).t()\n\n# computes the weight for mean function of the first layer;\n# it is PCA of X (from 784D to 30D).\n_, _, V = np.linalg.svd(X.numpy(), full_matrices=False)\nW = torch.from_numpy(V[:30, :])\n\nmean_fn = LinearT(784, 30)\nmean_fn.linear.weight.data = W\nmean_fn.linear.weight.requires_grad_(False);\n\nNow, we create a deep GP model by stacking 2 variational sparse layers. The first layer includes a mean function (which is defined as above), while the second layer includes a multi-class likelihood. Note that inducing inputs of second layer are initialized by taking the output of mean function on inducing inputs from first layer.\n\nclass DeepGP(pyro.nn.PyroModule):\n    def __init__(self, X, y, Xu, mean_fn):\n        super(DeepGP, self).__init__()\n        self.layer1 = gp.models.VariationalSparseGP(\n            X,\n            None,\n            gp.kernels.RBF(784, variance=torch.tensor(2.), lengthscale=torch.tensor(2.)),\n            Xu=Xu,\n            likelihood=None,\n            mean_function=mean_fn,\n            latent_shape=torch.Size([30]))\n        # make sure that the input for next layer is batch_size x 30\n        h = mean_fn(X).t()\n        hu = mean_fn(Xu).t()\n        self.layer2 = gp.models.VariationalSparseGP(\n            h,\n            y,\n            gp.kernels.RBF(30, variance=torch.tensor(2.), lengthscale=torch.tensor(2.)),\n            Xu=hu,\n            likelihood=gp.likelihoods.MultiClass(num_classes=10),\n            latent_shape=torch.Size([10]))\n\n    def model(self, X, y):\n        self.layer1.set_data(X, None)\n        h_loc, h_var = self.layer1.model()\n        # approximate with a Monte Carlo sample (formula 15 of [1])\n        h = dist.Normal(h_loc, h_var.sqrt())()\n        self.layer2.set_data(h.t(), y)\n        self.layer2.model()\n\n    def guide(self, X, y):\n        self.layer1.guide()\n        self.layer2.guide()\n\n    # make prediction\n    def forward(self, X_new):\n        # because prediction is stochastic (due to Monte Carlo sample of hidden layer),\n        # we make 100 prediction and take the most common one (as in [4])\n        pred = []\n        for _ in range(100):\n            h_loc, h_var = self.layer1(X_new)\n            h = dist.Normal(h_loc, h_var.sqrt())()\n            f_loc, f_var = self.layer2(h.t())\n            pred.append(f_loc.argmax(dim=0))\n        return torch.stack(pred).mode(dim=0)[0]\n\nDuring early iterations of training process, we want to play more weight on mean function, which is PCA of the input, and reduce the effect of the first layer’s kernel. To achieve that, we’ll force the inducing outputs of the first layer to be small by setting small initial u_scale_tril.\n\ndeepgp = DeepGP(X, y, Xu, mean_fn)\ndeepgp.layer1.u_scale_tril = deepgp.layer1.u_scale_tril * 1e-5\ndeepgp.cuda()\n\noptimizer = torch.optim.Adam(deepgp.parameters(), lr=0.01)\nloss_fn = TraceMeanField_ELBO().differentiable_loss\n\nNow, we make some utitilies to train and test our model, just like other PyTorch models.\n\ndef train(train_loader, gpmodule, optimizer, loss_fn, epoch):\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.cuda(), target.cuda()\n        data = data.reshape(-1, 784)\n        optimizer.zero_grad()\n        loss = loss_fn(gpmodule.model, gpmodule.guide, data, target)\n        loss.backward()\n        optimizer.step()\n        idx = batch_idx + 1\n        if idx % 10 == 0:\n            print(\"Train Epoch: {:2d} [{:5d}/{} ({:2.0f}%)]\\tLoss: {:.6f}\"\n                  .format(epoch, idx * len(data), len(train_loader.dataset),\n                          100. * idx / len(train_loader), loss))\n\ndef test(test_loader, gpmodule):\n    correct = 0\n    for data, target in test_loader:\n        data, target = data.cuda(), target.cuda()\n        data = data.reshape(-1, 784)\n        pred = gpmodule(data)\n        # compare prediction and target to count accuaracy\n        correct += pred.eq(target).long().cpu().sum().item()\n\n    print(\"\\nTest set: Accuracy: {}/{} ({:.2f}%)\\n\"\n          .format(correct, len(test_loader.dataset), 100. * correct / len(test_loader.dataset)))\n\nHere I just run 20 steps to illustrate the process.\n\nfor i in range(20):\n    train(train_loader, deepgp, optimizer, loss_fn, i)\n    with torch.no_grad():\n        test(test_loader, deepgp)\n\nUserWarning: linear.weight was not registered in the param store because requires_grad=False\n\n\nTrain Epoch:  0 [10000/60000 (17%)] Loss: 212735.531250\nTrain Epoch:  0 [20000/60000 (33%)] Loss: 210797.875000\nTrain Epoch:  0 [30000/60000 (50%)] Loss: 198544.937500\nTrain Epoch:  0 [40000/60000 (67%)] Loss: 188923.781250\nTrain Epoch:  0 [50000/60000 (83%)] Loss: 173786.343750\nTrain Epoch:  0 [60000/60000 (100%)]    Loss: 141733.687500\n\nTest set: Accuracy: 8561/10000 (85.61%)\n\nTrain Epoch:  1 [10000/60000 (17%)] Loss: 97287.031250\nTrain Epoch:  1 [20000/60000 (33%)] Loss: 70768.703125\nTrain Epoch:  1 [30000/60000 (50%)] Loss: 57853.875000\nTrain Epoch:  1 [40000/60000 (67%)] Loss: 52993.148438\nTrain Epoch:  1 [50000/60000 (83%)] Loss: 50858.843750\nTrain Epoch:  1 [60000/60000 (100%)]    Loss: 46951.226562\n\nTest set: Accuracy: 9367/10000 (93.67%)\n\nTrain Epoch:  2 [10000/60000 (17%)] Loss: 48359.058594\nTrain Epoch:  2 [20000/60000 (33%)] Loss: 47204.945312\nTrain Epoch:  2 [30000/60000 (50%)] Loss: 46667.531250\nTrain Epoch:  2 [40000/60000 (67%)] Loss: 45861.250000\nTrain Epoch:  2 [50000/60000 (83%)] Loss: 44377.960938\nTrain Epoch:  2 [60000/60000 (100%)]    Loss: 42872.648438\n\nTest set: Accuracy: 9452/10000 (94.52%)\n\nTrain Epoch:  3 [10000/60000 (17%)] Loss: 43718.472656\nTrain Epoch:  3 [20000/60000 (33%)] Loss: 42701.117188\nTrain Epoch:  3 [30000/60000 (50%)] Loss: 42110.710938\nTrain Epoch:  3 [40000/60000 (67%)] Loss: 39179.777344\nTrain Epoch:  3 [50000/60000 (83%)] Loss: 42138.128906\nTrain Epoch:  3 [60000/60000 (100%)]    Loss: 39298.703125\n\nTest set: Accuracy: 9522/10000 (95.22%)\n\nTrain Epoch:  4 [10000/60000 (17%)] Loss: 37643.851562\nTrain Epoch:  4 [20000/60000 (33%)] Loss: 37684.792969\nTrain Epoch:  4 [30000/60000 (50%)] Loss: 37512.687500\nTrain Epoch:  4 [40000/60000 (67%)] Loss: 35796.890625\nTrain Epoch:  4 [50000/60000 (83%)] Loss: 39870.976562\nTrain Epoch:  4 [60000/60000 (100%)]    Loss: 38538.500000\n\nTest set: Accuracy: 9559/10000 (95.59%)\n\nTrain Epoch:  5 [10000/60000 (17%)] Loss: 35679.082031\nTrain Epoch:  5 [20000/60000 (33%)] Loss: 34451.484375\nTrain Epoch:  5 [30000/60000 (50%)] Loss: 34536.820312\nTrain Epoch:  5 [40000/60000 (67%)] Loss: 34720.652344\nTrain Epoch:  5 [50000/60000 (83%)] Loss: 35523.476562\nTrain Epoch:  5 [60000/60000 (100%)]    Loss: 34996.195312\n\nTest set: Accuracy: 9587/10000 (95.87%)\n\nTrain Epoch:  6 [10000/60000 (17%)] Loss: 32602.664062\nTrain Epoch:  6 [20000/60000 (33%)] Loss: 32038.414062\nTrain Epoch:  6 [30000/60000 (50%)] Loss: 32766.855469\nTrain Epoch:  6 [40000/60000 (67%)] Loss: 31712.023438\nTrain Epoch:  6 [50000/60000 (83%)] Loss: 32820.464844\nTrain Epoch:  6 [60000/60000 (100%)]    Loss: 30860.464844\n\nTest set: Accuracy: 9605/10000 (96.05%)\n\nTrain Epoch:  7 [10000/60000 (17%)] Loss: 33453.183594\nTrain Epoch:  7 [20000/60000 (33%)] Loss: 29956.320312\nTrain Epoch:  7 [30000/60000 (50%)] Loss: 30775.042969\nTrain Epoch:  7 [40000/60000 (67%)] Loss: 30294.603516\nTrain Epoch:  7 [50000/60000 (83%)] Loss: 30889.650391\nTrain Epoch:  7 [60000/60000 (100%)]    Loss: 32131.339844\n\nTest set: Accuracy: 9614/10000 (96.14%)\n\nTrain Epoch:  8 [10000/60000 (17%)] Loss: 31358.939453\nTrain Epoch:  8 [20000/60000 (33%)] Loss: 28076.316406\nTrain Epoch:  8 [30000/60000 (50%)] Loss: 28632.076172\nTrain Epoch:  8 [40000/60000 (67%)] Loss: 27690.781250\nTrain Epoch:  8 [50000/60000 (83%)] Loss: 26535.878906\nTrain Epoch:  8 [60000/60000 (100%)]    Loss: 27562.824219\n\nTest set: Accuracy: 9634/10000 (96.34%)\n\nTrain Epoch:  9 [10000/60000 (17%)] Loss: 28282.326172\nTrain Epoch:  9 [20000/60000 (33%)] Loss: 29352.490234\nTrain Epoch:  9 [30000/60000 (50%)] Loss: 24757.939453\nTrain Epoch:  9 [40000/60000 (67%)] Loss: 25494.769531\nTrain Epoch:  9 [50000/60000 (83%)] Loss: 25824.050781\nTrain Epoch:  9 [60000/60000 (100%)]    Loss: 22999.310547\n\nTest set: Accuracy: 9658/10000 (96.58%)\n\nTrain Epoch: 10 [10000/60000 (17%)] Loss: 23991.023438\nTrain Epoch: 10 [20000/60000 (33%)] Loss: 24084.507812\nTrain Epoch: 10 [30000/60000 (50%)] Loss: 24464.601562\nTrain Epoch: 10 [40000/60000 (67%)] Loss: 22447.724609\nTrain Epoch: 10 [50000/60000 (83%)] Loss: 19856.837891\nTrain Epoch: 10 [60000/60000 (100%)]    Loss: 23963.990234\n\nTest set: Accuracy: 9649/10000 (96.49%)\n\nTrain Epoch: 11 [10000/60000 (17%)] Loss: 22738.152344\nTrain Epoch: 11 [20000/60000 (33%)] Loss: 19739.845703\nTrain Epoch: 11 [30000/60000 (50%)] Loss: 21024.250000\nTrain Epoch: 11 [40000/60000 (67%)] Loss: 20558.265625\nTrain Epoch: 11 [50000/60000 (83%)] Loss: 21118.412109\nTrain Epoch: 11 [60000/60000 (100%)]    Loss: 21540.863281\n\nTest set: Accuracy: 9657/10000 (96.57%)\n\nTrain Epoch: 12 [10000/60000 (17%)] Loss: 21006.828125\nTrain Epoch: 12 [20000/60000 (33%)] Loss: 19925.472656\nTrain Epoch: 12 [30000/60000 (50%)] Loss: 19322.695312\nTrain Epoch: 12 [40000/60000 (67%)] Loss: 19545.306641\nTrain Epoch: 12 [50000/60000 (83%)] Loss: 18559.328125\nTrain Epoch: 12 [60000/60000 (100%)]    Loss: 18406.792969\n\nTest set: Accuracy: 9662/10000 (96.62%)\n\nTrain Epoch: 13 [10000/60000 (17%)] Loss: 18448.007812\nTrain Epoch: 13 [20000/60000 (33%)] Loss: 19414.882812\nTrain Epoch: 13 [30000/60000 (50%)] Loss: 15840.240234\nTrain Epoch: 13 [40000/60000 (67%)] Loss: 16599.394531\nTrain Epoch: 13 [50000/60000 (83%)] Loss: 16958.341797\nTrain Epoch: 13 [60000/60000 (100%)]    Loss: 18603.949219\n\nTest set: Accuracy: 9682/10000 (96.82%)\n\nTrain Epoch: 14 [10000/60000 (17%)] Loss: 16310.580078\nTrain Epoch: 14 [20000/60000 (33%)] Loss: 16222.394531\nTrain Epoch: 14 [30000/60000 (50%)] Loss: 17020.830078\nTrain Epoch: 14 [40000/60000 (67%)] Loss: 16931.587891\nTrain Epoch: 14 [50000/60000 (83%)] Loss: 15865.208008\nTrain Epoch: 14 [60000/60000 (100%)]    Loss: 16413.251953\n\nTest set: Accuracy: 9686/10000 (96.86%)\n\nTrain Epoch: 15 [10000/60000 (17%)] Loss: 15973.184570\nTrain Epoch: 15 [20000/60000 (33%)] Loss: 14486.261719\nTrain Epoch: 15 [30000/60000 (50%)] Loss: 16237.924805\nTrain Epoch: 15 [40000/60000 (67%)] Loss: 14279.332031\nTrain Epoch: 15 [50000/60000 (83%)] Loss: 16170.495117\nTrain Epoch: 15 [60000/60000 (100%)]    Loss: 15592.309570\n\nTest set: Accuracy: 9691/10000 (96.91%)\n\nTrain Epoch: 16 [10000/60000 (17%)] Loss: 13618.917969\nTrain Epoch: 16 [20000/60000 (33%)] Loss: 14135.177734\nTrain Epoch: 16 [30000/60000 (50%)] Loss: 13871.028320\nTrain Epoch: 16 [40000/60000 (67%)] Loss: 14454.966797\nTrain Epoch: 16 [50000/60000 (83%)] Loss: 15427.101562\nTrain Epoch: 16 [60000/60000 (100%)]    Loss: 13461.818359\n\nTest set: Accuracy: 9684/10000 (96.84%)\n\nTrain Epoch: 17 [10000/60000 (17%)] Loss: 13845.273438\nTrain Epoch: 17 [20000/60000 (33%)] Loss: 14617.742188\nTrain Epoch: 17 [30000/60000 (50%)] Loss: 13943.509766\nTrain Epoch: 17 [40000/60000 (67%)] Loss: 14690.369141\nTrain Epoch: 17 [50000/60000 (83%)] Loss: 16039.636719\nTrain Epoch: 17 [60000/60000 (100%)]    Loss: 14275.579102\n\nTest set: Accuracy: 9693/10000 (96.93%)\n\nTrain Epoch: 18 [10000/60000 (17%)] Loss: 13436.442383\nTrain Epoch: 18 [20000/60000 (33%)] Loss: 15688.101562\nTrain Epoch: 18 [30000/60000 (50%)] Loss: 15190.672852\nTrain Epoch: 18 [40000/60000 (67%)] Loss: 15058.431641\nTrain Epoch: 18 [50000/60000 (83%)] Loss: 14980.768555\nTrain Epoch: 18 [60000/60000 (100%)]    Loss: 12926.894531\n\nTest set: Accuracy: 9690/10000 (96.90%)\n\nTrain Epoch: 19 [10000/60000 (17%)] Loss: 14068.406250\nTrain Epoch: 19 [20000/60000 (33%)] Loss: 14795.143555\nTrain Epoch: 19 [30000/60000 (50%)] Loss: 13799.461914\nTrain Epoch: 19 [40000/60000 (67%)] Loss: 14706.441406\nTrain Epoch: 19 [50000/60000 (83%)] Loss: 13907.800781\nTrain Epoch: 19 [60000/60000 (100%)]    Loss: 15816.891602\n\nTest set: Accuracy: 9707/10000 (97.07%)\n\n\n\nIn [4], the authors run 2-layer Deep GP for more than 300 epochs and achieve 97,94% accuaracy. Despite that stacking many layers can improve performance of Gaussian Processes, it seems to me that following the line of deep kernels is a more reliable approach. Kernels, which are usually underrated, are indeed the core of Gaussian Processes. As demonstrated in Pyro’s Deep Kernel Learning example, we can achieve a state-of-the-art result without having to tuning hyperparameters or using many tricks as in the above example (e.g. fixing a linear mean function, reducing the kernel effect of the first layer).\n\nReferences\n[1] MCMC for Variationally Sparse Gaussian Processes arxiv James Hensman, Alexander G. de G. Matthews, Maurizio Filippone, Zoubin Ghahramani\n[2] Doubly Stochastic Variational Inference for Deep Gaussian Processes arxiv Hugh Salimbeni, Marc Peter Deisenroth\n[3] https://github.com/ICL-SML/Doubly-Stochastic-DGP/blob/master/demos/demo_step_function.ipynb\n[4] https://github.com/ICL-SML/Doubly-Stochastic-DGP/blob/master/demos/demo_mnist.ipynb"
  },
  {
    "objectID": "blog/sampling-hmm-pyro.html",
    "href": "blog/sampling-hmm-pyro.html",
    "title": "Sampling Hidden Markov Model with Pyro",
    "section": "",
    "text": "To understand the multimodal phenomenon of unsupervised hidden Markov models (HMM) when reading some discussions in PyMC discourse, I decide to reimplement in Pyro various models from Stan. The main reference which we’ll use is Stan User’s Guide.\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set(palette=\"bright\")\nimport torch\nimport warnings; warnings.simplefilter(\"ignore\", FutureWarning)\n\n# this post assumes a Pyro version in dev branch (dated 2019-01-01):\n# pip install git+https://github.com/uber/pyro@4e42613\nimport pyro\nimport pyro.distributions as dist\nfrom pyro.infer.mcmc import MCMC, NUTS\n\npyro.set_rng_seed(1)\nAs in Stan user’s guide, we use the notation categories for latent states and words for observations. The following data information is taken from Stan’s example-models repository.\nnum_categories = 3\nnum_words = 10\nnum_supervised_data = 100\nnum_data = 600\n\ntransition_prior = torch.empty(num_categories).fill_(1.)\nemission_prior = torch.empty(num_words).fill_(0.1)\n\ntransition_prob = dist.Dirichlet(transition_prior).sample(torch.Size([num_categories]))\nemission_prob = dist.Dirichlet(emission_prior).sample(torch.Size([num_categories]))\nWe need to generate data randomly from the above transition probability and emission probability. In addition, we will generate an initial category from the equilibrium distribution of its Markov chain.\ndef equilibrium(mc_matrix):\n    n = mc_matrix.size(0)\n    return (torch.eye(n) - mc_matrix.t() + 1).inverse().matmul(torch.ones(n))\n\nstart_prob = equilibrium(transition_prob)\n\n# simulate data\ncategories, words = [], []\nfor t in range(num_data):\n    if t == 0 or t == num_supervised_data:\n        category = dist.Categorical(start_prob).sample()\n    else:\n        category = dist.Categorical(transition_prob[category]).sample()\n    word = dist.Categorical(emission_prob[category]).sample()\n    categories.append(category)\n    words.append(word)\ncategories, words = torch.stack(categories), torch.stack(words)\n\n# split into supervised data and unsupervised data\nsupervised_categories = categories[:num_supervised_data]\nsupervised_words = words[:num_supervised_data]\nunsupervised_words = words[num_supervised_data:]\nTo observe the posterior, which are samples drawn from a Markov chain Monte Carlo sampling, we’ll make a convenient plotting function.\ndef plot_posterior(mcmc):\n    # get `transition_prob` samples from posterior\n    trace_transition_prob = mcmc.get_samples()[\"transition_prob\"]\n\n    plt.figure(figsize=(10, 6))\n    for i in range(num_categories):\n        for j in range(num_categories):\n            sns.distplot(trace_transition_prob[:, i, j], hist=False, kde_kws={\"lw\": 2},\n                         label=\"transition_prob[{}, {}], true value = {:.2f}\"\n                         .format(i, j, transition_prob[i, j]))\n    plt.xlabel(\"Probability\", fontsize=13)\n    plt.ylabel(\"Frequency\", fontsize=13)\n    plt.title(\"Transition probability posterior\", fontsize=15)"
  },
  {
    "objectID": "blog/sampling-hmm-pyro.html#supervised-hmm",
    "href": "blog/sampling-hmm-pyro.html#supervised-hmm",
    "title": "Sampling Hidden Markov Model with Pyro",
    "section": "Supervised HMM",
    "text": "Supervised HMM\nWhen we know all hidden states (categories), we can use a supervised HMM model. Implementing it in Pyro is quite straightforward (to get familiar with Pyro, please checkout its tutorial page).\n\ndef supervised_hmm(categories, words):\n    with pyro.plate(\"prob_plate\", num_categories):\n        transition_prob = pyro.sample(\"transition_prob\", dist.Dirichlet(transition_prior))\n        emission_prob = pyro.sample(\"emission_prob\", dist.Dirichlet(emission_prior))\n\n    category = categories[0]  # start with first category\n    for t in range(len(words)):\n        if t &gt; 0:\n            category = pyro.sample(\"category_{}\".format(t), dist.Categorical(transition_prob[category]),\n                                   obs=categories[t])\n        pyro.sample(\"word_{}\".format(t), dist.Categorical(emission_prob[category]), obs=words[t])\n\n\n# enable jit_compile to improve the sampling speed\nnuts_kernel = NUTS(supervised_hmm, jit_compile=True, ignore_jit_warnings=True)\nmcmc = MCMC(nuts_kernel, num_samples=100)\n# we run MCMC to get posterior\nmcmc.run(supervised_categories, supervised_words)\n# after that, we plot the posterior\nplot_posterior(mcmc)\n\nSample: 100%|██████████| 200/200 [00:54,  3.70it/s, step size=5.18e-02, acc. prob=0.963]\n\n\n\n\n\n\n\n\n\nWe can see that MCMC gives a good posterior in this supervised context. Let’s see how things change for an unsupervised model."
  },
  {
    "objectID": "blog/sampling-hmm-pyro.html#unsupervised-hmm",
    "href": "blog/sampling-hmm-pyro.html#unsupervised-hmm",
    "title": "Sampling Hidden Markov Model with Pyro",
    "section": "Unsupervised HMM",
    "text": "Unsupervised HMM\nIn this case, we don’t know yet which categories generate observed words. These hidden states (categories) are discrete latent variables. Although Pyro supports maginalizing out discrete latent variables, we won’t use that technique here because it is slow for HMM. We instead will use the forward algorithm to reduce time complexity.\n\ndef forward_log_prob(prev_log_prob, curr_word, transition_log_prob, emission_log_prob):\n    log_prob = emission_log_prob[:, curr_word] + transition_log_prob + prev_log_prob.unsqueeze(dim=1)\n    return log_prob.logsumexp(dim=0)\n\n\ndef unsupervised_hmm(words):\n    with pyro.plate(\"prob_plate\", num_categories):\n        transition_prob = pyro.sample(\"transition_prob\", dist.Dirichlet(transition_prior))\n        emission_prob = pyro.sample(\"emission_prob\", dist.Dirichlet(emission_prior))\n\n    transition_log_prob = transition_prob.log()\n    emission_log_prob = emission_prob.log()\n    log_prob = emission_log_prob[:, words[0]]\n    for t in range(1, len(words)):\n        log_prob = forward_log_prob(log_prob, words[t], transition_log_prob, emission_log_prob)\n    prob = log_prob.logsumexp(dim=0).exp()\n    # a trick to inject an additional log_prob into model's log_prob\n    pyro.sample(\"forward_prob\", dist.Bernoulli(prob), obs=torch.tensor(1.))\n\n\nnuts_kernel = NUTS(unsupervised_hmm, jit_compile=True, ignore_jit_warnings=True)\nmcmc = MCMC(nuts_kernel, num_samples=100)\nmcmc.run(unsupervised_words)\nplot_posterior(mcmc)\n\nSample: 100%|██████████| 200/200 [05:24,  1.62s/it, step size=1.52e-01, acc. prob=0.860]\n\n\n\n\n\n\n\n\n\nWe can see that the posterior distributions highly spread over the interval \\([0, 1]\\) (though they seem to favor the first half). This posterior will not be useful for making further predictions."
  },
  {
    "objectID": "blog/sampling-hmm-pyro.html#semi-supervised-hmm",
    "href": "blog/sampling-hmm-pyro.html#semi-supervised-hmm",
    "title": "Sampling Hidden Markov Model with Pyro",
    "section": "Semi-supervised HMM",
    "text": "Semi-supervised HMM\nTo fix the above issue, we will use supervised data for inference.\n\ndef semisupervised_hmm(supervised_categories, supervised_words, unsupervised_words):\n    with pyro.plate(\"prob_plate\", num_categories):\n        transition_prob = pyro.sample(\"transition_prob\", dist.Dirichlet(transition_prior))\n        emission_prob = pyro.sample(\"emission_prob\", dist.Dirichlet(emission_prior))\n\n    category = supervised_categories[0]\n    for t in range(len(supervised_words)):\n        if t &gt; 0:\n            category = pyro.sample(\"category_{}\".format(t), dist.Categorical(transition_prob[category]),\n                                   obs=supervised_categories[t])\n        pyro.sample(\"word_{}\".format(t), dist.Categorical(emission_prob[category]),\n                    obs=supervised_words[t])\n\n    transition_log_prob = transition_prob.log()\n    emission_log_prob = emission_prob.log()\n    log_prob = emission_log_prob[:, unsupervised_words[0]]\n    for t in range(1, len(unsupervised_words)):\n        log_prob = forward_log_prob(log_prob, unsupervised_words[t],\n                                    transition_log_prob, emission_log_prob)\n    prob = log_prob.logsumexp(dim=0).exp()\n    pyro.sample(\"forward_prob\", dist.Bernoulli(prob), obs=torch.tensor(1.))\n\n\nnuts_kernel = NUTS(semisupervised_hmm, jit_compile=True, ignore_jit_warnings=True)\nmcmc = MCMC(nuts_kernel, num_samples=100)\nmcmc.run(supervised_categories, supervised_words, unsupervised_words)\nplot_posterior(mcmc)\n\nSample: 100%|██████████| 200/200 [10:20,  3.10s/it, step size=9.60e-02, acc. prob=0.857]\n\n\n\n\n\n\n\n\n\nThe posterior is much better now. Which means that the additional information from supervised data has helped a lot!"
  },
  {
    "objectID": "blog/sampling-hmm-pyro.html#some-takeaways",
    "href": "blog/sampling-hmm-pyro.html#some-takeaways",
    "title": "Sampling Hidden Markov Model with Pyro",
    "section": "Some takeaways",
    "text": "Some takeaways\n\nWhen we don’t have much labeled data, consider using semi-supervised learning.\nUsing additional algorithms (which include the forward algorithm in this case) can significantly improve the speed of our models.\n\nFor a variational inference approach to HMM, please check out this excellent example in Pyro tutorial page."
  },
  {
    "objectID": "blog/grapheme-to-phoneme.html",
    "href": "blog/grapheme-to-phoneme.html",
    "title": "How to build a Grapheme-to-Phoneme (G2P) model using PyTorch",
    "section": "",
    "text": "introduction\nGrapheme-to-Phoneme (G2P) model is one of the core components of a typical Text-to-Speech (TTS) system, e.g. WaveNet and Deep Voice. In this notebook, we will try to replicate the Encoder-decoder LSTM model from the paper https://arxiv.org/abs/1506.00196.\nThroughout this tutorial, we will learn how to: + Implement a sequence-to-sequence (seq2seq) model + Implement global attention into seq2seq model + Use beam-search decoder + Use Levenshtein distance to compute phoneme-error-rate (PER) + Use torchtext package\n\n\nsetup\nFirst, we will import necessary modules. You can install PyTorch as suggested in its main page. To install torchtext, simply call &gt; pip install git+https://github.com/pytorch/text.git\nDue to this bug, it is important to update your torchtext to the lastest version (using the above installing command is enough).\n\nimport argparse\nimport os\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.optim as optim\nfrom torch.nn.utils import clip_grad_norm\nimport torchtext.data as data\n\nargparse is a default python module which is used for command-line script parsing. To run this notebook as a python script, simply comment out all the markdown cell and change the following code cell to the real argparse code. \n\nparser = {\n    'data_path': '../data/cmudict/',\n    'epochs': 50,\n    'batch_size': 100,\n    'max_len': 20,  # max length of grapheme/phoneme sequences\n    'beam_size': 3,  # size of beam for beam-search\n    'd_embed': 500,  # embedding dimension\n    'd_hidden': 500,  # hidden dimension\n    'attention': True,  # use attention or not\n    'log_every': 100,  # number of iterations to log and validate training\n    'lr': 0.007,  # initial learning rate\n    'lr_decay': 0.5,  # decay lr when not observing improvement in val_loss\n    'lr_min': 1e-5,  # stop when lr is too low\n    'n_bad_loss': 5,  # number of bad val_loss before decaying\n    'clip': 2.3,  # clip gradient, to avoid exploding gradient\n    'cuda': True,  # using gpu or not\n    'seed': 5,  # initial seed\n    'intermediate_path': '../intermediate/g2p/',  # path to save models\n}\nargs = argparse.Namespace(**parser)\n\nNext, we need to download the data. We will use the free CMUdict dataset. The seed 5 is used to generate random numbers for the purpose of replicating the result. However, we still observe distinct scores for different runs of the notebook.\n\nargs.cuda = args.cuda and torch.cuda.is_available()\n\nif not os.path.isdir(args.intermediate_path):\n    os.makedirs(args.intermediate_path)\nif not os.path.isdir(args.data_path):\n    URL = \"https://github.com/cmusphinx/cmudict/archive/master.zip\"\n    !wget $URL -O ../data/cmudict.zip\n    !unzip ../data/cmudict.zip -d ../data/\n    !mv ../data/cmudict-master $args.data_path\n\ntorch.manual_seed(args.seed)\nif args.cuda:\n    torch.cuda.manual_seed(args.seed)\n\n\n\nmodel\nNow, it is time to define our model. The following figure (taken from the paper) is a two layer Encoder-Decoder LSTM model (which is a variant of Sequence-to-Sequence model). To understand how LSTM works, we can look at the excellent blog post http://colah.github.io/posts/2015-08-Understanding-LSTMs/. Each rectangle in the figure will be an LSTMCell in our code.\n\nBefore looking into the code, we need to review some PyTorch modules and functions: + nn.Embedding: a lookup table to convert indices to vectors. Theoretically, it does one-hot encoding followed by a fully connected layer (with no bias). + nn.Linear: nothing but a fully connected layer. + nn.LSTMCell: a long short-term memory cell, which is mentioned above. + size: get the size of tensor. + unsqueeze: create a new dimension (with size 1) for a tensor. + squeeze: drop a (size 1) dimension of a tensor. + chunk: split a tensor along a dimension into smaller-size tensors. There is also the function split which help us obtain the same effect. + stack: concatenate a list of tensors along a new dimension. If we want to concatenate a long a “known” dimension, then we can use cat function. + bmm: batch matrix multiplication. + index_select: select values of a tensor by providing indices. + F.softmax, F.tanh: non-linear activation functions.\nPyTorch’s implementation of the encoder is quite straight forward. If you are not familiar with PyTorch, we recommend you to look at the official tutorials. It is noted that the dimension for input tensor x_seq is seq_len x batch_size. After embedding, we get a tensor of size seq_len x batch_size x vector_dim, not batch_size x seq_len x vector_dim. For us, this order of dimensions is useful for getting subsequence tensor, or an element of the sequence (for examples, to get the first element of the sequence x_seq, we just take x_seq[0]). Note that this is also the default order of input tensor for any recurrent module in PyTorch.\n\nclass Encoder(nn.Module):\n\n    def __init__(self, vocab_size, d_embed, d_hidden):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_embed)\n        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n        self.d_hidden = d_hidden\n\n    def forward(self, x_seq, cuda=False):\n        o = []\n        e_seq = self.embedding(x_seq)  # seq x batch x dim\n        tt = torch.cuda if cuda else torch  # use cuda tensor or not\n        # create initial hidden state and initial cell state\n        h = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n        c = Variable(tt.FloatTensor(e_seq.size(1), self.d_hidden).zero_())\n        \n        for e in e_seq.chunk(e_seq.size(0), 0):\n            e = e.squeeze(0)\n            h, c = self.lstm(e, (h, c))\n            o.append(h)\n        return torch.stack(o, 0), h, c\n\nNext, we want to implement the decoder with attention mechanism. The article http://distill.pub/2016/augmented-rnns/ explains very well the idea behind the notion “attention”. Here we use dot global attention from the paper https://arxiv.org/abs/1508.04025. (The following figure is taken from this blog.)\n\n\n# Based on https://github.com/OpenNMT/OpenNMT-py\n\nclass Attention(nn.Module):\n    \"\"\"Dot global attention from https://arxiv.org/abs/1508.04025\"\"\"\n    def __init__(self, dim):\n        super(Attention, self).__init__()\n        self.linear = nn.Linear(dim*2, dim, bias=False)\n        \n    def forward(self, x, context=None):\n        if context is None:\n            return x\n        assert x.size(0) == context.size(0)  # x: batch x dim\n        assert x.size(1) == context.size(2)  # context: batch x seq x dim\n        attn = F.softmax(context.bmm(x.unsqueeze(2)).squeeze(2))\n        weighted_context = attn.unsqueeze(1).bmm(context).squeeze(1)\n        o = self.linear(torch.cat((x, weighted_context), 1))\n        return F.tanh(o)\n\nNote that if we do not want to add attention to the decoder, then simply set the args.attention to False. In our experiment, adding attention gave us worse result.\n\nclass Decoder(nn.Module):\n    \n    def __init__(self, vocab_size, d_embed, d_hidden):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(vocab_size, d_embed)\n        self.lstm = nn.LSTMCell(d_embed, d_hidden)\n        self.attn = Attention(d_hidden)\n        self.linear = nn.Linear(d_hidden, vocab_size)\n\n    def forward(self, x_seq, h, c, context=None):\n        o = []\n        e_seq = self.embedding(x_seq)\n        for e in e_seq.chunk(e_seq.size(0), 0):\n            e = e.squeeze(0)\n            h, c = self.lstm(e, (h, c))\n            o.append(self.attn(h, context))\n        o = torch.stack(o, 0)\n        o = self.linear(o.view(-1, h.size(1)))\n        return F.log_softmax(o).view(x_seq.size(0), -1, o.size(1)), h, c\n\nThe following G2P model is a combination of the above encoder and decoder into an end-to-end setting. We also use beam search to find the best converted phoneme sequence. To learn more about beam search, the following clip is helpful. In the implementation of beam search, we deal with one sequence at a time (try to find the phoneme sequence ending with token eos). So we have to make sure batch_size == 1.\n\nclass G2P(nn.Module):\n    \n    def __init__(self, config):\n        super(G2P, self).__init__()\n        self.encoder = Encoder(config.g_size, config.d_embed,\n                               config.d_hidden)\n        self.decoder = Decoder(config.p_size, config.d_embed,\n                               config.d_hidden)\n        self.config = config\n        \n    def forward(self, g_seq, p_seq=None):\n        o, h, c = self.encoder(g_seq, self.config.cuda)\n        context = o.t() if self.config.attention else None\n        if p_seq is not None:  # not generate\n            return self.decoder(p_seq, h, c, context)\n        else:\n            assert g_seq.size(1) == 1  # make sure batch_size = 1\n            return self._generate(h, c, context)\n        \n    def _generate(self, h, c, context):\n        beam = Beam(self.config.beam_size, cuda=self.config.cuda)\n        # Make a beam_size batch.\n        h = h.expand(beam.size, h.size(1))  \n        c = c.expand(beam.size, c.size(1))\n        context = context.expand(beam.size, context.size(1), context.size(2))\n        \n        for i in range(self.config.max_len):  # max_len = 20\n            x = beam.get_current_state()\n            o, h, c = self.decoder(Variable(x.unsqueeze(0)), h, c, context)\n            if beam.advance(o.data.squeeze(0)):\n                break\n            h.data.copy_(h.data.index_select(0, beam.get_current_origin()))\n            c.data.copy_(c.data.index_select(0, beam.get_current_origin()))\n        tt = torch.cuda if self.config.cuda else torch\n        return Variable(tt.LongTensor(beam.get_hyp(0)))\n\n\n\nutils\nThe following class is the implementation of Beam search. Note that the special tokens pad, bos, eos have to match the corresponding tokens in phoneme dictionary.\n\n# Based on https://github.com/MaximumEntropy/Seq2Seq-PyTorch/\nclass Beam(object):\n    \"\"\"Ordered beam of candidate outputs.\"\"\"\n\n    def __init__(self, size, pad=1, bos=2, eos=3, cuda=False):\n        \"\"\"Initialize params.\"\"\"\n        self.size = size\n        self.done = False\n        self.pad = pad\n        self.bos = bos\n        self.eos = eos\n        self.tt = torch.cuda if cuda else torch\n\n        # The score for each translation on the beam.\n        self.scores = self.tt.FloatTensor(size).zero_()\n\n        # The backpointers at each time-step.\n        self.prevKs = []\n\n        # The outputs at each time-step.\n        self.nextYs = [self.tt.LongTensor(size).fill_(self.pad)]\n        self.nextYs[0][0] = self.bos\n\n    # Get the outputs for the current timestep.\n    def get_current_state(self):\n        \"\"\"Get state of beam.\"\"\"\n        return self.nextYs[-1]\n\n    # Get the backpointers for the current timestep.\n    def get_current_origin(self):\n        \"\"\"Get the backpointer to the beam at this step.\"\"\"\n        return self.prevKs[-1]\n\n    def advance(self, workd_lk):\n        \"\"\"Advance the beam.\"\"\"\n        num_words = workd_lk.size(1)\n\n        # Sum the previous scores.\n        if len(self.prevKs) &gt; 0:\n            beam_lk = workd_lk + self.scores.unsqueeze(1).expand_as(workd_lk)\n        else:\n            beam_lk = workd_lk[0]\n\n        flat_beam_lk = beam_lk.view(-1)\n\n        bestScores, bestScoresId = flat_beam_lk.topk(self.size, 0,\n                                                     True, True)\n        self.scores = bestScores\n\n        # bestScoresId is flattened beam x word array, so calculate which\n        # word and beam each score came from\n        prev_k = bestScoresId / num_words\n        self.prevKs.append(prev_k)\n        self.nextYs.append(bestScoresId - prev_k * num_words)\n        # End condition is when top-of-beam is EOS.\n        if self.nextYs[-1][0] == self.eos:\n            self.done = True\n        return self.done\n\n    def get_hyp(self, k):\n        \"\"\"Get hypotheses.\"\"\"\n        hyp = []\n        # print(len(self.prevKs), len(self.nextYs), len(self.attn))\n        for j in range(len(self.prevKs) - 1, -1, -1):\n            hyp.append(self.nextYs[j + 1][k])\n            k = self.prevKs[j][k]\n        return hyp[::-1]\n\nLevenshtein distance is used to compute phoneme-error-rate (PER) for phoneme sequences (similar to word-error-rate for word sequences). In the paper, there is another metric named word-error-rate, which is obtained by calculating the number of wrong predictions. For example, the phoneme sequence “S W AY1 G ER0 D” is a wrong prediction for the word “sweigard” (real phoneme sequence is “S W EY1 G ER0 D”). Please not to be confused between these two metrics which have the same name.\n\n# Based on https://github.com/SeanNaren/deepspeech.pytorch/blob/master/decoder.py.\nimport Levenshtein  # https://github.com/ztane/python-Levenshtein/\n\ndef phoneme_error_rate(p_seq1, p_seq2):\n    p_vocab = set(p_seq1 + p_seq2)\n    p2c = dict(zip(p_vocab, range(len(p_vocab))))\n    c_seq1 = [chr(p2c[p]) for p in p_seq1]\n    c_seq2 = [chr(p2c[p]) for p in p_seq2]\n    return Levenshtein.distance(''.join(c_seq1),\n                                ''.join(c_seq2)) / len(c_seq2)\n\nThe following function helps to adjust learning rate for optimizer. Learning rate will be decayed if we do not see any improvement of the loss after args.n_bad_loss iterations.\n\ndef adjust_learning_rate(optimizer, lr_decay):\n    for param_group in optimizer.param_groups:\n        param_group['lr'] *= lr_decay\n\n\n\ntrain\nThe following functions will be used to train model, validate model (using early stopping). Then we apply the final model for test data to get WER and PER (using test function). Finally, the show function will display a few examples for us.\n\ndef train(config, train_iter, model, criterion, optimizer, epoch):\n    global iteration, n_total, train_loss, n_bad_loss\n    global init, best_val_loss, stop\n    \n    print(\"=&gt; EPOCH {}\".format(epoch))\n    train_iter.init_epoch()\n    for batch in train_iter:\n        iteration += 1\n        model.train()\n        \n        output, _, __ = model(batch.grapheme, batch.phoneme[:-1].detach())\n        target = batch.phoneme[1:]\n        loss = criterion(output.view(output.size(0) * output.size(1), -1),\n                         target.view(target.size(0) * target.size(1)))\n        \n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm(model.parameters(), config.clip, 'inf')\n        optimizer.step()\n        \n        n_total += batch.batch_size\n        train_loss += loss.data[0] * batch.batch_size\n        \n        if iteration % config.log_every == 0:\n            train_loss /= n_total\n            val_loss = validate(val_iter, model, criterion)\n            print(\"   % Time: {:5.0f} | Iteration: {:5} | Batch: {:4}/{}\"\n                  \" | Train loss: {:.4f} | Val loss: {:.4f}\"\n                  .format(time.time()-init, iteration, train_iter.iterations,\n                          len(train_iter), train_loss, val_loss))\n            \n            # test for val_loss improvement\n            n_total = train_loss = 0\n            if val_loss &lt; best_val_loss:\n                best_val_loss = val_loss\n                n_bad_loss = 0\n                torch.save(model.state_dict(), config.best_model)\n            else:\n                n_bad_loss += 1\n            if n_bad_loss == config.n_bad_loss:\n                best_val_loss = val_loss\n                n_bad_loss = 0\n                adjust_learning_rate(optimizer, config.lr_decay)\n                new_lr = optimizer.param_groups[0]['lr']\n                print(\"=&gt; Adjust learning rate to: {}\".format(new_lr))\n                if new_lr &lt; config.lr_min:\n                    stop = True\n                    break                \n\n\ndef validate(val_iter, model, criterion):\n    model.eval()\n    val_loss = 0\n    val_iter.init_epoch()\n    for batch in val_iter:\n        output, _, __ = model(batch.grapheme, batch.phoneme[:-1])\n        target = batch.phoneme[1:]\n        loss = criterion(output.squeeze(1), target.squeeze(1))\n        val_loss += loss.data[0] * batch.batch_size\n    return val_loss / len(val_iter.dataset)\n\n\ndef test(test_iter, model, criterion):\n    model.eval()\n    test_iter.init_epoch()\n    test_per = test_wer = 0\n    for batch in test_iter:\n        output = model(batch.grapheme).data.tolist()\n        target = batch.phoneme[1:].squeeze(1).data.tolist()\n        # calculate per, wer here\n        per = phoneme_error_rate(output, target) \n        wer = int(output != target)\n        test_per += per  # batch_size = 1\n        test_wer += wer\n        \n    test_per = test_per / len(test_iter.dataset) * 100\n    test_wer = test_wer / len(test_iter.dataset) * 100\n    print(\"Phoneme error rate (PER): {:.2f}\\nWord error rate (WER): {:.2f}\"\n          .format(test_per, test_wer))\n\n\ndef show(batch, model):\n    assert batch.batch_size == 1\n    g_field = batch.dataset.fields['grapheme']\n    p_field = batch.dataset.fields['phoneme']\n    prediction = model(batch.grapheme).data.tolist()[:-1]\n    grapheme = batch.grapheme.squeeze(1).data.tolist()[1:][::-1]\n    phoneme = batch.phoneme.squeeze(1).data.tolist()[1:-1]\n    print(\"&gt; {}\\n= {}\\n&lt; {}\\n\".format(\n        ''.join([g_field.vocab.itos[g] for g in grapheme]),\n        ' '.join([p_field.vocab.itos[p] for p in phoneme]),\n        ' '.join([p_field.vocab.itos[p] for p in prediction])))\n\n\n\nprepare\nNow, we move to the exciting part. We will create a class CMUDict based on data.Dataset from torchtext. It is recommended to read the document to understand how the Dataset works. The splits function helps us divide data into three datasets: 17/20 for training, 1/20 for validating, 2/20 for reporting final results.\nThe class CMUDict contains all pairs of a grapheme sequence and the corresponding phoneme sequence. Each line of the raw cmudict.dict file has the form “aachener AA1 K AH0 N ER0”. We first split it into sequences aachener and AA1 K AH0 N ER0. Each of them is a sequence of data belongs to a Field (for example, a sentence is a sequence of words and word is the Field of sentences). How to tokenize these sequences is implemented in the tokenize parameters of the definition of grapheme field and phoneme field. We also add init token and end-of-sequence token as in the original paper.\n\ng_field = data.Field(init_token='&lt;s&gt;',\n                     tokenize=(lambda x: list(x.split('(')[0])[::-1]))\np_field = data.Field(init_token='&lt;os&gt;', eos_token='&lt;/os&gt;',\n                     tokenize=(lambda x: x.split('#')[0].split()))\n\n\nclass CMUDict(data.Dataset):\n\n    def __init__(self, data_lines, g_field, p_field):\n        fields = [('grapheme', g_field), ('phoneme', p_field)]\n        examples = []  # maybe ignore '...-1' grapheme\n        for line in data_lines:\n            grapheme, phoneme = line.split(maxsplit=1)\n            examples.append(data.Example.fromlist([grapheme, phoneme],\n                                                  fields))\n        self.sort_key = lambda x: len(x.grapheme)\n        super(CMUDict, self).__init__(examples, fields)\n    \n    @classmethod\n    def splits(cls, path, g_field, p_field, seed=None):\n        import random\n        \n        if seed is not None:\n            random.seed(seed)\n        with open(path) as f:\n            lines = f.readlines()\n        random.shuffle(lines)\n        train_lines, val_lines, test_lines = [], [], []\n        for i, line in enumerate(lines):\n            if i % 20 == 0:\n                val_lines.append(line)\n            elif i % 20 &lt; 3:\n                test_lines.append(line)\n            else:\n                train_lines.append(line)\n        train_data = cls(train_lines, g_field, p_field)\n        val_data = cls(val_lines, g_field, p_field)\n        test_data = cls(test_lines, g_field, p_field)\n        return (train_data, val_data, test_data)\n\n\nfilepath = os.path.join(args.data_path, 'cmudict.dict')\ntrain_data, val_data, test_data = CMUDict.splits(filepath, g_field, p_field,\n                                                 args.seed)\n\nTo make the dictionaries for grapheme field and phoneme field, we use the function build_vocab. Read its definition to get more information.\n\ng_field.build_vocab(train_data, val_data, test_data)\np_field.build_vocab(train_data, val_data, test_data)\n\nNow, we will make Iterator from our datasets. These iterators will help us get data in batch. The BucketIterator will make the sequences in each batch have similar length while still preserves the randomness.\n\ndevice = None if args.cuda else -1  # None is current gpu\ntrain_iter = data.BucketIterator(train_data, batch_size=args.batch_size,\n                                 repeat=False, device=device)\nval_iter = data.Iterator(val_data, batch_size=1,\n                         train=False, sort=False, device=device)\ntest_iter = data.Iterator(test_data, batch_size=1,\n                          train=False, shuffle=True, device=device)\n\nNow, it is time to create the model.\n\nconfig = args\nconfig.g_size = len(g_field.vocab)\nconfig.p_size = len(p_field.vocab)\nconfig.best_model = os.path.join(config.intermediate_path,\n                                 \"best_model_adagrad_attn.pth\")\n\nmodel = G2P(config)\ncriterion = nn.NLLLoss()\nif config.cuda:\n    model.cuda()\n    criterion.cuda()\noptimizer = optim.Adagrad(model.parameters(), lr=config.lr)  # use Adagrad\n\n\n\nrun\nWe start to train our model. It will be stopped if there is no observation on the improvement of validation loss. It take around 10 minutes for each epoch (trained on GTX 1060).\n\nif 1 == 1:  # change to True to train\n    iteration = n_total = train_loss = n_bad_loss = 0\n    stop = False\n    best_val_loss = 10\n    init = time.time()\n    for epoch in range(1, config.epochs+1):\n        train(config, train_iter, model, criterion, optimizer, epoch)\n        if stop:\n            break\n\n=&gt; EPOCH 1\n   % Time:    56 | Iteration:   100 | Batch:  100/1148 | Train loss: 1.3804 | Val loss: 0.8835\n   % Time:   111 | Iteration:   200 | Batch:  200/1148 | Train loss: 0.5339 | Val loss: 0.5970\n   % Time:   166 | Iteration:   300 | Batch:  300/1148 | Train loss: 0.4361 | Val loss: 0.5435\n   % Time:   220 | Iteration:   400 | Batch:  400/1148 | Train loss: 0.3836 | Val loss: 0.4764\n   % Time:   275 | Iteration:   500 | Batch:  500/1148 | Train loss: 0.3578 | Val loss: 0.4410\n   % Time:   331 | Iteration:   600 | Batch:  600/1148 | Train loss: 0.3315 | Val loss: 0.4162\n   % Time:   387 | Iteration:   700 | Batch:  700/1148 | Train loss: 0.3230 | Val loss: 0.4009\n   % Time:   442 | Iteration:   800 | Batch:  800/1148 | Train loss: 0.3186 | Val loss: 0.4340\n   % Time:   498 | Iteration:   900 | Batch:  900/1148 | Train loss: 0.2955 | Val loss: 0.3801\n   % Time:   554 | Iteration:  1000 | Batch: 1000/1148 | Train loss: 0.2954 | Val loss: 0.3637\n   % Time:   609 | Iteration:  1100 | Batch: 1100/1148 | Train loss: 0.2801 | Val loss: 0.3642\n=&gt; EPOCH 2\n   % Time:   664 | Iteration:  1200 | Batch:   52/1148 | Train loss: 0.2815 | Val loss: 0.3511\n   % Time:   720 | Iteration:  1300 | Batch:  152/1148 | Train loss: 0.2525 | Val loss: 0.3467\n   % Time:   776 | Iteration:  1400 | Batch:  252/1148 | Train loss: 0.2519 | Val loss: 0.3396\n   % Time:   831 | Iteration:  1500 | Batch:  352/1148 | Train loss: 0.2548 | Val loss: 0.3322\n   % Time:   887 | Iteration:  1600 | Batch:  452/1148 | Train loss: 0.2522 | Val loss: 0.3294\n   % Time:   944 | Iteration:  1700 | Batch:  552/1148 | Train loss: 0.2457 | Val loss: 0.3278\n   % Time:  1000 | Iteration:  1800 | Batch:  652/1148 | Train loss: 0.2460 | Val loss: 0.3224\n   % Time:  1055 | Iteration:  1900 | Batch:  752/1148 | Train loss: 0.2465 | Val loss: 0.3175\n   % Time:  1112 | Iteration:  2000 | Batch:  852/1148 | Train loss: 0.2321 | Val loss: 0.3157\n   % Time:  1167 | Iteration:  2100 | Batch:  952/1148 | Train loss: 0.2480 | Val loss: 0.3141\n   % Time:  1222 | Iteration:  2200 | Batch: 1052/1148 | Train loss: 0.2309 | Val loss: 0.3120\n=&gt; EPOCH 3\n   % Time:  1278 | Iteration:  2300 | Batch:    4/1148 | Train loss: 0.2322 | Val loss: 0.3077\n   % Time:  1334 | Iteration:  2400 | Batch:  104/1148 | Train loss: 0.2225 | Val loss: 0.3069\n   % Time:  1390 | Iteration:  2500 | Batch:  204/1148 | Train loss: 0.2131 | Val loss: 0.3050\n   % Time:  1445 | Iteration:  2600 | Batch:  304/1148 | Train loss: 0.2201 | Val loss: 0.3006\n   % Time:  1502 | Iteration:  2700 | Batch:  404/1148 | Train loss: 0.2241 | Val loss: 0.3019\n   % Time:  1557 | Iteration:  2800 | Batch:  504/1148 | Train loss: 0.2138 | Val loss: 0.2980\n   % Time:  1613 | Iteration:  2900 | Batch:  604/1148 | Train loss: 0.2181 | Val loss: 0.2967\n   % Time:  1670 | Iteration:  3000 | Batch:  704/1148 | Train loss: 0.2160 | Val loss: 0.2951\n   % Time:  1726 | Iteration:  3100 | Batch:  804/1148 | Train loss: 0.2170 | Val loss: 0.2919\n   % Time:  1782 | Iteration:  3200 | Batch:  904/1148 | Train loss: 0.2156 | Val loss: 0.2915\n   % Time:  1837 | Iteration:  3300 | Batch: 1004/1148 | Train loss: 0.2158 | Val loss: 0.2899\n   % Time:  1893 | Iteration:  3400 | Batch: 1104/1148 | Train loss: 0.2117 | Val loss: 0.2880\n=&gt; EPOCH 4\n   % Time:  1948 | Iteration:  3500 | Batch:   56/1148 | Train loss: 0.2026 | Val loss: 0.2869\n   % Time:  2003 | Iteration:  3600 | Batch:  156/1148 | Train loss: 0.2011 | Val loss: 0.2839\n   % Time:  2060 | Iteration:  3700 | Batch:  256/1148 | Train loss: 0.1960 | Val loss: 0.2856\n   % Time:  2117 | Iteration:  3800 | Batch:  356/1148 | Train loss: 0.2036 | Val loss: 0.2848\n   % Time:  2173 | Iteration:  3900 | Batch:  456/1148 | Train loss: 0.1982 | Val loss: 0.2823\n   % Time:  2228 | Iteration:  4000 | Batch:  556/1148 | Train loss: 0.1970 | Val loss: 0.2820\n   % Time:  2283 | Iteration:  4100 | Batch:  656/1148 | Train loss: 0.2014 | Val loss: 0.2796\n   % Time:  2338 | Iteration:  4200 | Batch:  756/1148 | Train loss: 0.2015 | Val loss: 0.2801\n   % Time:  2393 | Iteration:  4300 | Batch:  856/1148 | Train loss: 0.1924 | Val loss: 0.2782\n   % Time:  2450 | Iteration:  4400 | Batch:  956/1148 | Train loss: 0.1991 | Val loss: 0.2777\n   % Time:  2506 | Iteration:  4500 | Batch: 1056/1148 | Train loss: 0.1971 | Val loss: 0.2774\n=&gt; EPOCH 5\n   % Time:  2563 | Iteration:  4600 | Batch:    8/1148 | Train loss: 0.1975 | Val loss: 0.2764\n   % Time:  2619 | Iteration:  4700 | Batch:  108/1148 | Train loss: 0.1846 | Val loss: 0.2752\n   % Time:  2675 | Iteration:  4800 | Batch:  208/1148 | Train loss: 0.1878 | Val loss: 0.2741\n   % Time:  2731 | Iteration:  4900 | Batch:  308/1148 | Train loss: 0.1843 | Val loss: 0.2742\n   % Time:  2788 | Iteration:  5000 | Batch:  408/1148 | Train loss: 0.1832 | Val loss: 0.2733\n   % Time:  2843 | Iteration:  5100 | Batch:  508/1148 | Train loss: 0.1898 | Val loss: 0.2740\n   % Time:  2898 | Iteration:  5200 | Batch:  608/1148 | Train loss: 0.1856 | Val loss: 0.2713\n   % Time:  2955 | Iteration:  5300 | Batch:  708/1148 | Train loss: 0.1872 | Val loss: 0.2704\n   % Time:  3011 | Iteration:  5400 | Batch:  808/1148 | Train loss: 0.1889 | Val loss: 0.2715\n   % Time:  3068 | Iteration:  5500 | Batch:  908/1148 | Train loss: 0.1861 | Val loss: 0.2703\n   % Time:  3123 | Iteration:  5600 | Batch: 1008/1148 | Train loss: 0.1837 | Val loss: 0.2700\n   % Time:  3178 | Iteration:  5700 | Batch: 1108/1148 | Train loss: 0.1873 | Val loss: 0.2696\n=&gt; EPOCH 6\n   % Time:  3233 | Iteration:  5800 | Batch:   60/1148 | Train loss: 0.1859 | Val loss: 0.2662\n   % Time:  3290 | Iteration:  5900 | Batch:  160/1148 | Train loss: 0.1745 | Val loss: 0.2677\n   % Time:  3346 | Iteration:  6000 | Batch:  260/1148 | Train loss: 0.1755 | Val loss: 0.2658\n   % Time:  3403 | Iteration:  6100 | Batch:  360/1148 | Train loss: 0.1725 | Val loss: 0.2678\n   % Time:  3458 | Iteration:  6200 | Batch:  460/1148 | Train loss: 0.1791 | Val loss: 0.2659\n   % Time:  3514 | Iteration:  6300 | Batch:  560/1148 | Train loss: 0.1762 | Val loss: 0.2655\n   % Time:  3570 | Iteration:  6400 | Batch:  660/1148 | Train loss: 0.1745 | Val loss: 0.2657\n   % Time:  3626 | Iteration:  6500 | Batch:  760/1148 | Train loss: 0.1739 | Val loss: 0.2637\n   % Time:  3682 | Iteration:  6600 | Batch:  860/1148 | Train loss: 0.1755 | Val loss: 0.2646\n   % Time:  3738 | Iteration:  6700 | Batch:  960/1148 | Train loss: 0.1766 | Val loss: 0.2641\n   % Time:  3794 | Iteration:  6800 | Batch: 1060/1148 | Train loss: 0.1730 | Val loss: 0.2637\n=&gt; EPOCH 7\n   % Time:  3851 | Iteration:  6900 | Batch:   12/1148 | Train loss: 0.1757 | Val loss: 0.2621\n   % Time:  3906 | Iteration:  7000 | Batch:  112/1148 | Train loss: 0.1631 | Val loss: 0.2614\n   % Time:  3961 | Iteration:  7100 | Batch:  212/1148 | Train loss: 0.1665 | Val loss: 0.2641\n   % Time:  4017 | Iteration:  7200 | Batch:  312/1148 | Train loss: 0.1683 | Val loss: 0.2616\n   % Time:  4073 | Iteration:  7300 | Batch:  412/1148 | Train loss: 0.1698 | Val loss: 0.2618\n   % Time:  4128 | Iteration:  7400 | Batch:  512/1148 | Train loss: 0.1679 | Val loss: 0.2605\n   % Time:  4185 | Iteration:  7500 | Batch:  612/1148 | Train loss: 0.1689 | Val loss: 0.2594\n   % Time:  4240 | Iteration:  7600 | Batch:  712/1148 | Train loss: 0.1673 | Val loss: 0.2597\n   % Time:  4296 | Iteration:  7700 | Batch:  812/1148 | Train loss: 0.1706 | Val loss: 0.2591\n   % Time:  4352 | Iteration:  7800 | Batch:  912/1148 | Train loss: 0.1658 | Val loss: 0.2585\n   % Time:  4409 | Iteration:  7900 | Batch: 1012/1148 | Train loss: 0.1705 | Val loss: 0.2577\n   % Time:  4465 | Iteration:  8000 | Batch: 1112/1148 | Train loss: 0.1669 | Val loss: 0.2585\n=&gt; EPOCH 8\n   % Time:  4521 | Iteration:  8100 | Batch:   64/1148 | Train loss: 0.1577 | Val loss: 0.2581\n   % Time:  4576 | Iteration:  8200 | Batch:  164/1148 | Train loss: 0.1636 | Val loss: 0.2555\n   % Time:  4633 | Iteration:  8300 | Batch:  264/1148 | Train loss: 0.1569 | Val loss: 0.2568\n   % Time:  4689 | Iteration:  8400 | Batch:  364/1148 | Train loss: 0.1599 | Val loss: 0.2560\n   % Time:  4745 | Iteration:  8500 | Batch:  464/1148 | Train loss: 0.1593 | Val loss: 0.2570\n   % Time:  4802 | Iteration:  8600 | Batch:  564/1148 | Train loss: 0.1607 | Val loss: 0.2555\n   % Time:  4858 | Iteration:  8700 | Batch:  664/1148 | Train loss: 0.1546 | Val loss: 0.2553\n   % Time:  4915 | Iteration:  8800 | Batch:  764/1148 | Train loss: 0.1636 | Val loss: 0.2565\n   % Time:  4971 | Iteration:  8900 | Batch:  864/1148 | Train loss: 0.1616 | Val loss: 0.2537\n   % Time:  5027 | Iteration:  9000 | Batch:  964/1148 | Train loss: 0.1614 | Val loss: 0.2550\n   % Time:  5083 | Iteration:  9100 | Batch: 1064/1148 | Train loss: 0.1591 | Val loss: 0.2559\n=&gt; EPOCH 9\n   % Time:  5140 | Iteration:  9200 | Batch:   16/1148 | Train loss: 0.1624 | Val loss: 0.2565\n   % Time:  5197 | Iteration:  9300 | Batch:  116/1148 | Train loss: 0.1513 | Val loss: 0.2552\n   % Time:  5253 | Iteration:  9400 | Batch:  216/1148 | Train loss: 0.1559 | Val loss: 0.2545\n=&gt; Adjust learning rate to: 0.0035\n   % Time:  5309 | Iteration:  9500 | Batch:  316/1148 | Train loss: 0.1471 | Val loss: 0.2519\n   % Time:  5366 | Iteration:  9600 | Batch:  416/1148 | Train loss: 0.1512 | Val loss: 0.2510\n   % Time:  5421 | Iteration:  9700 | Batch:  516/1148 | Train loss: 0.1508 | Val loss: 0.2504\n   % Time:  5477 | Iteration:  9800 | Batch:  616/1148 | Train loss: 0.1493 | Val loss: 0.2512\n   % Time:  5532 | Iteration:  9900 | Batch:  716/1148 | Train loss: 0.1542 | Val loss: 0.2500\n   % Time:  5588 | Iteration: 10000 | Batch:  816/1148 | Train loss: 0.1480 | Val loss: 0.2498\n   % Time:  5644 | Iteration: 10100 | Batch:  916/1148 | Train loss: 0.1494 | Val loss: 0.2494\n   % Time:  5700 | Iteration: 10200 | Batch: 1016/1148 | Train loss: 0.1483 | Val loss: 0.2490\n   % Time:  5755 | Iteration: 10300 | Batch: 1116/1148 | Train loss: 0.1499 | Val loss: 0.2484\n=&gt; EPOCH 10\n   % Time:  5811 | Iteration: 10400 | Batch:   68/1148 | Train loss: 0.1406 | Val loss: 0.2492\n   % Time:  5866 | Iteration: 10500 | Batch:  168/1148 | Train loss: 0.1467 | Val loss: 0.2494\n   % Time:  5922 | Iteration: 10600 | Batch:  268/1148 | Train loss: 0.1433 | Val loss: 0.2495\n   % Time:  5978 | Iteration: 10700 | Batch:  368/1148 | Train loss: 0.1454 | Val loss: 0.2490\n   % Time:  6033 | Iteration: 10800 | Batch:  468/1148 | Train loss: 0.1428 | Val loss: 0.2494\n=&gt; Adjust learning rate to: 0.00175\n   % Time:  6089 | Iteration: 10900 | Batch:  568/1148 | Train loss: 0.1447 | Val loss: 0.2482\n   % Time:  6144 | Iteration: 11000 | Batch:  668/1148 | Train loss: 0.1493 | Val loss: 0.2479\n   % Time:  6200 | Iteration: 11100 | Batch:  768/1148 | Train loss: 0.1445 | Val loss: 0.2479\n   % Time:  6257 | Iteration: 11200 | Batch:  868/1148 | Train loss: 0.1415 | Val loss: 0.2476\n   % Time:  6312 | Iteration: 11300 | Batch:  968/1148 | Train loss: 0.1436 | Val loss: 0.2469\n   % Time:  6368 | Iteration: 11400 | Batch: 1068/1148 | Train loss: 0.1423 | Val loss: 0.2473\n=&gt; EPOCH 11\n   % Time:  6423 | Iteration: 11500 | Batch:   20/1148 | Train loss: 0.1487 | Val loss: 0.2474\n   % Time:  6478 | Iteration: 11600 | Batch:  120/1148 | Train loss: 0.1435 | Val loss: 0.2478\n   % Time:  6535 | Iteration: 11700 | Batch:  220/1148 | Train loss: 0.1402 | Val loss: 0.2475\n   % Time:  6591 | Iteration: 11800 | Batch:  320/1148 | Train loss: 0.1378 | Val loss: 0.2476\n=&gt; Adjust learning rate to: 0.000875\n   % Time:  6647 | Iteration: 11900 | Batch:  420/1148 | Train loss: 0.1451 | Val loss: 0.2474\n   % Time:  6702 | Iteration: 12000 | Batch:  520/1148 | Train loss: 0.1400 | Val loss: 0.2475\n   % Time:  6759 | Iteration: 12100 | Batch:  620/1148 | Train loss: 0.1377 | Val loss: 0.2473\n   % Time:  6814 | Iteration: 12200 | Batch:  720/1148 | Train loss: 0.1383 | Val loss: 0.2474\n   % Time:  6871 | Iteration: 12300 | Batch:  820/1148 | Train loss: 0.1442 | Val loss: 0.2471\n   % Time:  6927 | Iteration: 12400 | Batch:  920/1148 | Train loss: 0.1383 | Val loss: 0.2471\n   % Time:  6983 | Iteration: 12500 | Batch: 1020/1148 | Train loss: 0.1407 | Val loss: 0.2472\n   % Time:  7040 | Iteration: 12600 | Batch: 1120/1148 | Train loss: 0.1398 | Val loss: 0.2469\n=&gt; EPOCH 12\n   % Time:  7095 | Iteration: 12700 | Batch:   72/1148 | Train loss: 0.1427 | Val loss: 0.2470\n   % Time:  7151 | Iteration: 12800 | Batch:  172/1148 | Train loss: 0.1362 | Val loss: 0.2473\n   % Time:  7208 | Iteration: 12900 | Batch:  272/1148 | Train loss: 0.1395 | Val loss: 0.2473\n   % Time:  7264 | Iteration: 13000 | Batch:  372/1148 | Train loss: 0.1396 | Val loss: 0.2474\n   % Time:  7320 | Iteration: 13100 | Batch:  472/1148 | Train loss: 0.1377 | Val loss: 0.2472\n=&gt; Adjust learning rate to: 0.0004375\n   % Time:  7376 | Iteration: 13200 | Batch:  572/1148 | Train loss: 0.1377 | Val loss: 0.2472\n   % Time:  7432 | Iteration: 13300 | Batch:  672/1148 | Train loss: 0.1415 | Val loss: 0.2470\n   % Time:  7488 | Iteration: 13400 | Batch:  772/1148 | Train loss: 0.1387 | Val loss: 0.2469\n   % Time:  7544 | Iteration: 13500 | Batch:  872/1148 | Train loss: 0.1402 | Val loss: 0.2470\n   % Time:  7600 | Iteration: 13600 | Batch:  972/1148 | Train loss: 0.1397 | Val loss: 0.2469\n   % Time:  7655 | Iteration: 13700 | Batch: 1072/1148 | Train loss: 0.1370 | Val loss: 0.2469\n=&gt; EPOCH 13\n   % Time:  7712 | Iteration: 13800 | Batch:   24/1148 | Train loss: 0.1405 | Val loss: 0.2469\n   % Time:  7769 | Iteration: 13900 | Batch:  124/1148 | Train loss: 0.1368 | Val loss: 0.2470\n   % Time:  7824 | Iteration: 14000 | Batch:  224/1148 | Train loss: 0.1343 | Val loss: 0.2470\n   % Time:  7879 | Iteration: 14100 | Batch:  324/1148 | Train loss: 0.1376 | Val loss: 0.2471\n   % Time:  7934 | Iteration: 14200 | Batch:  424/1148 | Train loss: 0.1400 | Val loss: 0.2472\n=&gt; Adjust learning rate to: 0.00021875\n   % Time:  7990 | Iteration: 14300 | Batch:  524/1148 | Train loss: 0.1347 | Val loss: 0.2471\n   % Time:  8047 | Iteration: 14400 | Batch:  624/1148 | Train loss: 0.1405 | Val loss: 0.2471\n   % Time:  8103 | Iteration: 14500 | Batch:  724/1148 | Train loss: 0.1392 | Val loss: 0.2471\n   % Time:  8159 | Iteration: 14600 | Batch:  824/1148 | Train loss: 0.1359 | Val loss: 0.2470\n   % Time:  8214 | Iteration: 14700 | Batch:  924/1148 | Train loss: 0.1396 | Val loss: 0.2470\n   % Time:  8270 | Iteration: 14800 | Batch: 1024/1148 | Train loss: 0.1365 | Val loss: 0.2471\n   % Time:  8327 | Iteration: 14900 | Batch: 1124/1148 | Train loss: 0.1355 | Val loss: 0.2470\n=&gt; EPOCH 14\n   % Time:  8383 | Iteration: 15000 | Batch:   76/1148 | Train loss: 0.1354 | Val loss: 0.2470\n   % Time:  8439 | Iteration: 15100 | Batch:  176/1148 | Train loss: 0.1397 | Val loss: 0.2470\n   % Time:  8495 | Iteration: 15200 | Batch:  276/1148 | Train loss: 0.1350 | Val loss: 0.2471\n   % Time:  8551 | Iteration: 15300 | Batch:  376/1148 | Train loss: 0.1393 | Val loss: 0.2471\n   % Time:  8607 | Iteration: 15400 | Batch:  476/1148 | Train loss: 0.1378 | Val loss: 0.2470\n=&gt; Adjust learning rate to: 0.000109375\n   % Time:  8663 | Iteration: 15500 | Batch:  576/1148 | Train loss: 0.1364 | Val loss: 0.2470\n   % Time:  8720 | Iteration: 15600 | Batch:  676/1148 | Train loss: 0.1372 | Val loss: 0.2470\n   % Time:  8776 | Iteration: 15700 | Batch:  776/1148 | Train loss: 0.1346 | Val loss: 0.2470\n   % Time:  8832 | Iteration: 15800 | Batch:  876/1148 | Train loss: 0.1371 | Val loss: 0.2470\n   % Time:  8888 | Iteration: 15900 | Batch:  976/1148 | Train loss: 0.1360 | Val loss: 0.2470\n   % Time:  8944 | Iteration: 16000 | Batch: 1076/1148 | Train loss: 0.1378 | Val loss: 0.2470\n=&gt; EPOCH 15\n   % Time:  9001 | Iteration: 16100 | Batch:   28/1148 | Train loss: 0.1364 | Val loss: 0.2470\n=&gt; Adjust learning rate to: 5.46875e-05\n   % Time:  9057 | Iteration: 16200 | Batch:  128/1148 | Train loss: 0.1377 | Val loss: 0.2471\n   % Time:  9113 | Iteration: 16300 | Batch:  228/1148 | Train loss: 0.1374 | Val loss: 0.2471\n   % Time:  9170 | Iteration: 16400 | Batch:  328/1148 | Train loss: 0.1355 | Val loss: 0.2471\n   % Time:  9225 | Iteration: 16500 | Batch:  428/1148 | Train loss: 0.1350 | Val loss: 0.2471\n   % Time:  9281 | Iteration: 16600 | Batch:  528/1148 | Train loss: 0.1362 | Val loss: 0.2471\n=&gt; Adjust learning rate to: 2.734375e-05\n   % Time:  9337 | Iteration: 16700 | Batch:  628/1148 | Train loss: 0.1357 | Val loss: 0.2471\n   % Time:  9394 | Iteration: 16800 | Batch:  728/1148 | Train loss: 0.1392 | Val loss: 0.2471\n   % Time:  9450 | Iteration: 16900 | Batch:  828/1148 | Train loss: 0.1398 | Val loss: 0.2470\n   % Time:  9505 | Iteration: 17000 | Batch:  928/1148 | Train loss: 0.1384 | Val loss: 0.2470\n   % Time:  9561 | Iteration: 17100 | Batch: 1028/1148 | Train loss: 0.1374 | Val loss: 0.2470\n   % Time:  9618 | Iteration: 17200 | Batch: 1128/1148 | Train loss: 0.1394 | Val loss: 0.2470\n=&gt; EPOCH 16\n   % Time:  9674 | Iteration: 17300 | Batch:   80/1148 | Train loss: 0.1362 | Val loss: 0.2470\n   % Time:  9730 | Iteration: 17400 | Batch:  180/1148 | Train loss: 0.1383 | Val loss: 0.2470\n   % Time:  9786 | Iteration: 17500 | Batch:  280/1148 | Train loss: 0.1372 | Val loss: 0.2470\n   % Time:  9841 | Iteration: 17600 | Batch:  380/1148 | Train loss: 0.1346 | Val loss: 0.2470\n=&gt; Adjust learning rate to: 1.3671875e-05\n   % Time:  9897 | Iteration: 17700 | Batch:  480/1148 | Train loss: 0.1363 | Val loss: 0.2470\n   % Time:  9952 | Iteration: 17800 | Batch:  580/1148 | Train loss: 0.1385 | Val loss: 0.2470\n   % Time: 10008 | Iteration: 17900 | Batch:  680/1148 | Train loss: 0.1374 | Val loss: 0.2470\n   % Time: 10063 | Iteration: 18000 | Batch:  780/1148 | Train loss: 0.1365 | Val loss: 0.2470\n   % Time: 10118 | Iteration: 18100 | Batch:  880/1148 | Train loss: 0.1374 | Val loss: 0.2470\n   % Time: 10173 | Iteration: 18200 | Batch:  980/1148 | Train loss: 0.1366 | Val loss: 0.2470\n   % Time: 10230 | Iteration: 18300 | Batch: 1080/1148 | Train loss: 0.1376 | Val loss: 0.2470\n=&gt; EPOCH 17\n   % Time: 10287 | Iteration: 18400 | Batch:   32/1148 | Train loss: 0.1369 | Val loss: 0.2470\n   % Time: 10343 | Iteration: 18500 | Batch:  132/1148 | Train loss: 0.1371 | Val loss: 0.2470\n   % Time: 10398 | Iteration: 18600 | Batch:  232/1148 | Train loss: 0.1358 | Val loss: 0.2470\n=&gt; Adjust learning rate to: 6.8359375e-06\n\n\n\n\ntest\nWe also want to report WER and PER. In this notebook, we use attention. Setting args.attention to False to disable it, which will improve the results.\n\nmodel.load_state_dict(torch.load(config.best_model))\ntest(test_iter, model, criterion)\n\nPhoneme error rate (PER): 9.81\nWord error rate (WER): 40.66\n\n\nNow we display 10 examples. The first line is the word, the second line is its ‘true’ phoneme, and the third line is our prediction.\n\ntest_iter.init_epoch()\nfor i, batch in enumerate(test_iter):\n    show(batch, model)\n    if i == 10:\n        break\n\n&gt; pacheco\n= P AH0 CH EH1 K OW0\n&lt; P AH0 CH EH1 K OW0\n\n&gt; affable\n= AE1 F AH0 B AH0 L\n&lt; AE1 F AH0 B AH0 L\n\n&gt; mauriello\n= M AO2 R IY0 EH1 L OW0\n&lt; M AO0 R IY0 EH1 L OW0\n\n&gt; schadler\n= SH EY1 D AH0 L ER0\n&lt; SH AE1 D L ER0\n\n&gt; chandon\n= CH AE1 N D IH0 N\n&lt; CH AE1 N D AH0 N\n\n&gt; sines\n= S AY1 N Z\n&lt; S AY1 N Z\n\n&gt; nostrums\n= N AA1 S T R AH0 M Z\n&lt; N AA1 S T R AH0 M Z\n\n&gt; guandong's\n= G W AA1 N D OW2 NG Z\n&lt; G W AA1 N D AO1 NG Z\n\n&gt; pry\n= P R AY1\n&lt; P R AY1\n\n&gt; biddie\n= B IH1 D IY0\n&lt; B IH1 D IY0\n\n&gt; manes\n= M EY1 N Z\n&lt; M EY1 N Z\n\n\n\nAs you can see, the result is quite good. Happy learning!\n\n\nacknowledgement\nThis tutorial is done under my study with Hoang Le and Hoang Nguyen. Thank you very much for your help!\n\n\n\n\n Back to top"
  },
  {
    "objectID": "blog/projecteuler-second50.html",
    "href": "blog/projecteuler-second50.html",
    "title": "Solutions to Project Euler’s second 50 problems",
    "section": "",
    "text": "These are codes to solve the second 50 problems in Project Euler. These python codes will give solutions in less than 1 second. This is achieved by using the excellent numba package.\n\nProblem 51\n\ndef is_prime(n):\n    if n &lt; 5:\n        return n == 2 or n == 3\n    elif n % 2 == 0 or n % 3 == 0:\n        return False\n    for i in range(5, int(n**0.5)+1, 6):\n        if n % i == 0 or n % (i+2) == 0:\n            return False\n    return True\n\n\n%%time\ndef problem51(max_digits=6):\n    position_list = [[i, j, k] for i in range(6) for j in range(i+1, 6)\n                     for k in range(j+1, 6)]\n    for origin in range(10**(max_digits-3)):\n        for position in position_list:\n            start_digit = 0 if position[0] == 0 else 1\n            fail_count = start_digit\n            answer = []\n            for digit in range(start_digit, 10):\n                origin_str = \"{:0{}d}\".format(origin, max_digits-3)\n                expanded_digits = [str(digit)] * 6\n                expanded_digits[position[0]] = origin_str[0]\n                expanded_digits[position[1]] = origin_str[1]\n                expanded_digits[position[2]] = origin_str[2]\n                generated_number = int(\"\".join(expanded_digits))\n                if is_prime(generated_number) == False:\n                    fail_count += 1\n                    if fail_count == 3:\n                        break\n                else:\n                    answer.append(generated_number)\n            if len(answer) == 8:\n                return answer[0]\n\nprint(problem51())\n\n121313\nCPU times: user 40 ms, sys: 0 ns, total: 40 ms\nWall time: 40.9 ms\n\n\n\n\nProblem 52\n\n%%time\ndef problem52(limit=10**7):\n    for i in range(1, limit):\n        sorted_i = sorted(str(i))\n        if sorted_i == sorted(str(2*i)):\n            if sorted_i == sorted(str(3*i)):\n                if sorted_i == sorted(str(4*i)):\n                    if sorted_i == sorted(str(5*i)):\n                        if sorted_i == sorted(str(6*i)):\n                            return i\n\nprint(problem52())\n\n142857\nCPU times: user 208 ms, sys: 0 ns, total: 208 ms\nWall time: 205 ms\n\n\n\n\nProblem 53\n\n\nfrom math import factorial\n\n\n%%time\ndef problem53():    \n    count = 0\n    for n in range(23, 101):\n        for r in range(1, n):\n            if factorial(n) // (factorial(r) * factorial(n-r)) &gt; 10**6:\n                count += 1\n    return count\n\nprint(problem53())\n\n4075\nCPU times: user 12 ms, sys: 0 ns, total: 12 ms\nWall time: 9.28 ms\n\n\n\n\nProblem 54\n\nfrom urllib.request import urlopen\n\nurl = 'https://projecteuler.net/project/resources/p054_poker.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\ndef get_rank_value(hand):\n    CARD_TO_VALUE = dict(zip('23456789TJQKA', range(13)))\n    RANK_LIST = ['Royal Flush', 'Straight Flush', 'Four of a Kind',\n                 'Full House', 'Flush', 'Straight', 'Three of a Kind',\n                 'Two Pairs', 'One Pair', 'High Card']\n    RANK = dict(zip(RANK_LIST, range(9,-1,-1)))\n    \n    cards = [CARD_TO_VALUE[card[0]] for card in hand]\n    is_flush = len(set([card[1] for card in hand])) == 1\n    card_freqs = sorted([(cards.count(card), card) for card in set(cards)])\n    n_cards = len(card_freqs)\n    value = sum([card_freqs[i][1]*10**(2*i) for i in range(n_cards)])\n    if card_freqs[-1][0] == 4:\n        return (RANK['Four of a Kind'], value)\n    elif card_freqs[-1][0] == 3:\n        if n_cards == 2:\n            return (RANK['Full House'], value)\n        else:\n            return (RANK['Three of a Kind'], value)\n    elif card_freqs[-1][0] == 2:\n        if n_cards == 3:\n            return (RANK['Two Pairs'], value)\n        else:\n            return (RANK['One Pair'], value)\n    elif (((card_freqs[4][1] - card_freqs[0][1]) == 4) or\n          ((card_freqs[4][1] == CARD_TO_VALUE['A'] and\n            card_freqs[0][1] == CARD_TO_VALUE['2'] and\n            card_freqs[3][1] == CARD_TO_VALUE['5']))):\n        if is_flush:\n            if card_freqs[0][1] == CARD_TO_VALUE['A']:\n                return (RANK['Royal Flush'], value)\n            else:\n                return (RANK['Straight Flush'], value)\n        else:\n            return (RANK['Straight'], value)\n    elif is_flush:\n        return (RANK['Flush'], value)\n    else:\n        return (RANK['High Card'], value)\n\n\n%%time\ndef problem54():\n    lines = DATA.strip().split('\\n')\n    count = 0\n    for line in lines:\n        cards = line.split()\n        player_1 = cards[:5]\n        player_2 = cards[5:]\n        if get_rank_value(player_1) &gt; get_rank_value(player_2):\n            count += 1\n    return count\n\nprint(problem54())\n\n376\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\nWall time: 17.8 ms\n\n\n\n\nProblem 55\n\n%%time\ndef problem55(limit=10000):\n    c = 0\n    for i in range(limit):\n        n = i\n        n_t = int(str(n)[::-1])\n        is_Lychrel = True\n        for j in range(49):\n            n = n + n_t\n            n_t = int(str(n)[::-1])\n            if n == n_t:\n                is_Lychrel = False\n                break\n        if is_Lychrel:\n            c += 1\n    return c\n    \nprint(problem55())\n\n249\nCPU times: user 40 ms, sys: 0 ns, total: 40 ms\nWall time: 40.7 ms\n\n\n\n\nProblem 56\n\n%%time\ndef problem56():\n    max_digital_sum = 0\n    for a in range(100):\n        for b in range(100):\n            d_sum = sum([int(x) for x in str(a**b)])\n            if d_sum &gt; max_digital_sum:\n                max_digital_sum = d_sum\n    return max_digital_sum\n\nprint(problem56())\n\n972\nCPU times: user 128 ms, sys: 0 ns, total: 128 ms\nWall time: 130 ms\n\n\n\n\nProblem 57\n\nfrom fractions import Fraction\n\n\n%%time\ndef problem57():    \n    x = 2 + Fraction(1, 2)\n    count = 0\n    for i in range(1000):\n        y = x - 1\n        if len(str(y.numerator)) &gt; len(str(y.denominator)):\n            count += 1\n        x = 2 + Fraction(1, x)\n    return count\n\nprint(problem57())\n\n153\nCPU times: user 40 ms, sys: 0 ns, total: 40 ms\nWall time: 40.9 ms\n\n\n\n\nProblem 58\n\nfrom numba import jit\n\n@jit\ndef is_prime(n):\n    if n &lt; 5:\n        return n == 2 or n == 3\n    elif n % 2 == 0 or n % 3 == 0:\n        return False\n    for i in range(5, int(n**0.5)+1, 6):\n        if n % i == 0 or n % (i+2) == 0:\n            return False\n    return True\n\n\n%%time\n@jit\ndef problem58():\n    x = 1\n    gap = 0\n    n_prime_x10 = 0\n    n_x = 1\n    while True:\n        gap = gap + 2\n        for i in range(3):\n            x = x + gap\n            if is_prime(x):\n                n_prime_x10 += 10\n        x = x + gap\n        n_x += 4\n        if n_prime_x10 &lt; n_x:\n            return int(x**0.5)\n\nprint(problem58())\n\n26241\nCPU times: user 216 ms, sys: 4 ms, total: 220 ms\nWall time: 220 ms\n\n\n\n\nProblem 59\n\nfrom urllib.request import urlopen\n    \nurl = 'https://projecteuler.net/project/resources/p059_cipher.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem59():\n    cipher = list(map(lambda x: int(x), DATA.rstrip().split(',')))\n    length = len(cipher)\n    ord_a = ord('a')\n    for i in range(ord_a, ord_a+26):\n        for j in range(ord_a, ord_a+26):\n            for k in range(ord_a, ord_a+26):\n                key = ([i, j, k] * (length//3+1))[:length]\n                message = bytes([x^y for x, y in zip(cipher, key)])\n                if b'that' in message and b'have' in message:\n                    return sum([ord(x) for x in message.decode()])\n\nprint(problem59())\n\n107359\nCPU times: user 332 ms, sys: 0 ns, total: 332 ms\nWall time: 333 ms\n\n\n\n\nProblem 60\n\nimport numpy as np\nfrom numba import jit, int8, int64\n\n@jit(int8[:](int64))\ndef create_sieve_of_halfprimes(n):\n    sieve = np.ones(n//2, dtype=np.int8)\n    for i in range(3, int(n**0.5)+1, 2):\n        if sieve[i//2]:\n            sieve[i*i//2::i] = 0\n    return sieve\n\n@jit(int8[:,:](int8[:], int64[:]))\ndef create_check_matrix(sieve, primes):\n    check_matrix = np.zeros((len(primes), len(primes)), dtype=np.int8)\n    for i1 in range(len(primes)):\n        for i2 in range(i1+1, len(primes)):\n            half_prime12 = (primes[i1]\n                            * 10**(int(np.log10(primes[i2]))+1)\n                            + primes[i2]) // 2\n            if sieve[half_prime12]:\n                half_prime21 = (primes[i2]\n                                * 10**(int(np.log10(primes[i1]))+1)\n                                + primes[i1]) // 2\n                if sieve[half_prime21]:\n                    check_matrix[i1, i2] = 1\n    return check_matrix\n\n@jit(int64(int64[:], int8[:,:]))\ndef find_answer(primes, check_matrix):\n    for a1 in range(len(primes)):\n        for a2 in range(a1+1, len(primes)):\n            if check_matrix[a1, a2]:\n                for a3 in range(a2+1, len(primes)):\n                    if check_matrix[a2, a3] and check_matrix[a1, a3]:\n                        for a4 in range(a3+1, len(primes)):\n                            if (check_matrix[a3, a4] and check_matrix[a2, a4]\n                                    and check_matrix[a1, a4]):\n                                for a5 in range(a4+1, len(primes)):\n                                    if (check_matrix[a4, a5]\n                                            and check_matrix[a3, a5]\n                                            and check_matrix[a2, a5]\n                                            and check_matrix[a1, a5]):\n                                        return (primes[a1] + primes[a2]\n                                                + primes[a3] + primes[a4]\n                                                + primes[a5])\n    return 0\n\n\n%%time\ndef problem60(limit=10**4):\n    sieve = create_sieve_of_halfprimes(limit**2)\n    primes = 2*np.nonzero(sieve[:limit//2])[0][1:] + 1\n    check_matrix = create_check_matrix(sieve, primes)\n    return find_answer(primes, check_matrix)\n\nprint(problem60())\n\n26033\nCPU times: user 296 ms, sys: 4 ms, total: 300 ms\nWall time: 298 ms\n\n\n\n\nProblem 61\n\ndef create_polygonal_numbers(k):\n    l = []\n    p = n = 1\n    while p &lt; 10000:\n        if p &gt;= 1000:\n            l.append(p)\n        n = n + 1\n        p = n * ((k-2)*n - k + 4) // 2\n    return l\n\ndef check(a, b, c, d, e, f, polygonal_dict):\n    ab = a*100 + b\n    bc = b*100 + c\n    cd = c*100 + d\n    de = d*100 + e\n    ef = e*100 + f\n    fa = f*100 + a\n    for i1 in polygonal_dict[ab]:\n        for i2 in polygonal_dict[bc]:\n            for i3 in polygonal_dict[cd]:\n                for i4 in polygonal_dict[de]:\n                    for i5 in polygonal_dict[ef]:\n                        for i6 in polygonal_dict[fa]:\n                            if len(set([i1,i2,i3,i4,i5,i6])) == 6:\n                                return True\n    return False\n\n\n%%time\ndef problem61():\n    polygonal_dict = dict()\n    for k in range(3, 9):\n        for n in create_polygonal_numbers(k):\n            if n not in polygonal_dict:\n                polygonal_dict[n] = [k]\n            else:\n                polygonal_dict[n].append(k)\n                \n    cyclic = dict()\n    for n in polygonal_dict:\n        a, b = (n//100, n%100)\n        if b &lt; 10:\n            continue\n        if a not in cyclic:\n            cyclic[a] = [b]\n        else:\n            cyclic[a].append(b)\n\n    for a in cyclic:\n        for b in cyclic[a]:\n            if b not in cyclic:\n                continue\n            for c in cyclic[b]:\n                if c not in cyclic:\n                    continue\n                for d in cyclic[c]:\n                    if d not in cyclic:\n                        continue\n                    for e in cyclic[d]:\n                        if e not in cyclic:\n                            continue\n                        for f in cyclic[e]:\n                            if f not in cyclic:\n                                continue\n                            if a not in cyclic[f]:\n                                continue\n                            if check(a, b, c, d, e, f, polygonal_dict):\n                                return sum([a,b,c,d,e,f]) * 101\n\nprint(problem61())\n\n28684\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 741 µs\n\n\n\n\nProblem 62\n\nfrom collections import Counter\n\n\n%%time\ndef problem62():    \n    k = 1\n    while True:\n        cubes = []\n        for n in range(int(int('1'*k)**(1/3))+1, int(int('9'*k)**(1/3))+1):\n            cubes.append(n**3)\n        arranged_cubes = [''.join(sorted(str(x))) for x in cubes]    \n        counter = Counter(arranged_cubes)\n        for c in list(counter):\n            if counter[c] == 5:\n                return cubes[arranged_cubes.index(c)]\n        k += 1\n\nprint(problem62())\n\n127035954683\nCPU times: user 28 ms, sys: 0 ns, total: 28 ms\nWall time: 28.3 ms\n\n\n\n\nProblem 63\n\n%%time\ndef problem63():\n    n = 1\n    c = 1\n    while len(str(9**n)) &gt;= n:\n        for i in range(2, 10):\n            if i**n &gt;= 10**(n-1):\n                c += 1\n        n += 1\n    return c\n\nprint(problem63())\n\n49\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 141 µs\n\n\n\n\nProblem 64\n\ndef count_period(n):\n    if n == int(n**0.5)**2:\n        return 0\n    b = 0\n    c = 1\n    l = []\n    while True:\n        if (b,c) in l:\n            return len(l) - l.index((b,c))\n        l.append((b,c))\n        b = int((n**0.5 + b) / c) * c - b\n        c = (n - b**2) // c\n\n\n%%time\ndef problem64(limit=10**4):\n    c = 0\n    for i in range(2, limit+1):\n        if count_period(i) % 2 == 1:\n            c += 1\n    return c\n\nprint(problem64())\n\n1322\nCPU times: user 420 ms, sys: 0 ns, total: 420 ms\nWall time: 419 ms\n\n\n\n\nProblem 65\n\nfrom fractions import Fraction\n\n\n%%time\ndef problem65():\n    fraction_list = [2]\n    for i in range(1, 34):\n        fraction_list += [1, 2*i, 1]\n    rational_approx = Fraction(fraction_list[-1])\n    for a in fraction_list[-2::-1]:\n        rational_approx = a + 1 / rational_approx\n    return sum([int(x) for x in str(rational_approx.numerator)])\n\nprint(problem65())\n\n272\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 1.02 ms\n\n\n\n\nProblem 66\n\nfrom fractions import Fraction\n\ndef diophantine_solution(D):\n    if D == int(D**0.5)**2:\n        return 1\n    b = 0\n    c = 1\n    a = a0 = int(D**0.5)\n    fraction_list = []\n    while True:\n        a = int((D**0.5 + b) / c)\n        if a == 2*a0:\n            break\n        fraction_list.append(a)\n        b = a*c - b\n        c = (D - b**2) // c\n    rational_approx = Fraction(fraction_list[-1])\n    for a in fraction_list[-2::-1]:\n        rational_approx = a + 1 / rational_approx\n    if rational_approx.numerator**2 - D*rational_approx.denominator**2 == 1:\n        return rational_approx.numerator\n    rational_approx = rational_approx + a0\n    for a in fraction_list[::-1]:\n        rational_approx = a + 1 / rational_approx\n    return rational_approx.numerator\n\n\n%%time\ndef problem66(limit=1000):\n    x_max = 3\n    d_max = 2\n    for i in range(3, limit+1):\n        x = diophantine_solution(i)\n        if x &gt; x_max:\n            x_max = x\n            d_max = i\n    return d_max\n\nprint(problem66())\n\n661\nCPU times: user 96 ms, sys: 0 ns, total: 96 ms\nWall time: 93.3 ms\n\n\n\n\nProblem 67\n\nfrom urllib.request import urlopen\n    \nurl = 'https://projecteuler.net/project/resources/p067_triangle.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem67():\n    triangle = [[int(y) for y in x.split()] for x in DATA.rstrip().split('\\n')]\n    route = [59]\n    for i, row in enumerate(triangle[1:]):\n        current = [row[0] + route[0]] \\\n            + [x + max(route[j], route[j+1]) for j, x in enumerate(row[1:-1])] \\\n            + [row[-1] + route[-1]]\n        route = current\n    return max(route)\n\nprint(problem67())\n\n7273\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 2.3 ms\n\n\n\n\nProblem 68\n\n%%time\ndef problem68():\n    solutions = []\n    a = 10\n    for b in range(1, 10):\n        for c in range(1, 10):\n            if b == c:\n                continue\n            for d in range(1, 10):\n                if d in [b, c]:\n                    continue\n                e = a + b - d\n                if e &lt; 1 or e &gt; 9 or e in [b, c, d]:\n                    continue\n                for f in range(1, 10):\n                    if f in [b, c, d, e]:\n                        continue\n                    g = c + d - f\n                    if g &lt; 1 or g &gt; 9 or g in [b, c, d, e, f]:\n                        continue\n                    for h in range(1, 10):\n                        if h in [b, c, d, e, f, g]:\n                            continue\n                        i = e + f - h\n                        j = g + h - b\n                        if i &lt; 1 or j &lt; 1 or i &gt; 9 or j &gt; 9:\n                            continue\n                        if len(set([a,b,c,d,e,f,g,h,i,j])) == 10:\n                            solutions.append([a,b,c,d,c,e,f,e,g,h,g,i,j,i,b])\n    ordered_solutions = []\n    for solution in solutions:\n        i_min = solution.index(min(solution[0], solution[3],\n                                   solution[6], solution[9], solution[12]))\n        ordered_solution = solution[i_min:] + solution[:i_min]\n        ordered_solution_to_int = int(''.join(\n            [str(x) for x in ordered_solution]))\n        ordered_solutions.append(ordered_solution_to_int)\n    return max(ordered_solutions)\n\nprint(problem68())\n\n6531031914842725\nCPU times: user 0 ns, sys: 4 ms, total: 4 ms\nWall time: 1.34 ms\n\n\n\n\nProblem 69\n\nimport numpy as np\n\n\n%%time\ndef problem69(limit=10**6):    \n    sieve = np.ones(limit+1)\n    n_per_phi_n_list = np.ones(limit+1)\n    for i in range(2, int(limit**0.5)+1):\n        if sieve[i] == 1:\n            sieve[i::i] = 0\n            n_per_phi_n_list[i::i] *= i / (i-1)\n    return np.argmax(n_per_phi_n_list)\n\nprint(problem69())\n\n510510\nCPU times: user 28 ms, sys: 0 ns, total: 28 ms\nWall time: 27.5 ms\n\n\n\n\nProblem 70\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef create_sieve(n):\n    sieve = np.ones(n, dtype=np.int8)\n    sieve[0] = sieve[1] = 0\n    sieve[4::2] = 0\n    for i in range(3, int(n**0.5)+1, 2):\n        if sieve[i]:\n            sieve[i*i::i] = 0\n    return sieve\n\n\n%%time\ndef problem70(limit=10**7):\n    sieve = create_sieve(limit)\n    prime_list = np.nonzero(sieve)[0]\n    nb_primes = len(prime_list)\n    n_min = 87109\n    ratio_min = 87109 / 79180\n    i_max = np.nonzero((prime_list ** 2) &gt;= 10**7)[0][0]-1\n    for i in range(i_max, -1, -1):\n        if prime_list[i] / (prime_list[i]-1) &gt;= ratio_min:\n            break\n        for j in range(i+1, nb_primes):\n            i_times_j = prime_list[i] * prime_list[j]\n            if i_times_j &gt;= 10**7:\n                break\n            phi_i_times_j = (prime_list[i]-1) * (prime_list[j]-1)\n            ratio = i_times_j / phi_i_times_j\n            if ratio &lt; ratio_min:\n                if sorted(str(i_times_j)) == sorted(str(phi_i_times_j)):\n                    n_min = i_times_j\n                    ratio_min = ratio\n    return n_min\n\nprint(problem70())\n\n8319823\nCPU times: user 280 ms, sys: 0 ns, total: 280 ms\nWall time: 279 ms\n\n\n\n\nProblem 71\n\nfrom fractions import Fraction\n\n\n%%time\ndef problem71(limit=10**6):\n    d_min = 5\n    n_min = 2\n    for d in range(2, limit+1):\n        n = 3 * d // 7\n        if d % 7 == 0:\n            n = n-1\n        if n * d_min &gt; d * n_min:\n            n_min = n\n            d_min = d\n    return Fraction(n_min, d_min).numerator\n\nprint(problem71())\n\n428570\nCPU times: user 220 ms, sys: 0 ns, total: 220 ms\nWall time: 219 ms\n\n\n\n\nProblem 72\n\nimport numpy as np\nfrom numba import jit\n\n\n%%time\n@jit\ndef problem72(limit=10**6):\n    sieve = np.ones(limit+1)\n    phi_n_list = np.arange(limit+1)\n    phi_n_list[1] = 0\n    for i in range(2, limit+1):\n        if sieve[i] == 1:\n            sieve[i*i::i] = 0\n            phi_n_list[i::i] *= (i-1)\n            phi_n_list[i::i] //= i\n    return sum(phi_n_list)\n\nprint(problem72())\n\n303963552391\nCPU times: user 360 ms, sys: 0 ns, total: 360 ms\nWall time: 359 ms\n\n\n\n\nProblem 73\n\nfrom numba import jit\n\n@jit\ndef count_Stern_Brocot(limit, left_n, left_d, right_n, right_d):\n    mid_n = left_n + right_n\n    mid_d = left_d + right_d\n    if mid_d &gt; limit:\n        return 0\n    else:\n        count = 1\n        count += count_Stern_Brocot(limit, left_n, left_d, mid_n, mid_d)\n        count += count_Stern_Brocot(limit, mid_n, mid_d, right_n, right_d)\n        return count\n\n\n%%time\ndef problem73(limit=12000):\n    return count_Stern_Brocot(12000, 1, 3, 1, 2)\n\nprint(problem73())\n\n7295372\nCPU times: user 124 ms, sys: 0 ns, total: 124 ms\nWall time: 124 ms\n\n\n\n\nProblem 74\n\nfrom math import factorial\nfrom itertools import permutations\n\ndef count_permuation(n):\n    p = set(permutations(str(n)))\n    if str(n)[0] == '1':\n        new_str = '0' + str(n)[1:]\n        p = p.union(set(permutations(new_str)))\n    if str(n)[:2] == '11':\n        new_str = '00' + str(n)[2:]\n        p = p.union(set(permutations(new_str)))\n    if str(n)[:3] == '111':\n        new_str = '000' + str(n)[3:]\n        p = p.union(set(permutations(new_str)))\n    if str(n)[:4] == '1111':\n        new_str = '0000' + str(n)[4:]\n        p = p.union(set(permutations(new_str)))\n    if str(n)[:5] == '11111':\n        p = p.add(str(n)[5:]+'00000')\n    c = 0\n    for e in p:\n        if e[0] != '0':\n            c += 1\n    return c\n\n\n%%time\ndef problem74():\n    fac = [factorial(x) for x in range(10)]\n    a = [0] * 2200000\n    answer_list = []\n    for i1 in range(10):\n        for i2 in range(i1, 10):\n            for i3 in range(i2, 10):\n                for i4 in range(i3, 10):\n                    for i5 in range(i4, 10):\n                        for i6 in range(i5, 10):\n                            i = i1*10**5+i2*10**4+i3*1000+i4*100+i5*10+i6\n                            chain = []\n                            j = i\n                            while (a[j] == 0) and (j not in chain):\n                                chain.append(j)\n                                s = 0\n                                jr = j\n                                while jr != 0:\n                                    s += fac[jr % 10]\n                                    jr //= 10\n                                j = s\n                            if j in chain:\n                                for t in range(len(chain)):\n                                    a[chain[t]] = len(chain) - t\n                                    if chain[t] == j:\n                                        break\n                                for t1 in range(t+1, len(chain)):\n                                    a[chain[t1]] = len(chain) - t\n                            else:\n                                for t, k in enumerate(chain):\n                                    a[k] = len(chain) - t + a[j]\n                            if a[i] == 60:\n                                answer_list.append(i)\n    return sum([count_permuation(answer) for answer in answer_list])\n\nprint(problem74())\n\n402\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\nWall time: 18.7 ms\n\n\n\n\nProblem 75\n\nfrom numba import jit\n\n@jit\ndef gcd(m, n):\n    if m == 0:\n        return n\n    return gcd(n % m, m)\n\n\n%timeit\ndef problem75(limit=1500000):\n    d = [0] * (limit + 1)\n    for n in range(1, 1000):\n        for m in range(n+1, 1000, 2):\n            if gcd(m, n) != 1:\n                continue\n            L = 2 * m * (m + n)\n            k_max = limit // L\n            for k in range(1, k_max+1):\n                d[L*k] += 1\n    c = 0\n    for i in range(limit+1):\n        if d[i] == 1:\n            c += 1\n    return c\n\nprint(problem75())\n\n161667\n\n\n\n\nProblem 76\n\nimport numpy as np\n\n\n%%time\ndef problem76():\n    A = np.zeros((101, 101), dtype=np.int)\n    for i in range(1, 101):\n        for j in range(1, i+1):\n            A[i, j] = 1\n            for k in range(j, i//2+1):\n                A[i, j] += A[i-k, k]\n    return A[100, 1] - 1\n\nprint(problem76())\n\n190569291\nCPU times: user 24 ms, sys: 0 ns, total: 24 ms\nWall time: 21.3 ms\n\n\n\n\nProblem 77\n\nimport numpy as np\n\ndef is_prime(n):\n    if n &lt; 5:\n        return n == 2 or n == 3\n    elif n % 2 == 0 or n % 3 == 0:\n        return False\n    for i in range(5, int(n**0.5)+1, 6):\n        if n % i == 0 or n % (i+2) == 0:\n            return False\n    return True\n\n\n%%time\ndef problem77():\n    A = np.zeros((101, 101), dtype=np.int)\n    for i in range(1, 101):\n        for j in range(1, i+1):\n            if is_prime(i):\n                A[i, j] = 1\n            for k in range(j, i//2+1):\n                if is_prime(k):\n                    A[i, j] += A[i-k, k]\n            if A[i, j] &gt; 5000 + is_prime(i):\n                return i\n\nprint(problem77())\n\n71\nCPU times: user 20 ms, sys: 0 ns, total: 20 ms\nWall time: 20.5 ms\n\n\n\n\nProblem 78\n\nfrom numba import jit\n\n\n%%time\n@jit\ndef problem78(limit=60000):\n    p = [0] * limit\n    p[0] = 1\n    p[1] = 1\n    for i in range(2, limit):\n        k = 1\n        s = 1\n        g_k = 0\n        while True:\n            g_k += 2*k - 1\n            if g_k &gt; i:\n                break\n            p[i] = p[i] + s*p[i-g_k]\n            g_k += k\n            if g_k &gt; i:\n                break\n            p[i] = p[i] + s*p[i-g_k]\n            k += 1\n            s = -s\n        p[i] = p[i] % 10**6\n        if p[i] == 0:\n            return i\n        \nprint(problem78())\n\n55374\nCPU times: user 100 ms, sys: 0 ns, total: 100 ms\nWall time: 102 ms\n\n\n\n\nProblem 79\n\nfrom urllib.request import urlopen\nfrom itertools import permutations, combinations\n    \nurl = 'https://projecteuler.net/project/resources/p079_keylog.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem79():\n    for p in permutations(''.join(sorted(list(set(DATA) - {'\\n'})))):\n        passcode = ''.join(p)\n        passcode_keys = [''.join(x) for x in combinations(passcode, 3)]\n        flag = True\n        for key in DATA.split():\n            if key not in passcode_keys:\n                flag = False\n                break\n        if flag == True:\n            return passcode\n\nprint(problem79())\n\n73162890\nCPU times: user 300 ms, sys: 0 ns, total: 300 ms\nWall time: 300 ms\n\n\n\n\nProblem 80\n\nfrom decimal import getcontext, Decimal\n\n\n%%time\ndef problem80():\n    getcontext().prec = 102\n    s = 0\n    for i in range(1, 101):\n        if i not in [y**2 for y in range(1, 11)]:\n            s += sum([int(x) for x in str(Decimal(i).sqrt())[:101]\n                      if x != '.'])\n    return s\n\nprint(problem80())\n\n40886\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 4.47 ms\n\n\n\n\nProblem 81\n\nfrom urllib.request import urlopen\n    \nurl = 'https://projecteuler.net/project/resources/p081_matrix.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem81():\n    matrix = [[int(y) for y in x.split(',')] for x in DATA.split()]\n    path_sum = [[0]*80]*80\n    for i in range(80):\n        for j in range(80):\n            if i == 0 and j == 0:\n                min_path = 0\n            elif i == 0:\n                min_path = path_sum[i][j-1]\n            elif j == 0:\n                min_path = path_sum[i-1][j]\n            else:\n                min_path = min(path_sum[i][j-1], path_sum[i-1][j])\n            path_sum[i][j] = matrix[i][j] + min_path\n    return path_sum[79][79]\n\nprint(problem81())\n\n427337\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 3.94 ms\n\n\n\n\nProblem 82\n\nfrom urllib.request import urlopen\nimport numpy as np\n    \nurl = 'https://projecteuler.net/project/resources/p082_matrix.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem82():\n    matrix = np.array([[int(y) for y in x.split(',')] for x in DATA.split()], \n                      dtype='uint16')\n    path_sum = np.zeros((80, 80), dtype='uint32')\n    min_path = np.zeros((80, 80), dtype='uint32')\n\n    for j in range(80):\n        for i in range(80):\n            if j == 0:\n                min_path = [0]\n            else:\n                min_path = [path_sum[i, j-1]]\n                for k in range(80):\n                    if k &lt; i:\n                        min_path.append(matrix[k:i, j].sum()\n                                        + path_sum[k, j-1])\n                    if k &gt; i:\n                        min_path.append(matrix[i+1:k+1, j].sum()\n                                        + path_sum[k, j-1])\n            path_sum[i, j] = matrix[i, j] + min(min_path)\n    return path_sum[:,79].min()\n\nprint(problem82())\n\n260324\nCPU times: user 956 ms, sys: 0 ns, total: 956 ms\nWall time: 958 ms\n\n\n\n\nProblem 83\n\nfrom urllib.request import urlopen\nimport numpy as np\n    \nurl = 'https://projecteuler.net/project/resources/p083_matrix.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n    \ndef update(i, j, path_sum, matrix, inf):\n    min_around = [inf]\n    if i &gt; 0:\n        min_around.append(path_sum[i-1, j])\n    if i &lt; 79:\n        min_around.append(path_sum[i+1, j])\n    if j &gt; 0:\n        min_around.append(path_sum[i, j-1])\n    if j &lt; 79:\n        min_around.append(path_sum[i, j+1])\n    path_sum[i, j] = min(min_around) + matrix[i, j]\n\n\n%%time\ndef problem83():\n    matrix = np.array([[int(y) for y in x.split(',')] for x in DATA.split()], \n                      dtype='uint16')\n\n    inf = np.iinfo(np.uint32).max // 2\n    path_sum = np.zeros((80, 80), dtype='uint32')\n    path_sum[:] = inf\n    path_sum[0, 0] = matrix[0, 0]\n\n    visited = np.zeros((80, 80), dtype='bool')\n    visited[0, 0] = True\n    update(0, 1, path_sum, matrix, inf)\n    update(1, 0, path_sum, matrix, inf)\n    \n    while not visited[79, 79]:\n        i, j = np.unravel_index(np.ma.argmin(\n            np.ma.MaskedArray(path_sum, visited)), (80, 80))\n        visited[i, j] = True\n        if i &gt; 0 and not visited[i-1, j]:\n            update(i-1, j, path_sum, matrix, inf)\n        if i &lt; 79 and not visited[i+1, j]:\n            update(i+1, j, path_sum, matrix, inf)\n        if j &gt; 0 and not visited[i, j-1]:\n            update(i, j-1, path_sum, matrix, inf)\n        if j &lt; 79 and not visited[i, j+1]:\n            update(i, j+1, path_sum, matrix, inf)\n    return path_sum[79, 79]\n\nprint(problem83())\n\n425185\nCPU times: user 360 ms, sys: 0 ns, total: 360 ms\nWall time: 359 ms\n\n\n\n\nProblem 84\n\nfrom random import randint\n\n\n%%time\ndef problem84():\n    jail = 10\n    g2j = 30\n    cc = [2, 17, 33]\n    ch = [7, 22, 36]\n    double = 0\n    table = [0] * 40\n    i = 0\n    for j in range(100000):\n        dice1 = randint(1, 4)\n        dice2 = randint(1, 4)\n        if dice1 == dice2:\n            double += 1\n        else:\n            double = 0\n        if double == 3:\n            i = jail\n        else:\n            i = (i + dice1 + dice2) % 40\n        if i in cc:\n            cc_open = randint(1, 16)\n            if cc_open == 1:\n                i = 0\n            elif cc_open == 2:\n                i = jail\n        if i in ch:\n            ch_open = randint(1, 16)\n            if ch_open == 1:\n                i = 0\n            elif ch_open == 2:\n                i = jail\n            elif ch_open == 3:\n                i = 11\n            elif ch_open == 4:\n                i = 24\n            elif ch_open == 5:\n                i = 39\n            elif ch_open == 6:\n                i = 5\n            elif ch_open in [7, 8]:\n                if i == ch[0]:\n                    i = 15\n                elif i == ch[1]:\n                    i = 25\n                else:\n                    i = 5\n            elif ch_open == 9:\n                if i == ch[1]:\n                    i = 28\n                else:\n                    i = 12\n            elif ch_open == 10:\n                i = (i-3)%40\n        if i == g2j:\n            i = jail\n        table[i] += 1\n    a, b, c = sorted(table, reverse=True)[:3]\n    return '{:02d}{:02d}{:02d}'.format(table.index(a),\n                                       table.index(b), table.index(c))\n\nprint(problem84())\n\n101516\nCPU times: user 232 ms, sys: 0 ns, total: 232 ms\nWall time: 234 ms\n\n\n\n\nProblem 85\n\n%%time\ndef problem85():\n    areas = []\n    rectangles = []\n\n    for n in range(1, 3000):\n        for m in range(n, 3000//n):\n            areas.append(n * m)\n            rectangles.append(n * m * (n+1) * (m+1) // 4)\n\n    ds = [int(abs(r - 2e6)) for r in rectangles]\n    return areas[ds.index(min(ds))]\n\nprint(problem85())\n\n2772\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 6.8 ms\n\n\n\n\nProblem 86\n\nfrom numba import jit\n\n\n%%time\n@jit\ndef problem86():\n    n_sol = 0\n    M = 0\n    while n_sol &lt;= 10**6:\n        M += 1\n        for a_plus_b in range(2, 2*M+1):\n            min_square = M**2 + a_plus_b**2\n            if min_square == (int(min_square**0.5)**2):\n                b_min = max(1, a_plus_b-M)\n                b_max = a_plus_b//2\n                n_sol += (b_max - b_min + 1)\n    return M\n\nprint(problem86())\n\n1818\nCPU times: user 84 ms, sys: 0 ns, total: 84 ms\nWall time: 83.7 ms\n\n\n\n\nProblem 87\n\nimport numpy as np\nfrom numba import jit\n\n\n%%time\n@jit\ndef problem87():\n    limit = 5*10**7-1\n    x_max = int(limit**(1/2))\n    y_max = int(limit**(1/3))\n    z_max = int(limit**(1/4))\n    sieve = np.ones(x_max+1, dtype=np.bool)\n    sieve[0] = sieve[1] = 0\n    for i in range(2, int(limit**0.5)+1):\n        if sieve[i] == 1:\n            sieve[2*i::i] = 0\n    x_list = np.nonzero(sieve[:x_max+1])[0]\n    y_list = np.nonzero(sieve[:y_max+1])[0]\n    z_list = np.nonzero(sieve[:z_max+1])[0]\n    A = np.zeros(limit+1, dtype=np.bool)\n    for x in x_list:\n        for y in y_list:\n            for z in z_list:\n                n = x**2 + y**3 + z**4\n                if n &lt;= limit:\n                    A[n] = 1\n    return A.sum()\n\nprint(problem87())\n\n1097343\nCPU times: user 512 ms, sys: 4 ms, total: 516 ms\nWall time: 514 ms\n\n\n\n\nProblem 88\n\n%%time\ndef problem88():\n    k_max = 12000\n    product_max = 2 * k_max\n    a_max = k_max // 4\n    m_max = k_max\n    prod = 1\n    mps = [2*k_max] * (k_max+1)\n    mps[0] = mps[1] = 0\n    for a in range(1, a_max//prod+1):\n        prod = a\n        for b in range(a, a_max//prod+1):\n            prod = a*b\n            for c in range(b, a_max//prod+1):\n                prod = a*b*c\n                for d in range(c, a_max//prod+1):\n                    prod = a*b*c*d\n                    for e in range(d, a_max//prod+1):\n                        prod = a*b*c*d*e\n                        for f in range(e, a_max//prod+1):\n                            prod = a*b*c*d*e*f\n                            for g in range(f, a_max//prod+1):\n                                prod = a*b*c*d*e*f*g\n                                for h in range(g, a_max//prod+1):\n                                    prod = a*b*c*d*e*f*g*h\n                                    for i in range(h, a_max//prod+1):\n                                        prod = a*b*c*d*e*f*g*h*i\n                                        for j in range(i, a_max//prod+1):\n                                            prod = a*b*c*d*e*f*g*h*i*j\n                                            for k in range(j, a_max//prod+1):\n                                                prod = a*b*c*d*e*f*g*h*i*j*k\n                                                l_min = max(2, k)\n                                                l_max = k_max // (2*prod)\n                                                for l in range(l_min,\n                                                               l_max+1):\n                                                    for m in range(l,\n                                                                   k_max+1):\n                                                        prod = (a*b*c*d*e*f*g\n                                                                *h*i*j*k*l*m)\n                                                        s = (a+b+c+d+e+f+g+h\n                                                             +i+j+k+l+m)\n                                                        kk = prod - s + 13\n                                                        if kk &gt; k_max:\n                                                            break\n                                                        if prod &lt; mps[kk]:\n                                                            mps[kk] = prod\n    return sum(set(mps))\n\nprint(problem88())\n\n7587457\nCPU times: user 232 ms, sys: 0 ns, total: 232 ms\nWall time: 235 ms\n\n\n\n\nProblem 89\n\nfrom urllib.request import urlopen\n    \nurl = 'https://projecteuler.net/project/resources/p089_roman.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n    \ndef minimize_form(number):\n    number = number.replace('VIIII', 'IX')\n    number = number.replace('IIII', 'IV')\n    number = number.replace('LXXXX', 'XC')\n    number = number.replace('XXXX', 'XL')\n    number = number.replace('DCCCC', 'CM')\n    number = number.replace('CCCC', 'CD')\n    return number\n\n\n%%time\ndef problem89():\n    return (sum([len(x) for x in DATA.split()])\n            - sum([len(minimize_form(x)) for x in DATA.split()]))\n\nprint(problem89())\n\n743\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 1.1 ms\n\n\n\n\nProblem 90\n\nfrom itertools import combinations, product\n\ndef check(square, pairs):\n    if (square[0], square[1]) in pairs or (square[1], square[0]) in pairs:\n        return True\n    else:\n        return False\n\n\n%%time\ndef problem90():\n    arrangements = [set(x) | {'6', '9'} if ('6' in x or '9' in x)\n                    else set(x) for x in combinations('0123456789', 6)]\n    squares = ['01', '04', '09', '16', '25', '36', '49', '64', '81']\n    c = 0\n    for dice1 in arrangements:\n        for dice2 in arrangements:\n            pairs = list(product(dice1, dice2))\n            flag = True\n            for square in squares:\n                if not check(square, pairs):\n                    flag = False\n                    break\n            if flag == True:\n                c += 1\n    return c//2\n\n\nprint(problem90())\n\n1217\nCPU times: user 360 ms, sys: 0 ns, total: 360 ms\nWall time: 360 ms\n\n\n\n\nProblem 91\n\nfrom numba import jit\n\n\n@jit\ndef problem91():\n    s = 0\n    for i in range(0, 51):\n        for j in range(0, i):\n            c = i**2 + j**2\n            for a in range(0, 51):\n                for b in range(0, 51):\n                    if a == 0 and b == 0:\n                        continue\n                    if a == i and b == j:\n                        continue\n                    if a**2 + b**2 + (i-a)**2 + (j-b)**2 == c:\n                        s += 1\n            for k in range(1, 51):\n                if k**2 + k**2 == c + (i-k)**2 + (j-k)**2:\n                    s += 1\n    return 2*s + 50*50\n\nprint(problem91())\n\n14234\n\n\n\n\nProblem 92\n\nfrom numba import jit\n\n@jit\ndef make_sum_digits2_table():\n    sum_digits2_table = [0] * 10**7\n    digits2 = [x**2 for x in range(10)]\n    for i1 in range(10):\n        for i2 in range(10):\n            for i3 in range(10):\n                for i4 in range(10):\n                    for i5 in range(10):\n                        for i6 in range(10):\n                            for i7 in range(10):\n                                num = (i1+i2*10+i3*100+i4*1000+i5*10000\n                                       +i6*100000+i7*1000000)\n                                sd2 = (digits2[i1]+digits2[i2]+digits2[i3]\n                                       +digits2[i4]+digits2[i5]\n                                       +digits2[i6]+digits2[i7])\n                                sum_digits2_table[num] = sd2\n    return sum_digits2_table\n\n\n%%time\n@jit\ndef problem92():\n    sum_digits2_table = make_sum_digits2_table()\n    chain_end = [0] * 10**7\n    chain_end[1] = 1\n    chain_end[89] = 2\n    for i in range(1, 10**7):\n        j = i\n        while chain_end[j] == 0:\n            j = sum_digits2_table[j]\n        chain_end[i] = chain_end[j]\n        \n    c = 0\n    for i in range(10**7):\n        if chain_end[i] == 2:\n            c += 1\n    return c\n\nprint(problem92())\n\n8581146\nCPU times: user 432 ms, sys: 16 ms, total: 448 ms\nWall time: 451 ms\n\n\n\n\nProblem 93\n\nfrom itertools import combinations, permutations\nfrom operator import add, truediv, mul, sub\n\n\n%%time\ndef problem93():\n    combos = dict()\n    ops = [add, truediv, mul, sub,\n           lambda a, b: sub(b, a), lambda a, b: truediv(b, a)]\n    for four_digit in list(combinations(range(1, 10), 4)):\n        A = [0]*10000\n        for (a, b, c, d) in permutations(four_digit):\n            for op1 in ops:\n                for op2 in ops:\n                    for op3 in ops:\n                        try:\n                            num = op3(op2(op1(a,b),c),d)\n                        except ZeroDivisionError:\n                            continue\n                        if num &gt; 0 and num - int(num) &lt; 0.001:\n                            A[int(num)] = 1\n        A[0] = 1\n        combos[four_digit] = A.index(0) - 1\n    max_combo = max(combos, key=combos.get)\n    return ''.join([str(x) for x in max_combo])\n\nprint(problem93())\n\n1258\nCPU times: user 304 ms, sys: 0 ns, total: 304 ms\nWall time: 303 ms\n\n\n\n\nProblem 94\n\nfrom numba import jit\n\n\n%%time\n@jit\ndef problem94():\n    n_max = (10**9 - 2) // 6\n    s = 0\n    for n in range(2, n_max+1):\n        h2 = 3*n*n + 4*n + 1\n        if h2 == int(h2**0.5)**2:\n            s += (6*n+2)\n        h2 = 3*n*n - 4*n + 1\n        if h2 == int(h2**0.5)**2:\n            s += (6*n-2)\n    n = n_max+1\n    h2 = 3*n*n - 4*n + 1\n    if h2 == int(h2**0.5)**2:\n        s += (6*n-2)\n    return s\n\nprint(problem94())\n\n518408346\nCPU times: user 632 ms, sys: 8 ms, total: 640 ms\nWall time: 639 ms\n\n\n\n\nProblem 95\n\nimport numpy as np\nfrom numba import jit\n\n@jit\ndef get_proper_sum(limit):\n    sieve = np.ones(limit+1)\n    proper_sum = np.ones(limit+1, dtype=np.int32)\n    current_n = np.arange(limit+1)\n    for p in range(2, int(limit**0.5)+1):\n        if sieve[p] == 1:\n            sieve[p::p] = 0\n            for n in range(p, limit+1, p):\n                temp = proper_sum[n]\n                while current_n[n] % p == 0:\n                    current_n[n] //= p\n                    proper_sum[n] = proper_sum[n]*p + temp\n    current_n = (current_n != 1) + current_n\n    proper_sum = proper_sum * current_n - np.arange(limit+1)\n    return proper_sum\n\n\n%%time\n@jit\ndef problem95():\n    limit = 10**6\n    proper_sum = get_proper_sum(limit)\n    checked = np.zeros(limit+1, dtype=np.int8)\n    checked[proper_sum &gt; limit] = -1\n    checked[proper_sum == 1] = -1\n    checked[1] = -1\n    len_max = 0\n    start_min = 0\n    for i in range(2, limit+1):\n        if checked[i] != 0:\n            continue\n        next_i = i\n        while checked[next_i] == 0:\n            checked[next_i] = 1\n            next_i = proper_sum[next_i]\n        if checked[next_i] == 1:\n            j = i\n            while j != next_i:\n                checked[j] = -1\n                j = proper_sum[j]\n            chain_len = 1\n            chain_min = next_i\n            j = proper_sum[next_i]\n            while j != next_i:\n                checked[j] = -1\n                chain_len += 1\n                if j &lt; chain_min:\n                    chain_min = j\n                j = proper_sum[j]\n            if chain_len &gt; len_max:\n                len_max = chain_len\n                start_min = chain_min\n        else:\n            j = i\n            while checked[j] == 1:\n                checked[j] = -1\n                j = proper_sum[j]\n    return start_min\n\nprint(problem95())\n\n14316\nCPU times: user 808 ms, sys: 0 ns, total: 808 ms\nWall time: 808 ms\n\n\n\n\nProblem 96\n\nfrom urllib.request import urlopen\nimport numpy as np\n    \nurl = 'https://projecteuler.net/project/resources/p096_sudoku.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\ndef scan(sudoku):\n    for num in range(9):\n        for a in range(3):\n            for b in range(3):\n                if sudoku[a,b,:,:,num].sum() == 1:\n                    c_list, d_list = np.nonzero(sudoku[a,b,:,:,num])\n                    return a, b, c_list[0], d_list[0], num\n    for num in range(9):\n        for c in range(3):\n            for d in range(3):\n                if sudoku[:,:,c,d,num].sum() == 1:\n                    a_list, b_list = np.nonzero(sudoku[:,:,c,d,num])\n                    return a_list[0], b_list[0], c, d, num\n    for num in range(9):\n        for a in range(3):\n            for c in range(3):\n                if sudoku[a,:,c,:,num].sum() == 1:\n                    b_list, d_list = np.nonzero(sudoku[a,:,c,:,num])\n                    return a, b_list[0], c, d_list[0], num\n    for a in range(3):\n        for b in range(3):\n            for c in range(3):\n                for d in range(3):\n                    if sudoku[a,b,c,d,:].sum() == 1:\n                        num_list = np.nonzero(sudoku[a,b,c,d,:])\n                        return a, b, c, d, num_list[0][0]\n    return 0, 0, 0, 0, -1\n\ndef fill(sudoku, a, b, c, d, num):\n    sudoku[a, b, :, :, num] = 0\n    sudoku[:, :, c, d, num] = 0\n    sudoku[a, :, c, :, num] = 0\n    sudoku[a, b, c, d, :] = 0\n    sudoku[a, b, c, d, num] = 10\n\ndef scan_and_fill(sudoku):\n    a, b, c, d, num = scan(sudoku)\n    if num == -1:\n        return False\n    fill(sudoku, a, b, c, d, num)\n    return True\n\ndef solve(sudoku):\n    unknown = 81 - (sudoku == 10).sum()\n    while unknown != 0:\n        if not scan_and_fill(sudoku):\n            a, b, c, d = np.unravel_index(sudoku.sum(axis=4).argmin(),\n                                          dims=(3,3,3,3))\n            num_list = np.nonzero(sudoku[a,b,c,d])[0]\n            for num in num_list:\n                new_sudoku = sudoku.copy()\n                fill(new_sudoku, a, b, c, d, num)\n                if solve(new_sudoku):\n                    sudoku[:] = new_sudoku[:]\n                    return True\n            return False\n        unknown -= 1\n    return True\n\ndef read(quiz):\n    sudoku = np.ones((3, 3, 3, 3, 9), dtype=np.uint8)\n    for row in range(9):\n        for col in range(9):\n            a = row // 3\n            b = row % 3\n            c = col // 3\n            d = col % 3\n            num = int(quiz[row][col])\n            if num != 0:\n                fill(sudoku, a, b, c, d, num-1)\n    return sudoku\n\n\n%%time\ndef problem96():\n    s = 0\n    for i in range(50):\n        quiz = DATA.split()[11*i+2:11*i+11]\n        sudoku = read(quiz)\n        solve(sudoku)\n        a, b, c = np.nonzero(sudoku[0,0,0,:])[1]\n        s += (a+1)*100 + (b+1)*10 + (c+1)\n    return s\n    \nprint(problem96())\n\n24702\nCPU times: user 552 ms, sys: 0 ns, total: 552 ms\nWall time: 552 ms\n\n\n\n\nProblem 97\n\n%%time\ndef problem97():\n    28433*2**7830457+1\n    two_to_100000 = (2**100000) % 10**10\n    two_to_30457 = (2**30457) % 10**10\n    return (28433 * two_to_100000**78 * two_to_30457 + 1) % 10**10\n\nprint(problem97())\n\n8739992577\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 86.5 µs\n\n\n\n\nProblem 98\n\nfrom urllib.request import urlopen\nfrom collections import defaultdict\n    \nurl = 'https://projecteuler.net/project/resources/p098_words.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem98():  \n    word_list = DATA.strip('\"').split('\",\"')\n    anagram_dict = defaultdict(list)\n    for word in word_list:\n        anagram_dict[''.join(sorted(word))].append(word)\n    filter_dict = dict((key, value)\n                       for (key, value) in anagram_dict.items() if len(value)&gt;1)\n\n    square_dict = dict()\n    for i in set(len(key) for key in filter_dict.keys()):\n        square_max = int((10**i-1)**0.5)\n        square_min = int((10**(i-1)-1)**0.5)+1\n        square_dict[i] = [x**2 for x in range(square_min, square_max+1)]\n        \n    s_max = 0\n    for key in filter_dict:\n        if len(filter_dict[key]) == 2:\n            word1, word2 = filter_dict[key]\n            for square in square_dict[len(key)]:\n                match = {w: s for w, s in zip(word1, str(square))}\n                reverse_match = {s: w for w, s in zip(word1, str(square))}\n                if len(match) != len(key) or len(reverse_match) != len(key):\n                    continue\n                square2 = int(''.join([match[c] for c in word2]))\n                if square2 in square_dict[len(key)]:\n                    if square &gt; s_max:\n                        s_max = square\n                    if square2 &gt; s_max:\n                        s_max = square2\n    return s_max\n\nprint(problem98())\n\n18769\nCPU times: user 116 ms, sys: 0 ns, total: 116 ms\nWall time: 115 ms\n\n\n\n\nProblem 99\n\nfrom urllib.request import urlopen\n    \nurl = 'https://projecteuler.net/project/resources/p099_base_exp.txt'\nwith urlopen(url) as response:\n    DATA = response.read().decode()\n\n\n%%time\ndef problem99():\n    x_max = 1\n    y_max = 1\n    line_max = 0\n    for line, pair in enumerate(DATA.split(), 1):\n        x, y = [int(a) for a in pair.split(',')]\n        if x &gt; x_max**(y_max/y):\n            x_max, y_max = x, y\n            line_max = line\n    return line_max\n\nprint(problem99())\n\n709\nCPU times: user 4 ms, sys: 0 ns, total: 4 ms\nWall time: 1.05 ms\n\n\n\n\nProblem 100\n\n%%time\ndef problem100():\n    pell_fund = (1, 1)\n    x, y = pell_fund\n    while x &lt;= 2*10**12-1:\n        x, y = 3*x + 4*y, 2*x + 3*y\n    return (y+1)//2\n    \nprint(problem100())\n\n756872327473\nCPU times: user 0 ns, sys: 0 ns, total: 0 ns\nWall time: 122 µs\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "fehiepsi's blog",
    "section": "",
    "text": "Inferences for Deep Gaussian Process models in Pyro\n\n\n\ndeep learning\n\ngaussian process\n\npytorch\n\npyro\n\n\n\n\n\n\n\n\n\nMay 15, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Hidden Markov Model with Pyro\n\n\n\nbayesian\n\nhidden markov model\n\npytorch\n\npyro\n\n\n\n\n\n\n\n\n\nJan 1, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nSolutions to Project Euler’s second 50 problems\n\n\n\nalgorithm\n\nproject euler\n\npython\n\nnumba\n\n\n\n\n\n\n\n\n\nOct 12, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nHow to build a Grapheme-to-Phoneme (G2P) model using PyTorch\n\n\n\ndeep learning\n\nseq2seq\n\npytorch\n\ntorchtext\n\n\n\n\n\n\n\n\n\nJun 15, 2017\n\n\n\n\n\n\n\n\n\n\n\n\nSome solutions to Rudin’s complex analysis book\n\n\n\nmath\n\ncomplex analysis\n\njupyter\n\nkatex\n\n\n\n\n\n\n\n\n\nSep 4, 2016\n\n\n\n\n\n\n\n\n\n\n\n\nSolutions to Project Euler’s first 50 problems\n\n\n\nalgorithm\n\nproject euler\n\npython\n\nnumba\n\n\n\n\n\n\n\n\n\nJul 6, 2016\n\n\n\n\n\nNo matching items\n Back to top"
  }
]